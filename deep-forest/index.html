<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 4.0.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="alternate" href="/atom.xml" title="BearCoding" type="application/atom+xml"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Gemini",version:"7.5.0",exturl:!1,sidebar:{position:"left",display:"post",offset:12,onmobile:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!1,scrollpercent:!1},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,algolia:{appID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},path:"search.xml",motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},translation:{copy_button:"复制",copy_success:"复制成功",copy_failure:"复制失败"},sidebarPadding:40}</script><meta name="description" content="摘要目前的深度学习模型大多建立在神经网络的基础上，即通过反向传播进行训练得到的多层参数化可微非线性模块。在本论文中，作者探讨了构建一个不可微模块的深度模型，并且作者推断深度神经网络的成功，其背后有三个特征，分别是逐层处理、模型中的特征转换和足够的模型复杂度。因此，作者提出了gcFroest方法，该方法能够生成具有上面三个特征的深度森林。该方法是一个决策树集成方法，相比于深度神经网络具有更少的参数，"><meta name="keywords" content="集成学习,深度学习"><meta property="og:type" content="article"><meta property="og:title" content="【2018】Deep Forest"><meta property="og:url" content="bearcoding.cn&#x2F;deep-forest&#x2F;index.html"><meta property="og:site_name" content="BearCoding"><meta property="og:description" content="摘要目前的深度学习模型大多建立在神经网络的基础上，即通过反向传播进行训练得到的多层参数化可微非线性模块。在本论文中，作者探讨了构建一个不可微模块的深度模型，并且作者推断深度神经网络的成功，其背后有三个特征，分别是逐层处理、模型中的特征转换和足够的模型复杂度。因此，作者提出了gcFroest方法，该方法能够生成具有上面三个特征的深度森林。该方法是一个决策树集成方法，相比于深度神经网络具有更少的参数，"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="&#x2F;deep-forest&#x2F;layer-by-layer-processing.png"><meta property="og:image" content="&#x2F;deep-forest&#x2F;cascade-forest-structure.png"><meta property="og:image" content="&#x2F;deep-forest&#x2F;traverses-path.png"><meta property="og:image" content="&#x2F;deep-forest&#x2F;multi-grained-scanning.png"><meta property="og:image" content="&#x2F;deep-forest&#x2F;multiple-sizes-windows.png"><meta property="og:image" content="&#x2F;deep-forest&#x2F;hyper-parameters.png"><meta property="og:updated_time" content="2020-01-19T08:12:10.375Z"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="&#x2F;deep-forest&#x2F;layer-by-layer-processing.png"><link rel="canonical" href="bearcoding.cn/deep-forest/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,isPage:!1,isArchive:!1}</script><title>【2018】Deep Forest | BearCoding</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-meta"><div><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">BearCoding</span><span class="logo-line-after"><i></i></span></a></div></div><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i> 首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i> 关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i> 标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i> 分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i> 归档</a></li><li class="menu-item menu-item-publication"><a href="/categories/%E5%8F%91%E8%A1%A8" rel="section"><i class="fa fa-fw fa-leanpub"></i> 发表</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="site-search"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"> <input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="搜索..." spellcheck="false" type="text" id="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"></div></div><div class="search-pop-overlay"></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><div class="reading-progress-bar"></div><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content"><div class="posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="bearcoding.cn/deep-forest/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.gif"><meta itemprop="name" content="陶文寅"><meta itemprop="description" content=""></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="BearCoding"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> 【2018】Deep Forest</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2019-12-21 19:32:33" itemprop="dateCreated datePublished" datetime="2019-12-21T19:32:33+08:00">2019-12-21</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i></span> <span class="post-meta-item-text">更新于</span> <time title="修改时间：2020-01-19 16:12:10" itemprop="dateModified" datetime="2020-01-19T16:12:10+08:00">2020-01-19</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E8%AE%BA%E6%96%87/" itemprop="url" rel="index"><span itemprop="name">论文</span></a></span></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="摘要">摘要</h1><p>目前的深度学习模型大多建立在神经网络的基础上，即通过反向传播进行训练得到的多层参数化可微非线性模块。在本论文中，作者探讨了构建一个不可微模块的深度模型，并且作者推断深度神经网络的成功，其背后有三个特征，分别是逐层处理、模型中的特征转换和足够的模型复杂度。因此，作者提出了gcFroest方法，该方法能够生成具有上面三个特征的深度森林。该方法是一个决策树集成方法，相比于深度神经网络具有更少的参数，并且模型的复杂度能够通过数据依赖的方法来进行调节。实验表明，该方法对于超参具有很强的鲁棒性，在大多数情况下，即使不同领域的数据，该方法也可以在默认设置下获得较好的性能。该研究开创了不可微模块在深度学习中的应用，并且展示了不适用反向传播构建深度学习模块的可能性。</p><a id="more"></a><h1 id="介绍">介绍</h1><p>深度学习已经成为各领域的的热点。那么，什么是深度学习？从crowd得到的答案可能是”深度学习是使用深度神经网络的机器学习的子领域“。事实上，深度神经网络（DNNs）再视觉和语音上的成功导致了深度学习的兴起，并且当前所有的深度学习应用都基于神经网络进行构建，或者更专业的说，是通过反向传播训练的多层参数化可微非线性模型。</p><p>虽然深度神经网络很强大，但是它有许多的不足。首先，DNNs有许多超参，并且学习的性能依赖于详细的参数调整。事实上，已经有许多学者使用了卷积神经网络，但由于卷积层的结构，他们实际上使用了不同的学习模型来处理不同的操作。事实上，这个问题使得DNNs的训练非常困难，并使得DNNs更像一个艺术，而不是科学或工程，而且DNNs的理论分析也非常困难，因为有太多的干扰因素和几乎无限的参数组合。第二，众所周知，DNNs训练需要大量的训练数据，因此DNNs就很难应用于小规模训练数据量的任务，有时甚至中等规模的训练数据也会失败。需要注意的是，在大数据时代，由于数据标记的成本更好，许多实际的任务缺乏足够数量的数据标记，因此这也导致了DNNs在这些任务中性能较差。众所周知，神经网络是一个黑盒，即决策过程很难被理解，并且学习行为也很难进行理论分析。此外，在训练神经网络之前，并要把网络结构确认，并且预先确定模型的复杂度。作者猜测，深度模型的复杂度通常比深度模型要复杂的多，正如在最近的研究中表明许多DNNs性能的提高是通过快捷链接、剪枝、二值化等方法，因此这些操作都是原来的网络更加简单，并且降低了模型的复杂度。假如模型模型的复杂度能通过数据来动态决定，那就更好了。值得注意的是，尽管DNNs已经发展的很好，但在许多问题上DNNs的表现并不十分优秀，有时甚至不足，例如随机森林、XGBoost仍然在许多Kaggle比赛中获奖。</p><p>作者相信为了解决学习任务的复杂度问题，必须对学习模型进行深入研究。然而，目前的深度模型总是建立在神经网络的基础上。基于上面的原因，有充分的理由去探索非神经网络模式的深度模型，换句话说，考虑是否可以与其他模块一起实现深度学习，因为它们有自己的优势，如果能够深入，可能会显示出巨大的潜力。特别是，因为神经网络是多层参数化可微非线性模型，在现实世界中，不是所有的属性都是可微的或最好的模型都是可微的，在本文中，我们试图解决这个基本问题：</p><p>”深度学习是否可以使用不可微模型进行实现？“</p><p>本文的结果可以帮助理解更多重要的问题，如（1）深度模型是否就是DNNs（或者，深度模型只能够通过可微模型进行构建）；（2）训练深度模型时候可以不适用反向传播？（反向传播必须要可微）；（3）是否有可能有一个深度模型比现有的其他模型（如随机森林或XGBoost）在任务上的表现更好？事实上，机器学习社区已经开发了许多不可微的学习模块，并且理解这些基于不可微模型的深度模型结构将有利于解决这些模块是否可以在深层学习中使用的问题。</p><p>本文，作者扩展了他们的初步研究，并提出了使用gcForest（多粒度级联森林）方法来构建深度森林，该方法不同于使用神经网络的深度模型。该模型是一个新颖的具有级联结构的集成决策树，并通过森林进行表达学习。它的表达学习能力可以通过多粒度扫描进一步增强，这使gcForest具有上下文或结构感知能力。其中级联的等级可以被自动确定，使得模型的复杂度能够依赖于数据来进行确认，而不是在训练前手动确定，这使得gcForest也能够在小规模数据上也能表现的很好，并且使得用户能够根据可用的计算资源来控制训练成本。此外，gcForest相比DNNs具有更少的超参。更好的消息是，它的性能对超参数设置非常健壮；通过实验表明，在大多数情况下它能够在默认设置下就获得优异的性能，即使是不同域的不同数据。</p><h1 id="灵感">灵感</h1><h2 id="从dnns获取的灵感">从DNNs获取的灵感</h2><p>普遍认为，深度神经网络的成功关键是表达学习能力。那么在DNNs中，表达学习的关键是什么？作者相信是逐层处理。图1提供了一个说明，当层从底部向上时，更高层次的抽象特性会出现。</p> <img src="/deep-forest/layer-by-layer-processing.png" class title="深度神经网络中的逐层处理"><p>图1：深度神经网络中的逐层处理：从底部向上，更高层次的抽象特性会出现</p><p>如果其他问题不变，那么模型的复杂度越高（即模型容量更大）就会获得更强的学习能力，那么将DNNs的成功性归因于巨大的模型复杂度听起来是合理的。然而，但这不能解释为什么浅层网络不如深层网络来的成功，因为可以通过增加无限的隐藏层单元来提高浅层网络的复杂度。因此，模型的复杂度不能用来说明DNNs的成功。相反，作者推测逐层处理是DNNs中最重要的因素之一，因为在平面网络（例如单隐层网络）中，不管其复杂性有多大，都不具备逐层处理的特性。虽然作者没有严格的论证，但这一猜想为gcForest的设计提供了重要的启示。</p><p>但是对于其他的一些学习模型，如决策树和Boosting，这些模型也都是逐层处理，但是它们却没有DNNs成功。作者认为，最重要的区别因素是，与图1所示生成的新特征的DNNs相比，决策树和Boosting在学习过程中总是在原始特征上进行表达，而没有创建新的特征，换句话说，就是模型没有对特征进行转换。此外，DNNs可以被赋予任意高度的模型复杂度，而决策树和Boosting只具有有限的模型复杂度。虽然模型复杂度不能解释DNNs的成功，但它仍然是重要的因素，因为大型模型需要有大型的训练数据。</p><p>因此，作者推测DNNs背后三个重要因素是，逐层处理、模型内的特征转换，和足够的模型复杂度。作者将尝试将这些特性赋予非NN模型的深度模型。</p><h2 id="从集成学习获得的灵感">从集成学习获得的灵感</h2><p>集成学习是机器学习中的一个范式，即通过多个学习器进行训练，并且通过组合完成一个任务。众所周知，一个集成学习器能够达到更好的性能，比单一的学习器。</p><p>为了构建更好的集成学习器，每一个独立学习器必须精准，并且多样。只结合精确的学习器往往不如结合了一些准确的学习器和一些相对较弱的学习器，因为互补性比单纯的准确性更重要。实际上，从误差模糊度分解理论上导出了一个美丽的公式。</p><p><span class="math display">\[\begin{equation} E=\bar{E}-\bar{A} \end{equation}\]</span></p><p>其中<span class="math inline">\(E\)</span>表示一个集成学习器的错误率，<span class="math inline">\(\bar{E}\)</span>表示在集成学习器中单独分类器的平均错误率，<span class="math inline">\(\bar{A}\)</span>表示各个分类器之间的平均歧义度，也被称为多样性。式子1表示，每个独立分类器越精确，并且多样性越高，那么集成学习器就更好。这为构建一个集成学习器提供了指导，但是该式子不能个作为优化的目标函数，因为歧义项在推导过程中是数学定义的，不能直接操作。后来，集成学习社区设计了许多多样性的度量指标，但是这些多样性的定义没有一个被广泛的接受。事实上，“什么是多样性？”仍是集成学习中的圣杯问题，近期的一些努力可以在其中找到。</p><p>在实际应用中，多样性增强的基本策略是在训练过程中引入基于启发式的随机性。粗略地说，主要有四类机制。</p><blockquote><ul><li>1、数据样本操作。它通过生成不同的数据样本来训练每一个学习器。例如，Bagging中的自主采样，AdaBoost中的连续重要性抽样。</li><li>2、输入特征操作。它通过生成二重特征子空间来训练每个学习器。例如，通过随机子空间方法为每个学习器随机选择一个子集特征。</li><li>3、学习参数操作。它通过为每个学习设置不同参数来生成不同的学习器。例如，为每个单独的神经网络设置不同的初始权值，而不同的分裂选择可以应用于每个单独的决策树。</li><li>4、输出表达操作。它通过使用不同的输出表征来产生不同的单个学习器。例如，通过ECOC方法通过纠错编码输出，而反转输出方法则随机改变一些训练实例的标签。</li></ul></blockquote><p>文献表明不同的机制可以一起使用。但值得注意的是，这些机制也不是一直有用。例如，数据样本处理对于稳定的学习器来说效果不好，因为它们的表现不会因为训练数据的轻微修改而显著改变。</p><p>下一节将会介绍gcForest，该方法可以被看作决策树的集成方法，几乎使用所有类别的机制来增强多样性。</p><h1 id="gcforest方法">gcForest方法</h1><h2 id="级联森林结构">级联森林结构</h2><p>深度神经网络的表达学习主要依赖于对原始特征的逐层处理。在这个认识的启发下，gcForest使用级联结构（如图2所示），即每个级联层都接收上一层特征信息处理结构，并把当前层的处理结果输出到下一层中。</p> <img src="/deep-forest/cascade-forest-structure.png" class title="级联森林结构"><p>图2：级联森林结构。假设每个层级都由两个随机森林（黑色）和两个完全随机森林（蓝色）组成。假设有三个类要预测，因此每个林将输出一个三维分类向量，然后将其连接起来以重新表达原始特征输入。</p><p>每层都是一个集成决策树森林，即一个集合。这里，作者包含了不同类型的森林以鼓励森林的多样性，因为多样性是集成结构的关键。为了简单，假设使用两个完全随机树森林和两个随机森林。每个完全随机树森林包含500个完全随机树，这些完全随机树都通过在每个树的结点随机选择一个特征进行分割，并且生长树直到只有叶子，每个叶节点仅包含相同类型的实例。同样的，每个随机森林包含500个树，这些树通过随机选择<span class="math inline">\(\sqrt{d}\)</span>个特征作为候选特征（<span class="math inline">\(d\)</span>是输入特征的数量），并且选择一个最大基尼指数作为分割点。在每个森林中树的数量是一个超参。</p><p>假定有一个实例，每个森林能够通过计算得到在每个叶节点上训练实例对于不同类别的概率，并对整个森林中的所有树进行平均，从而产生一个类别分布估计，如图3所示，其中红色部分是实例遍历到叶节点的路径。</p> <img src="/deep-forest/traverses-path.png" class title="遍历路径"><p>图3：类别向量生成。叶节点中不同标记表示不同类别。</p><p>类别的估计分布形成了一个类向量，并且与原始的特征向量连接后输入到下一级级联层中。例如，假设有三个类，那么每个森林中的每一个都会产生一个三维的类别向量，并且级联层的下一级将会收到<span class="math inline">\(12\left ( =3\times 4 \right )\)</span>个增强特征。</p><p>需要注意的是，作者采用了最简单的类别向量形式，即相关实例所在叶节点上的类别分布。但是很明显，这样少量的增强特征只能提供非常有限的增强信息，并且当原始特征向量是高维时，它很可能被淹没。通过实验中表明，这种简单的特征增强已经是有益的。假设有更多增强特征的引入，那么可以获得更好的效果。事实上，还可以结合更多的特征，例如表示先验分布的父节点的类别分布，以及表示互补分布的兄弟节点的类别分布等。这些可能可以在未来进行探索。</p><p>为了降低过拟合的风险，通过每个森林产生的类别向量都是通过<span class="math inline">\(k\)</span>份的交叉验证产生的。具体来说，每个实例将被用作训练数据<span class="math inline">\(k-1\)</span>次，并产生<span class="math inline">\(k-1\)</span>个类别向量，然后对这些向量进行平均，以产生最终类别向量作为下一级级联层的增强特征。在扩展新层后，整个级联结构的性能在验证集上进行评估，假设性能没有明显增加，那么训练过程终止，因此就确定了整个级联结构的数量。需要注意的是，当训练成本或者计算资源有限时，也可以使用训练误差而不是校验验证误差的方式来控制级联结构的增加。与大多数模型复杂度固定的深度神经网络相比，gcForest能够通过在适当的时候终止训练来自适应模型复杂度。这使得它能够适应不同规模的训练数据，而不仅仅限于大规模的训练数据。</p><h2 id="多粒度扫描">多粒度扫描</h2><p>深度神经网络非常适合处理特征之间的关系，如卷积神经网络能有效处理原始图像中关键像素之间的重要关系；循环神经网络能有效处理序列数据中关键的序列关系。受此启发，作者采用多粒度扫描的方法来增强级联森林。</p> <img src="/deep-forest/multi-grained-scanning.png" class title="多粒度扫描"><p>图4：使用滑动窗口扫描重新表示特征。假设有三个类别，原始特征为400维，滑动窗口为100维。</p><p>如图4所示，滑动窗口用于扫描原始特征。假设有400个原始特征，窗口大小为100。对于序列数据，通过滑动一个特征的窗口产生一个100维的特征向量，那么一共产生301个特征向量。假如原始特征具有空间相关性，那么一个<span class="math inline">\(20\times 20\)</span>的面板就由400个像素组成，然后一个<span class="math inline">\(10\times 10\)</span>的窗口能产生121个特征向量（即121个<span class="math inline">\(10\times 10\)</span>的面板）。从训练集的正例或负例中提取的特征向被视为正例或者负例，然后通过这些正例或负例生成类别向量，如3.1节所示，从相同大小的窗口中提取的实例被用于训练完全随机树森林和随机森林，然后生成类别向量，这些向量作为转换后的特征与原始特征进行拼接。如图4所示，假设有3个类别和一个100维的滑动窗口，那么每个森林产生301个3维的类别向量，从而得到与原始400维原始特征向量相对应的1806维变换特征向量。</p><p>对于从滑动窗口提取的实例，只需为这些实例分配原始训练示例标签，其中有些标签的分配是不正确的。例如，假设某个原始训练实例是“car”标签的正例，很明显许多提取的实例并不包含“car”，因此这些正例将会被分配为错误的标签。这实际上与翻转输出方法有关，翻转输出方法是集成多样性增强的输出表达操作的代表。</p><p>注意，当转换的特征向量太长而无法存储时，可以使用特征采样，例如，可以通过对滑动窗口扫描生成的实例进行子采样，因为完全随机的树不依赖于特征分割选择，而随机的林对不准确的特征分割选择不敏感。这种特征采样过程还与随机子空间方法有关，随机子空间方法是集成多样性增强中输入特征操作的代表。</p><p>图4只显示了一种尺寸的滑动窗口。通过使用多个不同大小的滑动窗口，将生成不同粒度的特征向量，如图5所示。</p> <img src="/deep-forest/multiple-sizes-windows.png" class title="多尺度滑动窗口"><p>图5：gcForest的整体过程。假设有三个分类需要预测，原始的特征有400维，并且有三个尺度的滑动窗口。</p><h2 id="总体过程和超参">总体过程和超参</h2><p>图5展示了gcForest的整体过程。假设原输入有400个原始特征，在多粒度扫描中有三个滑动窗口大小。如果有<span class="math inline">\(m\)</span>个训练实例，大小为100个特征的滑动窗口将会产生由<span class="math inline">\(301 \times m\)</span>个100维组成的训练数据集。这些数据将被用作训练完全随机树森林和随机森林，每个森林包含500棵树。假设由三个类别需要被预测，那么按照3.1的描述，就会有得到一个1806维的特征向量。这些训练数据的转换将被用于级联森林的第一级训练。</p><p>类似的，如果滑动窗口的大小分别是200和300个特征，那么将会为每个原始训练实例生成1206维和606维的特征向量。将转换后的特征向量与上一级生成的类别向量相结合，分别训练第二级和第三级森林。此过程将重复，直到验证性能收敛。换句话说，最终的模型是一个级联模型，每个级联节点都由多个级别组成，每个级别对应于一个扫描颗粒，如图5所示，第一个级联节点包含从层<span class="math inline">\(1_{A}\)</span>到层<span class="math inline">\(1_{C}\)</span>。对于不同的任务，用户可以在计算资源允许的情况下尝试更多的扫描颗粒。</p><p>给定一个测试实例，通过多粒度扫描得到其对应的转换特征表达，然后通过级联层直到最后一级。通过在最后一级聚合四个3维类向量，并用最大聚集值的类来获得NAR预测。最后的预测将通过在最后一级聚合四个3维类向量，并以最大聚集值的类别来获得。</p><p>表1总结了在实验环节使用的深度神经网络和gcForest的默认超参。</p><p>表1：默认设置和超参的总结。加粗部分是影响较大的超参；“？”表示默认值未知，或通常需要为不同的任务设置不同的参数。</p> <img src="/deep-forest/hyper-parameters.png" class title="默认超参"><h1 id="实验">实验</h1><h1 id="相关工作">相关工作</h1><p>gcForest是一种决策树集成方法。集成方法是一种强有力的机器学习技术，即对于一个任务集成了多个学习器。事实上，一些研究表明，利用具有深神经网络特征的随机森林等集成方法，其性能甚至优于单纯使用深神经网络，但是，作者使用集成方法目的并不是性能，而是构造一个非深度神经网络模式的深度模型。通过使用级联森林结构，作者希望赋予模型具有逐层处理、模型特征变换和足够的模型复杂度的特性。</p><p>在视觉任务中被广泛使用的随机森林是最成功的集成方法。近几年，人们发现完全随机树森林非常有用，如在异常检测中使用iForest，以及在数据流中处理新出现的分类的方法sencForest等。gcForest提供了另一个例子，展示了完全随机树森林的有用性。</p><p>许多研究都尝试在神经网络中连接随机森林，如将级联随机森林转换为卷积神经网络或利用随机森林来帮助初始化神经网络等。这些工作通常基于早期的将树与神经网络进行连接的研究，例如把树映射到网络、树结构的神经网络等。这些研究的目标都与本文不同。实际上，这些工作产生的最终模型都基于一个可微模型，而作者则是试图开发一个基于不可微模型的深度模型。</p><p>gcForest的多粒度扫描过程使用不同大小的滑动窗口来检测数据，这与小波等多分辨率检测过程有一定的关系。对于每个滑动窗口，都可以从一个训练实例中生成一组实例，这与多实例学习的bag生成器有关。特别是，图4的底部如果应用于图像，可以被视为<span class="math inline">\(SB\)</span>图像包生成器。</p><p>gcForest的级联过程也与Boosting有关，Boosting能够自动决定集成中的学习器的数量，尤其是级联Boosting过程在目标检测任务中取得了巨大的成功。注意的是，当使用多粒度时，gcForest级联结构中的每一层都包含了多个级，即多层级联结构，每一层都是一组合集。与以往对集成算法的研究不同，如使用Bagging作为基础学习器来Boosting，gcForest将同一级的集合用于特征的重新表达。</p><p>将一级的学习器输出作为另一个级学习器的输入是stacking的关系。基于对stacking的研究，作者使用交叉验证程序从一级输出生成下一级的输入。需要注意的是，stacking很容易造成过拟合，因此在深度模型中不能只使用stacking。</p><p>要构建一个好的集成模型，众所周知需要每个学习器都精确并且多样，但是没有被广泛接受的关于多样性的定义。因此，研究人员通常尝试通过启发式的方法来增强多样性，比如作者在每一个级中使用不同类型的森林来实现多样性。实际上，gcForest利用了所有四大类多样性增强机制。</p><p>作为一种基于树的方法，gcForest可能比深度神经网络更容易进行理论分析，尽管这超出了本文的范围。实际上，最近一些关于深度学习的理论研究，似乎更接近于基于树的模型。</p><h1 id="未来的问题">未来的问题</h1><p>一个重要的问题是增强在预测过程中的特征再表达。gcForest的实现目前采用最简单的类别向量形式，即相关实例所在叶节点上的类别分布。但是当原始特征向量为高维时，这样少量的增强特征很容易被淹没。显然，可能涉及更多的特征，例如表达先验分布的父节点的类分布、表达互补分布的兄弟节点、决策路径编码等。直观地说，更多的特征可以使更多的信息被合并，尽管不一定有助于泛化。此外，较长的类向量可以使联合多粒度扫描过程，从而获得更灵活的重新表示。最近，作者发现决策树森林可以作为自动编码器。这一方面说明了自编码能力并不像人们以前所认为的那样是神经网络的一个特殊性质；另一方面，它揭示了森林可以对丰富的信息进行编码，从而为丰富的特征重新表示提供了巨大的潜力。</p><p>另一个重要的问题是加速和减少内存消耗。按照4.8节，建立更大的深层森林可能会导致更好的泛化性能，而计算资源对于训练更大的模型至关重要。事实上，DNNs的成功很大程度上得益于GPUs的加速，但遗憾的是，森林结构并不适合GPUs。一种可能是一些新的计算设备，例如MIC（多集成核心）体系结构的Intel KNL；另一种可能是使用分布式计算实现。当多粒度扫描产生的转换特征向量太长而无法容纳时，可以执行特征采样；这不仅有助于减少存储量，而且还提供了另一个增强集合多样性的通道。这有点像将随机树森林与随机子空间相结合，这是另一种强大的集成方法。除了随机抽样，有趣的是探索更智能的采样策略，如BLB，或在适当的时候进行特征散列。Hard Negative Mining策略有助于提高泛化性能，提高Hard Negative Mining效率的努力也有助于多粒度扫描过程。通过在不同粒度扫描、类别向量生成、森林训练、完全随机树生成等过程中重用部分组件，可以进一步提高gcForest的效率。在学习模型较大的情况下，采用二次学习策略可以将模型缩小为较小的模型，这不仅有助于减少存储量，而且有助于提高预测效率。</p><p>使用完全随机的森林不仅有助于增强多样性，而且还提供了一个利用未标记数据的机会。请注意，完全随机树的生长不需要标签，而标签信息仅用于注释叶节点。直观地说，对于每个叶节点，如果要根据节点上的大多数集群对节点进行注释，则可能只需要一个标记示例；如果节点中的所有集群都不可忽略，则可能只需要每个集群一个标记示例。这也为gcForest提供了整合主动学习和（或）半监督学习策略的机会。</p><h1 id="结论">结论</h1><p>本文试图解决不可微模块能否实现深度学习的问题？作者推测，在深度神经网络的秘密背后，有三个关键特性，即逐层处理、模型特征变换和足够的模型复杂度，并且作者试图将这些特性赋予非神经网络风格的深模型。作者提出了一种基于决策树的深度模型gcForest方法，该方法能够构造深度森林，训练过程不依赖于反向传播。与深层神经网络相比，gcForest具有更少的超参数，并且在实验中，即使使用相同的参数设置，也可以在不同的领域获得优异的性能。值得注意的是，还有其他的可能性来建造深林。作为一项开创性的研究，作者在这个方向上只做了一些探索。事实上，本文最重要的价值在于，它可以为非神经网络式的深度学习或基于不可微模块的深度模型打开一扇门。</p><p>在实验中，作者发现gcForest能够在广泛的任务上实现与深度神经网络高度竞争的性能。但在某些图像任务中，其性能较差。一方面，我们认为gcForest的性能可以显著提高，例如，通过设计更好的特征重新表示方案，而不是使用当前简单的分类向量。另一方面，不可忽视的是，在深森林刚刚诞生的20多年里，诸如CNNs这样的深层神经网络模型已经被大量的研究人员（工程师）所研究。此外，图像任务是DNNs的杀手级应用。一般来说，要想在其杀手级应用程序上击败强大的技术是过于雄心勃勃的；例如，线性核支持向量机在文本分类方面仍然是最先进的，尽管DNN已经热了很多年。事实上，深度森林并不是为了取代深度神经网络而发展起来的；相反，当深度神经网络并不优越时，例如当DNNs性能低于随机森林和XGBoost时，它提供了一种替代方法。有很多任务可以发现深层森林是有用的。</p></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> 陶文寅</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="/bearcoding.cn/deep-forest/" title="【2018】Deep Forest">bearcoding.cn/deep-forest/</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"> <a href="/tags/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" rel="tag"># 集成学习</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/ml-df/" rel="next" title="【2019】Multi-Label Learning with Deep Forest"><i class="fa fa-chevron-left"></i> 【2019】Multi-Label Learning with Deep Forest</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"> <a href="/data-preprocessing/" rel="prev" title="数据预处理-数据转换">数据预处理-数据转换<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#摘要"><span class="nav-number">1.</span> <span class="nav-text">摘要</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#介绍"><span class="nav-number">2.</span> <span class="nav-text">介绍</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#灵感"><span class="nav-number">3.</span> <span class="nav-text">灵感</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#从dnns获取的灵感"><span class="nav-number">3.1.</span> <span class="nav-text">从DNNs获取的灵感</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#从集成学习获得的灵感"><span class="nav-number">3.2.</span> <span class="nav-text">从集成学习获得的灵感</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#gcforest方法"><span class="nav-number">4.</span> <span class="nav-text">gcForest方法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#级联森林结构"><span class="nav-number">4.1.</span> <span class="nav-text">级联森林结构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#多粒度扫描"><span class="nav-number">4.2.</span> <span class="nav-text">多粒度扫描</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总体过程和超参"><span class="nav-number">4.3.</span> <span class="nav-text">总体过程和超参</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#实验"><span class="nav-number">5.</span> <span class="nav-text">实验</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#相关工作"><span class="nav-number">6.</span> <span class="nav-text">相关工作</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#未来的问题"><span class="nav-number">7.</span> <span class="nav-text">未来的问题</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#结论"><span class="nav-number">8.</span> <span class="nav-text">结论</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><p class="site-author-name" itemprop="name">陶文寅</p><div class="site-description" itemprop="description"></div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">17</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/"><span class="site-state-item-count">8</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">9</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="/mailto:wenyin.tao@163.com" title="E-Mail &amp;rarr; mailto:wenyin.tao@163.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i> E-Mail</a></span></div><div class="cc-license motion-element" itemprop="license"> <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright"> &copy; <span itemprop="copyrightYear">2020</span><span class="with-love"><i class="fa fa-user"></i></span> <span class="author" itemprop="copyrightHolder">陶文寅</span></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.0.0</div> <span class="post-meta-divider">|</span><div class="theme-info">主题 – <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.5.0</div></div></footer></div><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script><script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script></body></html>