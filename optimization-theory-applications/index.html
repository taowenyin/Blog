<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 4.0.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="alternate" href="/atom.xml" title="BearCoding" type="application/atom+xml"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Gemini",version:"7.5.0",exturl:!1,sidebar:{position:"left",display:"post",offset:12,onmobile:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!1,scrollpercent:!1},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,algolia:{appID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},path:"search.xml",motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},translation:{copy_button:"复制",copy_success:"复制成功",copy_failure:"复制失败"},sidebarPadding:40}</script><meta name="description" content="介绍优化问题\[\begin{matrix}     minimize &amp;amp; f_{0}\left ( \mathbf{x} \right ) &amp;amp; \\      st. &amp;amp; f_{i}\left ( \mathbf{x} \right ) \leq b_{i} &amp;amp; i=1,\cdots ,M\\      variables &amp;amp; \mathbf{x} &amp;am"><meta property="og:type" content="article"><meta property="og:title" content="优化理论和应用"><meta property="og:url" content="bearcoding.cn&#x2F;optimization-theory-applications&#x2F;index.html"><meta property="og:site_name" content="BearCoding"><meta property="og:description" content="介绍优化问题\[\begin{matrix}     minimize &amp;amp; f_{0}\left ( \mathbf{x} \right ) &amp;amp; \\      st. &amp;amp; f_{i}\left ( \mathbf{x} \right ) \leq b_{i} &amp;amp; i=1,\cdots ,M\\      variables &amp;amp; \mathbf{x} &amp;am"><meta property="og:locale" content="zh-CN"><meta property="og:updated_time" content="2020-01-03T14:07:50.248Z"><meta name="twitter:card" content="summary"><link rel="canonical" href="bearcoding.cn/optimization-theory-applications/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,isPage:!1,isArchive:!1}</script><title>优化理论和应用 | BearCoding</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-meta"><div><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">BearCoding</span><span class="logo-line-after"><i></i></span></a></div></div><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i> 首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i> 关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i> 标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i> 分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i> 归档</a></li><li class="menu-item menu-item-publication"><a href="/categories/%E5%8F%91%E8%A1%A8" rel="section"><i class="fa fa-fw fa-leanpub"></i> 发表</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="site-search"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"> <input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="搜索..." spellcheck="false" type="text" id="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"></div></div><div class="search-pop-overlay"></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><div class="reading-progress-bar"></div><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content"><div class="posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="bearcoding.cn/optimization-theory-applications/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.gif"><meta itemprop="name" content="陶文寅"><meta itemprop="description" content=""></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="BearCoding"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> 优化理论和应用</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2020-01-01 19:25:21" itemprop="dateCreated datePublished" datetime="2020-01-01T19:25:21+08:00">2020-01-01</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i></span> <span class="post-meta-item-text">更新于</span> <time title="修改时间：2020-01-03 22:07:50" itemprop="dateModified" datetime="2020-01-03T22:07:50+08:00">2020-01-03</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E8%AF%BE%E7%A8%8B/" itemprop="url" rel="index"><span itemprop="name">课程</span></a></span></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="介绍">介绍</h1><h2 id="优化问题">优化问题</h2><p><span class="math display">\[\begin{matrix} minimize &amp; f_{0}\left ( \mathbf{x} \right ) &amp; \\ st. &amp; f_{i}\left ( \mathbf{x} \right ) \leq b_{i} &amp; i=1,\cdots ,M\\ variables &amp; \mathbf{x} &amp; \end{matrix}\]</span></p><a id="more"></a><p><span class="math inline">\(\mathbf{x}=\left(x_{1}, \dots, x_{n}\right)^{\top}\)</span>：优化变量</p><p><span class="math inline">\(f_{0}: \mathbb{R}^{n} \rightarrow \mathbb{R}\)</span>：目标函数</p><p><span class="math inline">\(f_{i}: \mathbb{R}^{n} \rightarrow \mathbb{R}, i=1, \ldots, M\)</span>：约束函数</p><p>最优解：在满足约束的情况下，<span class="math inline">\(\mathbf{X}^{\star}\)</span>是<span class="math inline">\(f_{0}\)</span>中所有向量的最小值。</p><p>可以高效可靠解决的问题：</p><blockquote><ul><li>1、线性规划问题</li><li>2、最小二乘问题</li><li>3、凸优化问题</li></ul></blockquote><h2 id="线性规划问题">线性规划问题</h2><p><span class="math display">\[\begin{matrix} minimize &amp; \mathbf{c}^{\top} \mathbf{x} &amp; \\ st. &amp; \mathbf{a}_{i}^{\top} \mathbf{x} \leq b_{i} &amp; i=1,\cdots ,M\\ variables &amp; \mathbf{x} &amp; \end{matrix}\]</span></p><h3 id="整数线性规划问题">整数线性规划问题</h3><p><span class="math display">\[\begin{matrix} minimize &amp; \mathbf{c}^{\top} \mathbf{x} &amp; \\ st. &amp; \mathbf{a}_{i}^{\top} \mathbf{x} \leq b_{i} &amp; i=1,\cdots ,M\\ &amp; \mathbf{x} \in \mathbb{Z}^{n} &amp; \\ variables &amp; \mathbf{x} &amp; \end{matrix}\]</span></p><h2 id="最小二乘问题">最小二乘问题</h2><p><span class="math display">\[\begin{matrix} minimize &amp; \left \| \mathbf{A}\mathbf{x}-\mathbf{b} \right \|_{2}^{2} &amp; \\ variables &amp; \mathbf{x} &amp; \end{matrix}\]</span></p><h2 id="凸优化问题">凸优化问题</h2><p><span class="math display">\[\begin{matrix} minimize &amp; f_{0}(\mathbf{x}) &amp; \\ st. &amp; f_{i}(\mathbf{x}) \leq b_{i} &amp; i=1,\cdots ,M\\ variables &amp; \mathbf{x} &amp; \end{matrix}\]</span></p><blockquote><ul><li>目标函数和约束函数都是凸函数。</li></ul></blockquote><p><span class="math inline">\(f_{i}(\alpha \mathbf{x}+\beta \mathbf{y}) \leq \alpha f_{i}(\mathbf{x})+\beta f_{i}(\mathbf{y})\)</span>， 假如<span class="math inline">\(\alpha+\beta=1, \alpha \geq 0, \beta \geq 0\)</span></p><blockquote><ul><li>最小二乘和特殊情况下的线性规划是凸优化问题。</li></ul></blockquote><h2 id="优化模型分类">优化模型分类</h2><blockquote><ul><li>1、按照解决方法：数值优化和分析优化。</li><li>2、按照约束条件：无约束优化和有约束优化，其中有约束优化分为集约束优化、等式约束优化、不等式约束优化。</li><li>3、按照信息完整性：正态优化和信息不确定性的优化，其中信息不确定性的优化分为随机优化和鲁棒优化。</li><li>4、按照问题属性：线性规划和非线性规划，其中非线性规划分为凸优化、二次规划、一般非线性规划。</li><li>5、按照目标：单目标优化和多目标优化。</li></ul></blockquote><h1 id="基础知识">基础知识</h1><h2 id="符号说明">符号说明</h2><p>1、<span class="math inline">\(\mathbb{X}\)</span>是一个集合，<span class="math inline">\(x \in \mathbb{X}\)</span>表示<span class="math inline">\(x\)</span>是<span class="math inline">\(\mathbb{X}\)</span>中的元素。</p><p>2、<span class="math inline">\(\{x 1, x 2, x 3, \ldots\}\)</span>和<span class="math inline">\(\{x: x \in \mathbb{R}, x&gt;5\}\)</span>都可以表示一个集合。</p><p>3、<span class="math inline">\(\mathbb{X}\)</span>和<span class="math inline">\(\mathbb{Y}\)</span>都是集合，如果<span class="math inline">\(\mathbb{X} \subset \mathbb{Y}\)</span>，那么表示<span class="math inline">\(\mathbb{X}\)</span>是<span class="math inline">\(\mathbb{Y}\)</span>的子集。</p><p>4、<span class="math inline">\(f: \mathbb{X} \rightarrow \mathbb{Y}\)</span>表示<span class="math inline">\(f\)</span>是从集合<span class="math inline">\(\mathbb{X}\)</span>到<span class="math inline">\(\mathbb{Y}\)</span>的函数。</p><p>5、<span class="math inline">\(\overset{\triangle}{=}\)</span>表示“定义为”。</p><p>6、假设定义n阶向量<span class="math inline">\(\mathbf{a}=\left[\begin{array}{c}{a_{1}} \\ {a_{2}} \\ {\vdots} \\ {a_{n}}\end{array}\right]\)</span>，那么<span class="math inline">\(\mathbf{a}^{\top}=\left[a_{1}, a_{2}, \ldots, a_{n}\right]\)</span>。</p><h2 id="线性相关">线性相关</h2><p>向量集<span class="math inline">\(\left \{ \mathbf{a_{1}},\mathbf{a_{2}},\cdots ,\mathbf{a_{k}} \right \}\)</span>是线性相关，当且仅当集合中的一个向量可以表示为其他向量的线性组合。</p><p>必要条件：<span class="math inline">\(\alpha_{1}\mathbf{a_{1}}+\alpha_{2}\mathbf{a_{2}}+\cdots +\alpha_{k}\mathbf{a_{k}}=\mathbf{0}\)</span>，并且其中至少有一个<span class="math inline">\(\alpha_{i} \neq 0\)</span>，否则就是不相关。</p><p>充分条件：<span class="math inline">\(\left ( -1 \right )\mathbf{a_{1}}+\alpha_{2}\mathbf{a_{2}}+\cdots +\alpha_{k}\mathbf{a_{k}}=\mathbf{0}\)</span></p><h2 id="二次型函数">二次型函数</h2><p><span class="math display">\[f\left ( \mathbf{x} \right )=\mathbf{x}^{\top}\mathbf{Q}\mathbf{x}\]</span></p><p>对于任意非零向量<span class="math inline">\(\mathbf{x}\)</span>，都有<span class="math inline">\(\mathbf{x}^{\top}\mathbf{Q}\mathbf{x} &gt; 0\)</span>，并且如果<span class="math inline">\(\mathbf{Q}=\mathbf{Q}^{T}\)</span>，那么<span class="math inline">\(\mathbf{Q}\)</span>是正定的。如果<span class="math inline">\(\mathbf{x}^{\top}\mathbf{Q}\mathbf{x} \geq 0\)</span>，那么<span class="math inline">\(\mathbf{Q}\)</span>是半正定的。相反，还是负定和半负定。</p><h3 id="二次规划">二次规划</h3><p><span class="math display">\[\begin{matrix} minimize &amp; \mathbf{x}^{\top}\mathbf{Q}\mathbf{x} + \mathbf{c}^{\top}\mathbf{x} \\ st. &amp; \mathbf{Ax}\leq \mathbf{b}\\ \end{matrix}\]</span></p><h3 id="线性规划最优控制">线性规划最优控制</h3><p><span class="math display">\[\begin{matrix} minimize &amp; J\left(u, x_{0}, t_{0}, t_{f}\right)=\int_{t_{0}}^{t_{f}}\left[x^{T}(t) Q(t) x(t)+u^{T}(t) R(t) u(t)\right] d t+x\left(t_{f}\right)^{T} S x\left(t_{f}\right) &amp; \\ st. &amp; \dot{x}=A(t) x+B(t) u(t)\\ \end{matrix}\]</span></p><h2 id="矩阵">矩阵</h2><h3 id="矩阵范数">矩阵范数</h3><p><span class="math display">\[\left \| \mathbf{A} \right \|_{F}=\left ( \sum_{i=1}^{m}\sum_{j=1}^{n}\left ( a_{ij} \right )^{2} \right )^{\frac{1}{2}}\]</span></p><h3 id="线性方程">线性方程</h3><p>假设有矩阵$<span class="math inline">\(\mathbf{A}=\left[\mathbf{a}_{1}, \mathbf{a}_{2}, \ldots, \mathbf{a}_{n}\right]\)</span>，那么就可以用<span class="math inline">\(\mathbf{A x}=\mathbf{b}\)</span>表示一个线性方程。</p><h3 id="内积">内积</h3><p>假设<span class="math inline">\(\mathbf{x}, \mathbf{y} \in \mathbb{R}^{n}\)</span>，那么内积就可以表示为<span class="math inline">\(\left \langle \mathbf{x}, \mathbf{y} \right \rangle=\sum_{i=1}^{n} x_{i} y_{i}=\mathbf{x}^{T} \mathbf{y}\)</span></p><h3 id="特征值和特征矩阵">特征值和特征矩阵</h3><p>令<span class="math inline">\(\mathbf{A}\)</span>是<span class="math inline">\(n \times n\)</span>的矩阵，<span class="math inline">\(\lambda\)</span>是一个标量，<span class="math inline">\(\mathbf{v}\)</span>是一个非0矩阵，如果<span class="math inline">\(\mathbf{Av}=\lambda \mathbf{v}\)</span>，那么<span class="math inline">\(\lambda\)</span>就是特征值，就是特征矩阵。</p><h2 id="几何概念">几何概念</h2><h3 id="线段">线段</h3><p>线段的定义：<span class="math inline">\(\mathbf{z}=\alpha \mathbf{x}+(1-\alpha) \mathbf{y}\)</span>，并且<span class="math inline">\(\alpha \in \left [ 0,1 \right ]\)</span>。</p><h3 id="超平面">超平面</h3><p>假设<span class="math inline">\(u_{1}, u_{2}, \ldots, u_{n}, v \in \mathbb{R}\)</span>，其中至少有一个<span class="math inline">\(u_{i}\)</span>是非零，并且所有点的集合<span class="math inline">\(\mathbf{x}=\left[x_{1}, x_{2}, \ldots, x_{n}\right]^{T}\)</span>组成的线性方程</p><p><span class="math display">\[u_{1} x_{1}+u_{2} x_{2}+\ldots+u_{n} x_{n}=v\]</span></p><p>称为超平面。</p><p>当<span class="math inline">\(n=2\)</span>时，超平面为一条直线。</p><p>当<span class="math inline">\(n=3\)</span>时，超平面为一个平面。</p><p>即超平面为源空间维度的<span class="math inline">\(n-1\)</span>。</p><p>当<span class="math inline">\(u_{1} x_{1}+u_{2} x_{2}+\ldots+u_{n} x_{n} \geq v\)</span>，表示正半空间。<span class="math inline">\(H_{+}=\left\{\mathbf{x} \in \mathbb{R}^{n}: \mathbf{u}^{T} \mathbf{x} \geq v\right\}\)</span>。</p><p>当<span class="math inline">\(u_{1} x_{1}+u_{2} x_{2}+\ldots+u_{n} x_{n} \leq v\)</span>，表示正半空间。<span class="math inline">\(H_{-}=\left\{\mathbf{x} \in \mathbb{R}^{n}: \mathbf{u}^{T} \mathbf{x} \leq v\right\}\)</span>。</p><h3 id="凸集">凸集</h3><p>假设<span class="math inline">\(\Theta \in \mathbb{R}^{n}\)</span>是一个集合，那么如果集合中的任意两个向量<span class="math inline">\(\mathbf{u}\)</span>和<span class="math inline">\(\mathbf{v}\)</span>组成的线段中的点也在集合中，那么该集合为凸集，即<span class="math inline">\(\alpha \mathbf{u}+(1-\alpha) \mathbf{v} \in \Theta\)</span>，且<span class="math inline">\(\alpha \in[0,1]\)</span>。</p><p>凸集的例子：空集、由一个点组成的集合、一条直线和线段、子空间、超平面、半空间、<span class="math inline">\(\mathbb{R}^{n}\)</span>。</p><h3 id="凸包">凸包</h3><p><span class="math inline">\(\textbf{conv} \mathbb{C}=\left \{ \theta_{1} \mathbf{x}_{1}+\cdots+\theta_{k} \mathbf{x}_{k} \mid \mathbf{x}_{i} \in \mathbb{C}, \theta_{i} \geq 0, i=1, \ldots, k,\theta_{1}+\cdots+\theta_{k}=1 \right \}\)</span></p><h3 id="领域">领域</h3><p>点<span class="math inline">\(\mathbf{x} \in \mathbb{R}^{n}\)</span>的邻域可以定义为<span class="math inline">\(\left\{\mathbf{y} \in \mathbb{R}^{n}:\|\mathbf{y}-\mathbf{x}\|&lt;\varepsilon\right\}\)</span>，其中<span class="math inline">\(\varepsilon\)</span>为正值。</p><h2 id="微积分">微积分</h2><p>1、给定<span class="math inline">\(f: \mathbb{R}^{n} \rightarrow \mathbb{R}\)</span>，那么<span class="math inline">\(f\)</span>的梯度是一个函数<span class="math inline">\(\nabla f: \mathbb{R}^{n} \rightarrow \mathbb{R}^{n}\)</span>，记作<span class="math inline">\(\nabla f(\mathbf{x})=\left[\begin{array}{c}{\frac{\partial f}{\partial x_{1}}(\mathbf{x})} \\ {\vdots} \\ {\frac{\partial f}{\partial x_{n}}(\mathbf{x})}\end{array}\right]\)</span>，其中<span class="math inline">\(\nabla f(\mathbf{x})\)</span>是一个在<span class="math inline">\(\mathbb{R}^{n}\)</span>中的向量，记作<span class="math inline">\(\nabla f(\mathbf{x})=D f(\mathbf{x})^{\top}\)</span>。</p><p>2、给定<span class="math inline">\(f: \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}\)</span>，<span class="math inline">\(f=\left[f_{1}, \ldots, f_{m}\right]^{\top}\)</span>，那么<span class="math inline">\(f\)</span>的微分方程<span class="math inline">\(D f: \mathbb{R}^{n} \rightarrow \mathbb{R}^{m \times n}\)</span>为<span class="math inline">\(D f(x)=\left[\begin{array}{ccc}{\frac{\partial f_{1}}{\partial x_{1}}(\mathbf{x})} &amp; {\dots} &amp; {\frac{\partial f_{1}}{\partial x_{n}}(\mathbf{x})} \\ {\vdots} &amp; {} &amp; {\vdots} \\ {\frac{\partial f_{m}}{\partial x_{1}}(\mathbf{x})} &amp; {\cdots} &amp; {\frac{\partial f_{m}}{\partial x_{n}}(\mathbf{x})}\end{array}\right]\)</span>。</p><p>3、如果<span class="math inline">\(f\)</span>是二阶可微，那么<span class="math inline">\(\mathbf{F}=D^{2} f=\left[\begin{array}{cccc}{\frac{\partial^{2} f}{\partial x_{1}^{2}}} &amp; {\frac{\partial^{2} f}{\partial x_{2} \partial x_{1}}} &amp; {\cdots} &amp; {\frac{\partial^{2} f}{\partial x_{n} \partial x_{1}}} \\ {\frac{\partial^{2} f}{\partial x_{1} \partial x_{2}}} &amp; {\frac{\partial^{2} f}{\partial x_{2}^{2}}} &amp; {\cdots} &amp; {\frac{\partial^{2} f}{\partial x_{n} \partial x_{1}}} \\ {\vdots} &amp; {\vdots} &amp; {\ddots} &amp; {\vdots} \\ {\frac{\partial^{2} f}{\partial x_{1} \partial x_{n}}} &amp; {\frac{\partial^{2} f}{\partial x_{2} \partial x_{n}}} &amp; {\cdots} &amp; {\frac{\partial^{2} f}{\partial x_{n}^{2}}}\end{array}\right]\)</span>，该矩阵称为<span class="math inline">\(f\)</span>的黑塞矩阵（Hessian）。</p><h2 id="水平集和梯度">水平集和梯度</h2><p><span class="math inline">\(\nabla f\left(\mathbf{x}_{0}\right)\)</span>是<span class="math inline">\(f\)</span>在<span class="math inline">\(\mathbf{x}_{0}\)</span>点处最大增长率方向。</p><h1 id="无约束优化问题">无约束优化问题</h1><h2 id="局部极小点x">局部极小点x</h2><p><span class="math inline">\(\mathbf{x}^{\ast}\)</span>是局部极小点的条件是<span class="math inline">\(\nabla f\left(\mathbf{x}^{\ast}\right)=0\)</span>。</p><h2 id="可行方向">可行方向</h2><p>如果存在<span class="math inline">\(\alpha_{0}&gt;0\)</span>，并且满足<span class="math inline">\(\alpha \in\left[0, \alpha_{0}\right]\)</span>，使得<span class="math inline">\(\mathbf{x}+\alpha \mathbf{d} \in \Omega\)</span>，那么<span class="math inline">\(\mathbf{d} \in \mathbb{R}^{n}, \mathbf{d} \neq 0\)</span>就是<span class="math inline">\(\mathbf{x} \in \Omega\)</span>中的可行方向。</p><h3 id="方向导数">方向导数</h3><p>当可行方向<span class="math inline">\(\mathbf{d}\)</span>是单位向量时，那么函数<span class="math inline">\(f\)</span>在<span class="math inline">\(\mathbf{x}\)</span>处沿方向<span class="math inline">\(\mathbf{d}\)</span>的增长率可以用内积表示，即<span class="math inline">\(\frac{\partial f}{\partial \mathbf{d}}(\mathbf{x})=\mathbf{d}^{\top} \nabla f(\mathbf{x})=\nabla f(\mathbf{x})^{\top} \mathbf{d}=\left \langle \nabla f(\mathbf{x}), \mathbf{d} \right \rangle\)</span>。</p><h3 id="fonc一阶必要条件">FONC（一阶必要条件）</h3><p>如果<span class="math inline">\(\mathbf{x}^{\ast}\)</span>是函数<span class="math inline">\(f\)</span>在<span class="math inline">\(\Omega\)</span>上的局部极小点，那么对于<span class="math inline">\(\mathbf{x}^{\ast}\)</span>处的任意可行方向<span class="math inline">\(\mathbf{d}\)</span>都有<span class="math inline">\(\mathbf{d}^{T} \nabla f\left(\mathbf{x}^{\ast}\right) \geq 0\)</span>。</p><h3 id="sonc二阶必要条件">SONC（二阶必要条件）</h3><p>如果<span class="math inline">\(f\)</span>在约束集<span class="math inline">\(\Omega\)</span>上二阶连续可导，并且<span class="math inline">\(\mathbf{x}^{\ast}\)</span>是函数<span class="math inline">\(f\)</span>在<span class="math inline">\(\Omega\)</span>上的局部极小点，<span class="math inline">\(\mathbf{d}\)</span>是<span class="math inline">\(\mathbf{x}^{\ast}\)</span>处的可行方向，并且<span class="math inline">\(\mathbf{d}^{T} \nabla f\left(\mathbf{x}^{\ast}\right) = 0\)</span>，则<span class="math inline">\(\mathbf{d}^{T} F\left(\mathbf{x}^{\ast}\right) \mathbf{d} \geq 0\)</span>，其中<span class="math inline">\(F\)</span>为函数<span class="math inline">\(f\)</span>的黑塞矩阵。</p><h3 id="sosc二阶充分条件">SOSC（二阶充分条件）</h3><p>如果<span class="math inline">\(f\)</span>在约束集<span class="math inline">\(\Omega\)</span>上二阶连续可导，如果同时满足<span class="math inline">\(\nabla f\left(\mathbf{x}^{\ast}\right)=0\)</span>和<span class="math inline">\(F\left(\mathbf{x}^{\ast}\right)&gt;0\)</span>，那么<span class="math inline">\(\mathbf{X}^{\ast}\)</span>为严格局部极小点。</p><h1 id="一维搜索方法">一维搜索方法</h1><h2 id="介绍-1">介绍</h2><p>该方法主要使用迭代搜索算法或线搜法，基本流程是：</p><blockquote><ul><li>1、赋一个初值<span class="math inline">\(x^{\left ( 0 \right )}\)</span>。</li><li>2、生成迭代序列<span class="math inline">\(x^{\left ( 1 \right )},x^{\left ( 2 \right )},\cdots\)</span>。</li><li>3、每次迭代，下一个<span class="math inline">\(x^{\left ( k+1 \right )}\)</span>依赖于上一个<span class="math inline">\(x^{\left ( k \right )}\)</span>。</li></ul></blockquote><p>目标函数<span class="math inline">\(f\)</span>，一阶导数<span class="math inline">\({f}&#39;\)</span>，二阶导数<span class="math inline">\({f}&#39;&#39;\)</span>。</p><p>常用的一维搜索算法：</p><blockquote><ul><li>1、黄金分割法。（只使用目标函数<span class="math inline">\(f\)</span>）</li><li>2、斐波那契数列方法。（只使用目标函数<span class="math inline">\(f\)</span>）</li><li>3、二分法。（只使用目标函数的一阶导数<span class="math inline">\({f}&#39;\)</span>）</li><li>4、割线法。（只使用目标函数的一阶导数<span class="math inline">\({f}&#39;\)</span>）</li><li>5、牛顿法。（只使用目标函数的一阶导数<span class="math inline">\({f}&#39;\)</span>和二阶导数<span class="math inline">\({f}&#39;&#39;\)</span>）</li></ul></blockquote><h2 id="泰勒公式">泰勒公式</h2><p>泰勒公式是许多数值方法和优化模型的基础。</p><p>假设<span class="math inline">\(f: \mathbb{R} \rightarrow \mathbb{R}\)</span>在区间<span class="math inline">\([a, b]\)</span>上是<span class="math inline">\(m\)</span>次连续可微函数。假设<span class="math inline">\(h=[b-a]\)</span>，那么<span class="math inline">\(f\)</span>的第<span class="math inline">\(i\)</span>阶导数<span class="math inline">\(f^{(i)}\)</span>为</p><p><span class="math display">\[f(b)=f(a)+\frac{h}{1 !} f^{(1)}(a)+\frac{h^{2}}{2 !} f^{(2)}(a)+\ldots+\frac{h^{m-1}}{(m-1) !} f^{(m-1)}(a)+R_{m}\]</span></p><p>其中<span class="math inline">\(R_{m}\)</span>为余项</p><p><span class="math display">\[R_{m}=\frac{h^{m}(1-\theta)^{m-1}}{(m-1) !} f^{(m)}(a+\theta h)=\frac{h^{m}}{m !}\left(a+\theta^{\prime} h\right)\]</span></p><p>其中<span class="math inline">\(\theta, \theta^{\prime} \in(0,1)\)</span>。</p><h2 id="牛顿法牛顿切线法">牛顿法（牛顿切线法）</h2><p>牛顿法主要有两方面的应用</p><p>1、求方程根：使用一阶导数求实值函数的根。</p><p>2、优化：使用一阶和二阶导数求一个实值函数优化器的逼近。</p><blockquote><p>1、目标：</p></blockquote><p>求<span class="math inline">\(x^{\ast}\)</span>，使得<span class="math inline">\(g\left(x^{\ast}\right)=0\)</span></p><blockquote><p>2、核心思想：</p></blockquote><p>基于泰勒公式展开，构建新的函数来逼近原函数。</p><p><span class="math display">\[f(x)=g\left(x_{0}\right)+\left(x-x_{0}\right) g^{\prime}\left(x_{0}\right)\]</span></p><p>由于<span class="math inline">\(f(x)\)</span>是<span class="math inline">\(g\left(x\right)\)</span>的近似。因此，要求的<span class="math inline">\(f(x)\)</span>极小点，那么就要满足一阶必要条件，即<span class="math inline">\(f(x)=0\)</span>，那么上式就可以得到</p><p><span class="math display">\[x=x_{0}-\frac{g\left(x_{0}\right)}{g^{\prime}\left(x_{0}\right)}\]</span></p><p>从而得到整个迭代过程为</p><p><span class="math display">\[x_{k+1}=x_{k}-\frac{g\left(x_{k}\right)}{g^{\prime}\left(x_{k}\right)}\]</span></p><blockquote><p>3、几何角度</p></blockquote><p>几何上来说，就是找到<span class="math inline">\(x^{\left ( k \right )}\)</span>出的切线与<span class="math inline">\(x\)</span>轴的交点作为<span class="math inline">\(x^{\left ( k+1 \right )}\)</span>，然后再以<span class="math inline">\(x^{\left ( k+1 \right )}\)</span>点的切线找到<span class="math inline">\(x^{\left ( k+2 \right )}\)</span>，一次类推最终逼近极值点。</p><blockquote><p>4、注意事项</p></blockquote><p>如果<span class="math inline">\(\frac{g\left(x_{k}\right)}{g^{\prime}\left(x_{k}\right)}\)</span>的比值不是足够小时，牛顿法可能就会失效，因为会错过极小值，所以初始点的选择很重要。</p><h2 id="牛顿优化法">牛顿优化法</h2><p>构建一个在<span class="math inline">\(x^{(k)}\)</span>点的原函数、一阶函数和二界函数，并且构建一个逼近于原函数<span class="math inline">\(f(x)\)</span>的近似函数</p><p><span class="math display">\[q(x)=f\left(x^{(k)}\right)+f^{\prime}\left(x^{(k)}\right)\left(x-x^{(k)}\right)+\frac{1}{2} f^{\prime \prime}\left(x^{(k)}\right)\left(x-x^{(k)}\right)^{2}\]</span></p><p>其中<span class="math inline">\(q(x)\)</span>与<span class="math inline">\(f(x)\)</span>的一阶和二阶导数相比配</p><p><span class="math inline">\(q\left(x^{(k)}\right)=f\left(x^{(k)}\right)\)</span></p><p><span class="math inline">\(q^{\prime}\left(x^{(k)}\right)=f^{\prime}\left(x^{(k)}\right)\)</span></p><p><span class="math inline">\(q^{\prime \prime}\left(x^{(k)}\right)=f^{\prime \prime}\left(x^{(k)}\right)\)</span></p><p>原先去<span class="math inline">\(f\)</span>的最小值，现在去近似函数<span class="math inline">\(q\)</span>的最小值。</p><p>按照一阶必要条件（FONC）可以得到</p><p><span class="math display">\[q^{\prime}(x)=f^{\prime}\left(x^{(k)}\right)+f^{\prime \prime}\left(x^{(k)}\right)\left(x-x^{(k)}\right)=0\]</span></p><p>令<span class="math inline">\(x=x^{(k+1)}\)</span>，那么就可以得到</p><p><span class="math display">\[x^{(k+1)}=x^{(k)}-\frac{f^{\prime}\left(x^{(k)}\right)}{f^{\prime \prime}\left(x^{(k)}\right)}\]</span></p><p>注意，对于区间内任何<span class="math inline">\(x\)</span>都有<span class="math inline">\(f^{\prime \prime}(x)&gt;0\)</span>，那么牛顿法能正常工作，但如果对于一些<span class="math inline">\(x\)</span>造成<span class="math inline">\(f^{\prime \prime}(x)&lt;0\)</span>，那么牛顿法可能就会收敛到极大点，而不是极小点。</p><h2 id="割线法">割线法</h2><p>如果函数二阶不可导，那么二阶导数就可以近似为</p><p><span class="math display">\[f^{\prime \prime}\left(x^{(k)}\right) \approx \frac{f^{\prime}\left(x^{(k)}\right)-f^{\prime}\left(x^{(k-1)}\right)}{x^{(k)}-x^{(k-1)}}\]</span></p><p>因此，就可以得到割线法公式</p><p><span class="math display">\[x^{(k+1)}=x^{(k)}-\frac{x^{(k)}-x^{(k-1)}}{f^{\prime}\left(x^{(k)}\right)-f^{\prime}\left(x^{(k-1)}\right)} f^{\prime}\left(x^{(k)}\right)\]</span></p><p>该方法需要有两个初始值，分别是<span class="math inline">\(x^{(k)}\)</span>和<span class="math inline">\(x^{(k-1)}\)</span>。</p><h2 id="多维优化问题中的一维搜索">多维优化问题中的一维搜索</h2><p>一维搜索方法在多维优化问题中扮演非常重要的角色。特别是对于多维优化问题的迭代求解算法，通常每次迭代都会包括一维搜索过程。</p><p>令目标函数<span class="math inline">\(f: \mathbb{R}^{n} \rightarrow \mathbb{R}\)</span>，寻找<span class="math inline">\(f\)</span>极小值的迭代算法为</p><p><span class="math display">\[\mathbf{x}^{(k+1)}=\mathbf{x}^{(k)}+\alpha_{k} \mathbf{d}^{(k)}\]</span></p><p>其中<span class="math inline">\(\mathbf{x}^{(0)}\)</span>为给定的初始点，<span class="math inline">\(\alpha_{k} \geq 0\)</span>为步长，其确定方式为使下面函数最小</p><p><span class="math display">\[\phi_{k}(\alpha)=f\left(\mathbf{x}^{(k)}+\alpha \mathbf{d}^{(k)}\right)\]</span></p><p>其中<span class="math inline">\(\mathbf{d}\)</span>为搜索方向。通过一维搜索确定<span class="math inline">\(\alpha_{k}\)</span>后，可以使得<span class="math inline">\(f\left(\mathbf{x}^{(k+1)}\right) &lt; f\left(\mathbf{x}^{(k)}\right)\)</span>。</p><h1 id="梯度方法">梯度方法</h1><p>常用的梯度方法</p><blockquote><p>梯度下降</p></blockquote><blockquote><ul><li>1、固定步长的梯度下降</li><li>2、最速梯度下降</li><li>3、随机梯度下降</li></ul></blockquote><blockquote><p>共轭梯度</p></blockquote><h2 id="介绍-2">介绍</h2><p>因为<span class="math inline">\(\mathbf{d}=\nabla f(\mathbf{x})\)</span>是增长率最大的方向，因此<span class="math inline">\(\mathbf{d}=-\nabla f(\mathbf{x})\)</span>就是下降最快的方向。</p><p>梯度下降算法的核心实现：</p><p><span class="math display">\[\mathbf{x}^{(k+1)}=\mathbf{x}^{(k)}-\alpha_{k} \nabla f\left(\mathbf{x}^{(k)}\right)\]</span></p><p>其中<span class="math inline">\(\alpha_{k}\)</span>是步长，<span class="math inline">\(\nabla f\left(\mathbf{x}^{(k)}\right)\)</span>是方向。</p><p>1、<span class="math inline">\(\alpha_{k}\)</span>选择的影响：</p><blockquote><p>当<span class="math inline">\(\alpha_{k}\)</span>太小，那么迭代的次数就会变多。</p></blockquote><blockquote><p>当<span class="math inline">\(\alpha_{k}\)</span>太大，那么就会在最优点之间来回震荡。</p></blockquote><p>2、<span class="math inline">\(\alpha_{k}\)</span>的选择有许多方法：</p><blockquote><p>可以在每次迭代中使<span class="math inline">\(\alpha_{k}\)</span>固定，也可以让<span class="math inline">\(\alpha_{k}\)</span>随迭代变化而变化。</p></blockquote><blockquote><p>动态计算<span class="math inline">\(\alpha_{k}\)</span></p></blockquote><p><span class="math display">\[\alpha_{k}=\arg \min _{\alpha \geq 0} f\left(\mathbf{x}^{(\mathbf{k})}-\alpha \nabla f\left(\mathbf{x}^{(k)}\right)\right)\]</span></p><h2 id="最速下降法">最速下降法</h2><blockquote><p>该方法时一个梯度法。</p></blockquote><blockquote><p>选择步长以实现目标函数在每个单独步骤中的最大减少量。</p></blockquote><p><span class="math display">\[\alpha_{k}=\arg \min _{\alpha \geq 0} f\left(\mathbf{x}^{(\mathbf{k})}-\alpha \nabla f\left(\mathbf{x}^{(\mathbf{k})}\right)\right)\]</span></p><p>最速下降法的基本过程：x</p><blockquote><ul><li>1、每一步以<span class="math inline">\(\mathbf{x}^{(\mathbf{k})}\)</span>开始。</li><li>2、通过线搜法沿着方向<span class="math inline">\(-\nabla f\left(\mathbf{x}^{(\mathbf{k})}\right)\)</span>获取最小的<span class="math inline">\(\mathbf{x}^{(\mathbf{k}+\mathbf{1})}\)</span>。</li></ul></blockquote><p>在最速下降法中，假如<span class="math inline">\(\left\{\mathbf{x}^{(\mathbf{k})}\right\}_{k=0}^{\infty}\)</span>是每次迭代得到的值，那么对于每个<span class="math inline">\(k\)</span>，都有<span class="math inline">\(\mathbf{x}^{(\mathbf{k}+\mathbf{1})}-\mathbf{x}^{(\mathbf{k})}\)</span>与<span class="math inline">\(\mathbf{x}^{(\mathbf{k}+2)}-\mathbf{x}^{(\mathbf{k}+1)}\)</span>正交。</p><p>当<span class="math inline">\(\mathbf{x}^{(\mathbf{k}+\mathbf{1})}=\mathbf{x}^{(\mathbf{k})}\)</span>，那么这种情况就是算法停止的条件。</p><blockquote><p>梯度收敛的总结</p></blockquote><p>1、如果目标函数<span class="math inline">\(f\)</span>是凸的，则通过满足Wolfe条件的线搜索来选择步长，则相应的梯度法是全局收敛的。</p><p>2、如果目标函数是二次型<span class="math inline">\(f(\mathbf{x})=\frac{1}{2} \mathbf{x}^{T} Q \mathbf{x}-\mathbf{b}^{T} \mathbf{x}\)</span>，并且<span class="math inline">\(Q\)</span>是正定的，那么最速梯度法是全局收敛。</p><p>3、如果目标函数是二次型<span class="math inline">\(f(\mathbf{x})=\frac{1}{2} \mathbf{x}^{T} Q \mathbf{x}-\mathbf{b}^{T} \mathbf{x}\)</span>，并且<span class="math inline">\(Q\)</span>是正定的，那么固定步长梯度法也是全局收敛，并且<span class="math inline">\(0&lt;\alpha&lt;\frac{2}{\lambda_{\max }(Q)}\)</span>。</p><p>给定一个序列<span class="math inline">\(\left\{\mathbf{x}^{(k)}\right\}\)</span>，该序列收敛到<span class="math inline">\(\mathbf{x}^{\ast}\)</span>。当<span class="math inline">\(\lim _{k \rightarrow \infty}\left\|\mathbf{x}^{(k)}-\mathbf{x}^{*}\right\|=0\)</span>时，那么收敛的阶数是<span class="math inline">\(p\)</span>，并且<span class="math inline">\(p \in \mathbb{R}\)</span>。</p><p>当<span class="math inline">\(p=1\)</span>（一阶收敛），并且<span class="math inline">\(\lim _{k \rightarrow \infty} \frac{\left\|\mathbf{x}^{(k+1)}-\mathbf{x}^{*}\right\|}{\left\|\mathbf{x}^{(k)}-\mathbf{x}^{*}\right\|^{p}}=1\)</span>，那么收敛是次线性的。</p><p>当<span class="math inline">\(p=1\)</span>（一阶收敛），并且<span class="math inline">\(\lim _{k \rightarrow \infty} \frac{\left\|\mathbf{x}^{(k+1)}-\mathbf{x}^{*}\right\|}{\left\|\mathbf{x}^{(k)}-\mathbf{x}^{*}\right\|^{p}}&lt;1\)</span>，那么收敛是线性的。</p><p>当<span class="math inline">\(p&gt;1\)</span>，那么收敛是超线性的。</p><p>当<span class="math inline">\(p=2\)</span>（二阶收敛），那么收敛是二次型的。</p><p>当<span class="math inline">\(p=3\)</span>（三阶收敛），那么收敛是立方的。</p><blockquote><p>例子</p></blockquote><p>假设<span class="math inline">\(x^{\left ( k \right )}=\frac{1}{k}\)</span>，并且<span class="math inline">\(x^{\left ( k \right )} \rightarrow 0\)</span>，那么<span class="math inline">\(\frac{\left|x^{(k+1)}\right|}{\left|x^{(k)}\right|^{p}}=\frac{\frac{1}{k+1}}{\frac{1}{k^{p}}}=\frac{k^{p}}{k+1}\)</span></p><p>当<span class="math inline">\(p &gt; 1\)</span>时，上式趋近于<span class="math inline">\(\infty\)</span>。</p><p>当<span class="math inline">\(p &lt; 1\)</span>时，上式收敛到0。</p><p>当<span class="math inline">\(p = 1\)</span>时，上式收敛到1。</p><p>因此，收敛的阶数为1。</p><h1 id="牛顿法">牛顿法</h1><h2 id="基本思想">基本思想</h2><p>1、给定一个初值点。</p><p>2、构造目标函数的二次逼近，该目标函数与该点上的第一和第二导数值相匹配。</p><p>3、最小化近似二次函数代替原来的目标函数。</p><p>4、使用近似函数的极小值作为起点，迭代地重复该过程。</p><h2 id="牛顿法没有步阶大小或者步阶为1">牛顿法（没有步阶大小，或者步阶为1）</h2><p>1、给定<span class="math inline">\(f: \mathbb{R}^{n} \rightarrow \mathbb{R}\)</span>，并且当前迭代为<span class="math inline">\(\mathbf{x}^{(k)}\)</span>。</p><p>2、通过二次型近似函数<span class="math inline">\(f\)</span>求<span class="math inline">\(\mathbf{x}^{(k+1)}\)</span>。</p><p><span class="math display">\[q(\mathbf{x})=f\left(\mathbf{x}^{(k)}\right)+\left(\mathbf{x}-\mathbf{x}^{(k)}\right)^{\top} \mathbf{g}^{(k)}+\frac{1}{2}\left(\mathbf{x}-\mathbf{x}^{(k)}\right)^{\top} \mathbf{F}\left(\mathbf{x}^{(k)}\right)\left(\mathbf{x}-\mathbf{x}^{(k)}\right)\]</span></p><p>3、最小化<span class="math inline">\(q\)</span>来迭代下一个<span class="math inline">\(\mathbf{x}^{(k+1)}\)</span>。</p><p>4、假设<span class="math inline">\(\mathbf{g}^{(k)}=\nabla f\left(\mathbf{x}^{(k)}\right)\)</span>。通过一阶必要条件，可以知道<span class="math inline">\(\nabla q\left(\mathbf{x}^{(k)}\right)=0\)</span>。</p><p><span class="math display">\[\nabla q\left(\mathbf{x}^{(k)}\right)=\mathbf{g}^{(k)}+\mathbf{F}\left(\mathbf{x}^{(k)}\right)\left(\mathbf{x}-\mathbf{x}^{(k)}\right)=0\]</span></p><p>5、牛顿算法</p><p><span class="math display">\[\mathbf{x}^{(k+1)}=\mathbf{x}^{(k)}-\mathbf{F}\left(\mathbf{x}^{(k)}\right)^{-1} \mathbf{g}^{(k)}\]</span></p><p>搜索方向</p><p><span class="math display">\[\mathbf{d}^{(k)}=-\mathbf{F}\left(\mathbf{x}^{(k)}\right)^{-1} \mathbf{g}^{(k)}=\mathbf{x}^{(k+1)}-\mathbf{x}^{(k)}\]</span></p><h2 id="levenberg-marquardt修正">Levenberg-Marquardt修正</h2><p>如果黑塞矩阵<span class="math inline">\(\mathbf{F}\left(\mathbf{x}^{(k)}\right)\)</span>不正定，那么搜索方向就会使下降方向<span class="math inline">\(\mathbf{d}^{(k)}=-\mathbf{F}\left(\mathbf{x}^{(k)}\right)^{-1} \mathbf{g}^{(k)}\)</span>可能不会是下降方向，因此Levenberg-Marquardt修正解决了这个问题。</p><p><span class="math display">\[\mathbf{x}^{(k+1)}=\mathbf{x}^{(k)}-\alpha_{k}\left(\mathbf{F}\left(\mathbf{x}^{(k)}\right)+\mu_{k} \mathbf{I}\right)^{-1} \mathbf{g}^{(k)}\]</span></p><p>其中<span class="math inline">\(\mu_{k} \geq 0\)</span>，<span class="math inline">\(F\)</span>为对称矩阵。</p><p>1、<span class="math inline">\(\mu_{k} \rightarrow 0\)</span>：Levenberg-Marquardt修正逐步接近牛顿法。</p><p>2、<span class="math inline">\(\mu_{k} \rightarrow \infty\)</span>：Levenberg-Marquardt修正表现出步长较小时的梯度方法的特征。</p><p>实际应用中，<span class="math inline">\(\mu_{k}\)</span>的初始值较小，然后逐步增加，直到出现下降特征，即<span class="math inline">\(f\left ( \mathbf{x}^{k+1} \right ) &lt; f\left ( \mathbf{x}^{k} \right )\)</span>为止。</p><h2 id="牛顿法总结">牛顿法总结</h2><p>1、如果起始点与最小值足够近，牛顿法的效果会很好。</p><p>2、可以合并一个步长以确保下降</p><p>3、对于二次型，一步收敛。</p><h1 id="共轭方向法">共轭方向法</h1></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> 陶文寅</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="/bearcoding.cn/optimization-theory-applications/" title="优化理论和应用">bearcoding.cn/optimization-theory-applications/</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/data-preprocessing/" rel="next" title="数据预处理-数据转换"><i class="fa fa-chevron-left"></i> 数据预处理-数据转换</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"></div></div></footer></article></div></div></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#介绍"><span class="nav-number">1.</span> <span class="nav-text">介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#优化问题"><span class="nav-number">1.1.</span> <span class="nav-text">优化问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#线性规划问题"><span class="nav-number">1.2.</span> <span class="nav-text">线性规划问题</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#整数线性规划问题"><span class="nav-number">1.2.1.</span> <span class="nav-text">整数线性规划问题</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#最小二乘问题"><span class="nav-number">1.3.</span> <span class="nav-text">最小二乘问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#凸优化问题"><span class="nav-number">1.4.</span> <span class="nav-text">凸优化问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#优化模型分类"><span class="nav-number">1.5.</span> <span class="nav-text">优化模型分类</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#基础知识"><span class="nav-number">2.</span> <span class="nav-text">基础知识</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#符号说明"><span class="nav-number">2.1.</span> <span class="nav-text">符号说明</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#线性相关"><span class="nav-number">2.2.</span> <span class="nav-text">线性相关</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#二次型函数"><span class="nav-number">2.3.</span> <span class="nav-text">二次型函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#二次规划"><span class="nav-number">2.3.1.</span> <span class="nav-text">二次规划</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#线性规划最优控制"><span class="nav-number">2.3.2.</span> <span class="nav-text">线性规划最优控制</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#矩阵"><span class="nav-number">2.4.</span> <span class="nav-text">矩阵</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#矩阵范数"><span class="nav-number">2.4.1.</span> <span class="nav-text">矩阵范数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#线性方程"><span class="nav-number">2.4.2.</span> <span class="nav-text">线性方程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#内积"><span class="nav-number">2.4.3.</span> <span class="nav-text">内积</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#特征值和特征矩阵"><span class="nav-number">2.4.4.</span> <span class="nav-text">特征值和特征矩阵</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#几何概念"><span class="nav-number">2.5.</span> <span class="nav-text">几何概念</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#线段"><span class="nav-number">2.5.1.</span> <span class="nav-text">线段</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#超平面"><span class="nav-number">2.5.2.</span> <span class="nav-text">超平面</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#凸集"><span class="nav-number">2.5.3.</span> <span class="nav-text">凸集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#凸包"><span class="nav-number">2.5.4.</span> <span class="nav-text">凸包</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#领域"><span class="nav-number">2.5.5.</span> <span class="nav-text">领域</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#微积分"><span class="nav-number">2.6.</span> <span class="nav-text">微积分</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#水平集和梯度"><span class="nav-number">2.7.</span> <span class="nav-text">水平集和梯度</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#无约束优化问题"><span class="nav-number">3.</span> <span class="nav-text">无约束优化问题</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#局部极小点x"><span class="nav-number">3.1.</span> <span class="nav-text">局部极小点x</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#可行方向"><span class="nav-number">3.2.</span> <span class="nav-text">可行方向</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#方向导数"><span class="nav-number">3.2.1.</span> <span class="nav-text">方向导数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#fonc一阶必要条件"><span class="nav-number">3.2.2.</span> <span class="nav-text">FONC（一阶必要条件）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sonc二阶必要条件"><span class="nav-number">3.2.3.</span> <span class="nav-text">SONC（二阶必要条件）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sosc二阶充分条件"><span class="nav-number">3.2.4.</span> <span class="nav-text">SOSC（二阶充分条件）</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#一维搜索方法"><span class="nav-number">4.</span> <span class="nav-text">一维搜索方法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#介绍-1"><span class="nav-number">4.1.</span> <span class="nav-text">介绍</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#泰勒公式"><span class="nav-number">4.2.</span> <span class="nav-text">泰勒公式</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#牛顿法牛顿切线法"><span class="nav-number">4.3.</span> <span class="nav-text">牛顿法（牛顿切线法）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#牛顿优化法"><span class="nav-number">4.4.</span> <span class="nav-text">牛顿优化法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#割线法"><span class="nav-number">4.5.</span> <span class="nav-text">割线法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#多维优化问题中的一维搜索"><span class="nav-number">4.6.</span> <span class="nav-text">多维优化问题中的一维搜索</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#梯度方法"><span class="nav-number">5.</span> <span class="nav-text">梯度方法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#介绍-2"><span class="nav-number">5.1.</span> <span class="nav-text">介绍</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#最速下降法"><span class="nav-number">5.2.</span> <span class="nav-text">最速下降法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#牛顿法"><span class="nav-number">6.</span> <span class="nav-text">牛顿法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#基本思想"><span class="nav-number">6.1.</span> <span class="nav-text">基本思想</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#牛顿法没有步阶大小或者步阶为1"><span class="nav-number">6.2.</span> <span class="nav-text">牛顿法（没有步阶大小，或者步阶为1）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#levenberg-marquardt修正"><span class="nav-number">6.3.</span> <span class="nav-text">Levenberg-Marquardt修正</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#牛顿法总结"><span class="nav-number">6.4.</span> <span class="nav-text">牛顿法总结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#共轭方向法"><span class="nav-number">7.</span> <span class="nav-text">共轭方向法</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><p class="site-author-name" itemprop="name">陶文寅</p><div class="site-description" itemprop="description"></div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">17</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/"><span class="site-state-item-count">8</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">9</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="/mailto:wenyin.tao@163.com" title="E-Mail &amp;rarr; mailto:wenyin.tao@163.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i> E-Mail</a></span></div><div class="cc-license motion-element" itemprop="license"> <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright"> &copy; <span itemprop="copyrightYear">2020</span><span class="with-love"><i class="fa fa-user"></i></span> <span class="author" itemprop="copyrightHolder">陶文寅</span></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.0.0</div> <span class="post-meta-divider">|</span><div class="theme-info">主题 – <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.5.0</div></div></footer></div><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script><script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script></body></html>