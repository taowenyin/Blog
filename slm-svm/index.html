<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 4.0.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="alternate" href="/atom.xml" title="BearCoding" type="application/atom+xml"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Gemini",version:"7.5.0",exturl:!1,sidebar:{position:"left",display:"post",offset:12,onmobile:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!1,scrollpercent:!1},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,algolia:{appID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},path:"search.xml",motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},translation:{copy_button:"复制",copy_success:"复制成功",copy_failure:"复制失败"},sidebarPadding:40}</script><meta name="description" content="支持向量机学习方法包含构建由简至繁的模型，分别是线性可分支持向量机、线性支持向量机，以及非线性支持向量机。简单模型是复杂模型的基础，也是复杂模型的特殊情况。 当训练数据线性可分时，通过硬间隔最大化，学习一个线性的分类器，即线性可分支持向量机，又称为硬间隔支持向量机。 当训练数据近似线性可分时，通过软间隔最大化，学习一个线性的分类器，即线性支持向量机，又称为软间隔支持向量机。 当训练数据线性不可分时"><meta property="og:type" content="article"><meta property="og:title" content="S7-支持向量机"><meta property="og:url" content="bearcoding.cn&#x2F;slm-svm&#x2F;index.html"><meta property="og:site_name" content="BearCoding"><meta property="og:description" content="支持向量机学习方法包含构建由简至繁的模型，分别是线性可分支持向量机、线性支持向量机，以及非线性支持向量机。简单模型是复杂模型的基础，也是复杂模型的特殊情况。 当训练数据线性可分时，通过硬间隔最大化，学习一个线性的分类器，即线性可分支持向量机，又称为硬间隔支持向量机。 当训练数据近似线性可分时，通过软间隔最大化，学习一个线性的分类器，即线性支持向量机，又称为软间隔支持向量机。 当训练数据线性不可分时"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="&#x2F;slm-svm&#x2F;nonlinear.png"><meta property="og:updated_time" content="2020-08-06T14:44:32.213Z"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="&#x2F;slm-svm&#x2F;nonlinear.png"><link rel="canonical" href="bearcoding.cn/slm-svm/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,isPage:!1,isArchive:!1}</script><title>S7-支持向量机 | BearCoding</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-meta"><div><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">BearCoding</span><span class="logo-line-after"><i></i></span></a></div></div><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i> 首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i> 关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i> 标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i> 分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i> 归档</a></li><li class="menu-item menu-item-publication"><a href="/categories/%E5%8F%91%E8%A1%A8" rel="section"><i class="fa fa-fw fa-leanpub"></i> 发表</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="site-search"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"> <input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="搜索..." spellcheck="false" type="text" id="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"></div></div><div class="search-pop-overlay"></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><div class="reading-progress-bar"></div><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content"><div class="posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="bearcoding.cn/slm-svm/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.gif"><meta itemprop="name" content="陶文寅"><meta itemprop="description" content=""></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="BearCoding"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> S7-支持向量机</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2020-04-02 19:09:54" itemprop="dateCreated datePublished" datetime="2020-04-02T19:09:54+08:00">2020-04-02</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i></span> <span class="post-meta-item-text">更新于</span> <time title="修改时间：2020-08-06 22:44:32" itemprop="dateModified" datetime="2020-08-06T22:44:32+08:00">2020-08-06</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">统计学习方法</span></a></span></span></div></header><div class="post-body" itemprop="articleBody"><p>支持向量机学习方法包含构建由简至繁的模型，分别是线性可分支持向量机、线性支持向量机，以及非线性支持向量机。简单模型是复杂模型的基础，也是复杂模型的特殊情况。</p><ol type="1"><li>当训练数据线性可分时，通过硬间隔最大化，学习一个线性的分类器，即线性可分支持向量机，又称为硬间隔支持向量机。</li><li>当训练数据近似线性可分时，通过软间隔最大化，学习一个线性的分类器，即线性支持向量机，又称为软间隔支持向量机。</li><li>当训练数据线性不可分时，通过使用核技巧和软间隔最大化，学习一个非线性支持向量机。</li></ol><a id="more"></a><p>核函数（kernel function）表示将输入从输入空间映射到特征空间得到的特征向量得到的特征向量之间的内积。<strong>通过使用核函数可以学习非线性支持向量机，等价于隐式第在高维的特征空间中学习线性支持向量机。核方法是比支持向量机更为一般的机器学习方法。</strong></p><h1 id="线性可分支持向量机与硬间隔最大化">线性可分支持向量机与硬间隔最大化</h1><h2 id="线性可分支持向量机">线性可分支持向量机</h2><p>假设给定一个特征空间上的训练数据集</p><p><span class="math display">\[T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right\}\]</span></p><p>其中，<span class="math inline">\(x_{i} \in \mathcal{X}=\mathbf{R}^{n}\)</span>，<span class="math inline">\(y_{i} \in \mathcal{Y}=\{+1,-1\}\)</span>，<span class="math inline">\(i=1,2, \cdots, N\)</span>。<span class="math inline">\(x_{i}\)</span>为第<span class="math inline">\(i\)</span>个特征向量，<span class="math inline">\(y_{i}\)</span>为<span class="math inline">\(x_{i}\)</span>的类标记。当<span class="math inline">\(y_{i}=+1\)</span>时，<span class="math inline">\(x_{i}\)</span>为正例，当<span class="math inline">\(y_{i}=-1\)</span>时，<span class="math inline">\(x_{i}\)</span>为负例。</p><p><strong>学习的目标是在特征空间找到一个分离超平面，能将实例分到不同的类。</strong> 分离超平面对应于方程<span class="math inline">\(w \cdot x+b=0\)</span>，其中<span class="math inline">\(w\)</span>是法向量，<span class="math inline">\(b\)</span>是截距，可用<span class="math inline">\((w, b)\)</span>表示。分离超平面将特征空间分为两部分，法向量指向的一则为正类，另一侧为负类。</p><h2 id="函数间隔和几何间隔">函数间隔和几何间隔</h2><h3 id="函数间隔">函数间隔</h3><p>一个点距离超平面的远近可以表示为分类预测的置信度。在超平面<span class="math inline">\(w \cdot x+b=0\)</span>确定的情况下，<span class="math inline">\(|w \cdot x+b|\)</span>表示点<span class="math inline">\(x\)</span>距离超平面的远近，而<span class="math inline">\(w \cdot x+b=0\)</span>的符号与<span class="math inline">\(y\)</span>的符号是否一致表示分类是否正确，<strong>可以用<span class="math inline">\(y(w \cdot x+b)\)</span>来表示分类的正确性和置信度，即该函数就是函数间隔。</strong></p><p>对于给定训练数据集<span class="math inline">\(T\)</span>和超平面<span class="math inline">\((w, b)\)</span>，那么超平面关于样本点<span class="math inline">\(\left(x_{i}, y_{i}\right)\)</span>的函数间隔为</p><p><span class="math display">\[\hat{\gamma}_{i}=y_{i}\left(w \cdot x_{i}+b\right)\]</span></p><p>因此超平面关于训练数据<span class="math inline">\(T\)</span>的函数间隔为超平面关于<span class="math inline">\(T\)</span>中所有样本<span class="math inline">\(\left(x_{i}, y_{i}\right)\)</span>的函数间隔的最小值</p><p><span class="math display">\[\hat{\gamma}=\min _{i=1, \cdots, N} \hat{\gamma}_{i}\]</span></p><h3 id="几何间隔">几何间隔</h3><p>由于函数间隔会因为<span class="math inline">\(w\)</span>和<span class="math inline">\(b\)</span>的等比例缩放发生改变，<strong>因此就需要对法向量<span class="math inline">\(w\)</span>进行约束，如<span class="math inline">\(\|w\|=1\)</span>，从而使得间隔固定，该间隔就是几何间隔。</strong></p><p>对于给定训练数据集<span class="math inline">\(T\)</span>和超平面<span class="math inline">\((w, b)\)</span>，那么超平面关于样本点<span class="math inline">\(\left(x_{i}, y_{i}\right)\)</span>的几何间隔为</p><p><span class="math display">\[\gamma_{i}=y_{i}\left(\frac{w}{\|w\|} \cdot x_{i}+\frac{b}{\|w\|}\right)\]</span></p><p>因此超平面关于训练数据<span class="math inline">\(T\)</span>的几何间隔为超平面关于<span class="math inline">\(T\)</span>中所有样本<span class="math inline">\(\left(x_{i}, y_{i}\right)\)</span>的几何间隔的最小值</p><p><span class="math display">\[\gamma=\min _{i=1, \cdots, N} \gamma_{i}\]</span></p><p>因此，可以得到函数间隔<span class="math inline">\(\hat{\gamma}\)</span>和几何间隔<span class="math inline">\(\gamma\)</span>关系为：</p><p><span class="math display">\[\gamma=\frac{\hat{\gamma}}{\|w\|}\]</span></p><p>如果<span class="math inline">\(\|w\|=1\)</span>，那么函数间隔和几何间隔相同，如果<span class="math inline">\(w\)</span>和<span class="math inline">\(b\)</span>的等比例缩放，那么函数间隔也会按比例改变，但是几何间隔不变。</p><h2 id="间隔最大化">间隔最大化</h2><h3 id="最大间隔分离超平面">最大间隔分离超平面</h3><p>最大间隔分离超平面可以表示为下面的约束问题：</p><p><span class="math display">\[\begin{aligned} \underset{w,b}{max} &amp; \quad \gamma \\ \text { s.t. } &amp; \quad y_{i}\left(\frac{w}{\|w\|} \cdot x_{i}+\frac{b}{\|w\|}\right) \geqslant \gamma, i=1,2, \cdots, N \end{aligned}\]</span></p><p>其中约束条件表示超平面<span class="math inline">\((w, b)\)</span>关于每个训练样本点的几何间隔至少为<span class="math inline">\(\gamma\)</span>。如果改写为函数间隔，即</p><p><span class="math display">\[\begin{aligned} \underset{w,b}{max} &amp; \quad \frac{\hat{\gamma}}{\|w\|} \\ \text { s.t. } &amp; \quad y_{i}\left(w \cdot x_{i}+b\right) \geqslant \hat{\gamma}, i=1,2, \cdots, N \end{aligned}\]</span></p><p>当<span class="math inline">\(\hat{\gamma}=1\)</span>，则优化问题就变为</p><p><span class="math display">\[\begin{aligned} \underset{w,b}{max} &amp; \quad \frac{1}{\|w\|} \\ \text { s.t. } &amp; \quad y_{i}\left(w \cdot x_{i}+b\right) - 1 \geqslant 0, i=1,2, \cdots, N \end{aligned}\]</span></p><p>由因为最大化<span class="math inline">\(\frac{1}{\|w\|}\)</span>就等价于最小化<span class="math inline">\(\frac{1}{2}\|w\|^{2}\)</span>，因此，最大化优化问题就变成了最小化优化问题。</p><p><span class="math display">\[\begin{aligned} \underset{w,b}{min} &amp; \quad \frac{1}{2}\|w\|^{2} \\ \text { s.t. } &amp; \quad y_{i}\left(w \cdot x_{i}+b\right) - 1 \geqslant 0, i=1,2, \cdots, N \end{aligned}\]</span></p><p>上面的优化问题就是一个凸二次规划问题。求出约束最优化问题的解<span class="math inline">\(w^{*}\)</span>和<span class="math inline">\(b^{*}\)</span>就可以得到最大间隔超平面<span class="math inline">\(w^{*} \cdot x+b^{*}=0\)</span>和分类决策函数<span class="math inline">\(f(x)=\operatorname{sign}\left(w^{*} \cdot x+b^{*}\right)\)</span>。</p><p><strong>定理：若训练数据集<span class="math inline">\(T\)</span>线性可分，则可将训练数据集中的样本点完全正确分开的最大间隔分离超平面存在且唯一。</strong></p><h3 id="支持向量和间隔边界">支持向量和间隔边界</h3><p><strong>训练数据集的样本点中与分离超平面距离最近的样本点的实例称为支持向量。</strong> 支持向量的约束条件为</p><p><span class="math display">\[y_{i}\left(w \cdot x_{i}+b\right)-1=0\]</span></p><p>当<span class="math inline">\(y_{i}=+1\)</span>的正实例点，则支持向量的超平面为</p><p><span class="math display">\[H_{1}: w \cdot x+b=1\]</span></p><p>当<span class="math inline">\(y_{i}=-1\)</span>的正实例点，则支持向量的超平面为</p><p><span class="math display">\[H_{2}: w \cdot x+b=-1\]</span></p><p><span class="math inline">\(H_{1}\)</span>和<span class="math inline">\(H_{2}\)</span>平行，并没有实例点落在他们之间。<span class="math inline">\(H_{1}\)</span>和<span class="math inline">\(H_{2}\)</span>之间称为间隔。由于间隔至于法向量<span class="math inline">\(w\)</span>相关，因此当<span class="math inline">\(\hat{\gamma}=1\)</span>时，间隔为<span class="math inline">\(\frac{2}{\|w\|}\)</span>。</p><h2 id="学习的对偶算法">学习的对偶算法</h2><p><strong>采用对偶算法的优点：对偶问题往往更容易求解，同时对偶算法自然引入核函数，可以推广到非线性分类问题。</strong></p><p>构建拉格朗日函数。对不等式约束引入拉格朗日乘子<span class="math inline">\(\alpha_{i} \geqslant 0\)</span>，<span class="math inline">\(i=1,2, \cdots, N\)</span>，则根据约束</p><p><span class="math display">\[\begin{aligned} \underset{w,b}{min} &amp; \quad \frac{1}{2}\|w\|^{2} \\ \text { s.t. } &amp; \quad 1 - y_{i}\left(w \cdot x_{i}+b\right) \leqslant 0, i=1,2, \cdots, N \end{aligned}\]</span></p><p>可以得到</p><p><span class="math display">\[\begin{aligned} L(w, b, \alpha) &amp;=\frac{1}{2}\|w\|^{2} + \sum_{i=1}^{N} \left [ \alpha_{i} - \alpha_{i} y_{i}\left(w \cdot x_{i}+b\right) \right ] \\ &amp;=\frac{1}{2}\|w\|^{2} - \sum_{i=1}^{N} \left [ \alpha_{i} y_{i}\left(w \cdot x_{i}+b\right) - \alpha_{i} \right ] \\ &amp;=\frac{1}{2}\|w\|^{2}-\sum_{i=1}^{N} \alpha_{i} y_{i}\left(w \cdot x_{i}+b\right)+\sum_{i=1}^{N} \alpha_{i} \end{aligned}\]</span></p><p>其中，<span class="math inline">\(\alpha=\left(\alpha_{1}, \alpha_{2}, \cdots, \alpha_{N}\right)^{\mathrm{T}}\)</span>为拉格朗日乘子向量。根据拉格朗日对偶性，原始问题的对偶问题就是极大极小问题：</p><p><span class="math display">\[\max _{\alpha} \min _{w, b} L(w, b, \alpha)\]</span></p><p>因此是对<span class="math inline">\(L(w, b, \alpha)\)</span>求<span class="math inline">\(w\)</span>、<span class="math inline">\(b\)</span>的极小值，再对<span class="math inline">\(\alpha\)</span>求极大值。</p><blockquote><p>第一步：求<span class="math inline">\(\underset{w, b}{min}L(w, b, \alpha)\)</span></p></blockquote><p>拉格朗日函数<span class="math inline">\(L(w, b, \alpha)\)</span>分别对<span class="math inline">\(w\)</span>和<span class="math inline">\(b\)</span>求偏导，并令其等于0。</p><p><span class="math display">\[\nabla_{w} L(w, b, \alpha)=w-\sum_{i=1}^{N} \alpha_{i} y_{i} x_{i}=0\]</span></p><p><span class="math display">\[\nabla_{b} L(w, b, \alpha)=-\sum_{i=1}^{N} \alpha_{i} y_{i}=0\]</span></p><p>得</p><p><span class="math display">\[w=\sum_{i=1}^{N} \alpha_{i} y_{i} x_{i}\]</span></p><p><span class="math display">\[\sum_{i=1}^{N} \alpha_{i} y_{i}=0\]</span></p><p>因此代入拉格朗日函数得到</p><p><span class="math display">\[\begin{aligned} L(w, b, \alpha) &amp;=\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} y_{i}\left(\left(\sum_{j=1}^{N} \alpha_{j} y_{j} x_{j}\right) \cdot x_{i}+b\right)+\sum_{i=1}^{N} \alpha_{i} \\ &amp;=-\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)+\sum_{i=1}^{N} \alpha_{i} \end{aligned}\]</span></p><p>所以</p><p><span class="math display">\[\min _{w, b} L(w, b, \alpha)=-\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)+\sum_{i=1}^{N} \alpha_{i}\]</span></p><blockquote><p>第二步：求<span class="math inline">\(\underset{w, b}{min}L(w, b, \alpha)\)</span>对<span class="math inline">\(\alpha\)</span>的极大值，即是对偶问题</p></blockquote><p><span class="math display">\[\begin{aligned} \underset{\alpha}{max} &amp; \quad -\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)+\sum_{i=1}^{N} \alpha_{i} \\ \text { s.t. } &amp; \quad \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \\ &amp; \quad \alpha_{i} \geqslant 0, \quad i=1,2, \cdots, N \end{aligned}\]</span></p><p>对上式由求极大值转化为求极小值，可以得到</p><p><span class="math display">\[\begin{aligned} \underset{\alpha}{min} &amp; \quad \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} \\ \text { s.t. } &amp; \quad \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \\ &amp; \quad \alpha_{i} \geqslant 0, \quad i=1,2, \cdots, N \end{aligned}\]</span></p><p>此时就把原始问题转化为了求极小值的对偶问题，得到<span class="math inline">\(\alpha^{*}=\left(\alpha_{1}^{*}, \alpha_{2}^{*}, \cdots, \alpha_{l}^{*}\right)^{\mathrm{T}}\)</span>的最优解。从而解出<span class="math inline">\(w^{*}\)</span>和<span class="math inline">\(b^{*}\)</span>：</p><p><span class="math display">\[w^{*}=\sum_{i=1}^{N} \alpha_{i}^{*} y_{i} x_{i}\]</span></p><p><span class="math display">\[b^{*}=y_{j}-\sum_{i=1}^{N} \alpha_{i}^{*} y_{i}\left(x_{i} \cdot x_{j}\right)\]</span></p><p>因此分离超平面可以写为</p><p><span class="math display">\[\sum_{i=1}^{N} \alpha_{i}^{*} y_{i}\left(x \cdot x_{i}\right)+b^{*}=0\]</span></p><p>因此分类决策函数就可以写成</p><p><span class="math display">\[f(x)=\operatorname{sign}\left(\sum_{i=1}^{N} \alpha_{i}^{*} y_{i}\left(x \cdot x_{i}\right)+b^{*}\right)\]</span></p><p>该式称为线性可分支持向量机的对偶形式。</p><h1 id="线性支持向量机与软间隔最大化">线性支持向量机与软间隔最大化</h1><h2 id="线性支持向量机">线性支持向量机</h2><p>线性支持向量机主要用来解决线性不可分问题，即在训练数据集中有一些特异点，将这些特异点去除后，剩下的样本点组成的数据集线性可分。为了解决这个问题，对每个样本点<span class="math inline">\(\left(x_{i}, y_{i}\right)\)</span>引入一个松弛变量<span class="math inline">\(\xi_{i} \geqslant 0\)</span>，使得函数间隔加上松弛变量大于等于1。此时，约束条件变为</p><p><span class="math display">\[y_{i}\left(w \cdot x_{i}+b\right) \geqslant 1-\xi_{i}\]</span></p><p>由于引入了松弛变量，因此目标函数就需要支付一个代价<span class="math inline">\(\xi_{i}\)</span>，因此原先的目标函数<span class="math inline">\(\frac{1}{2}\|w\|^{2}\)</span>就变为</p><p><span class="math display">\[\frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{N} \xi_{i}\]</span></p><p>其中，<span class="math inline">\(C&gt;0\)</span>称为惩罚参数，<span class="math inline">\(C\)</span>值大时对误分类的惩罚增大，<span class="math inline">\(C\)</span>值小时对误分类的惩罚减小。该目标函数包含两层含义：使<span class="math inline">\(\frac{1}{2}\|w\|^{2}\)</span>尽量小，即间隔尽可能大。同时误分类点的个数尽可能小，<span class="math inline">\(C\)</span>是调和二者的系数。</p><p>相对于硬间隔最大化，该目标函数为软间隔最大化。同时，线性支持向量机的学习问题就变成下面的凸二次规划问题（原始问题）：</p><p><span class="math display">\[\begin{aligned} \underset{w,b,\xi}{min} &amp; \quad \frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{N} \xi_{i} \\ \text { s.t. } &amp; \quad y_{i}\left(w \cdot x_{i}+b\right) \geqslant 1-\xi_{i}, i=1,2, \cdots, N \\ &amp; \quad \xi_{i} \geqslant 0, \quad i=1,2, \cdots, N \end{aligned}\]</span></p><h2 id="学习的对偶算法-1">学习的对偶算法</h2><p>构建拉格朗日函数。对不等式约束引入拉格朗日乘子<span class="math inline">\(\alpha_{i} \geqslant 0\)</span>，<span class="math inline">\(\mu_{i} \geqslant 0\)</span>，<span class="math inline">\(i=1,2, \cdots, N\)</span>，则根据约束</p><p><span class="math display">\[\begin{aligned} \underset{w,b,\xi}{min} &amp; \quad \frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{N} \xi_{i} \\ \text { s.t. } &amp; \quad 1-\xi_{i} - y_{i}\left(w \cdot x_{i}+b\right) \leqslant 0, i=1,2, \cdots, N \\ &amp; \quad -\xi_{i} \leqslant 0, \quad i=1,2, \cdots, N \end{aligned}\]</span></p><p>可以得到</p><p><span class="math display">\[\begin{aligned} L(w, b, \xi, \alpha, \mu) &amp;=\frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{N} \xi_{i} + \sum_{i=1}^{N} \left [ \alpha_{i}-\alpha_{i}\xi_{i} - \alpha_{i}y_{i}\left(w \cdot x_{i}+b\right) \right ]-\sum_{i=1}^{N} \mu_{i} \xi_{i} \\ &amp;=\frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{N} \xi_{i} - \sum_{i=1}^{N} \alpha_{i} \left [ y_{i}\left(w \cdot x_{i}+b\right) - 1 +\xi_{i} \right ]-\sum_{i=1}^{N} \mu_{i} \xi_{i} \end{aligned}\]</span></p><p>根据拉格朗日对偶性，原始问题的对偶问题就是极大极小问题：</p><p><span class="math display">\[\max _{\alpha} \min _{w, b} L(w, b, \xi, \alpha, \mu)\]</span></p><p>因此是对<span class="math inline">\(L(w, b, \xi, \alpha, \mu)\)</span>求<span class="math inline">\(w\)</span>、<span class="math inline">\(b\)</span>、<span class="math inline">\(\xi\)</span>的极小值。</p><blockquote><p>第一步：求<span class="math inline">\(\underset{w, b}{min}L(w, b, \xi, \alpha, \mu)\)</span></p></blockquote><p>拉格朗日函数<span class="math inline">\(L(w, b, \xi, \alpha, \mu)\)</span>分别对<span class="math inline">\(w\)</span>、<span class="math inline">\(b\)</span>、<span class="math inline">\(\xi\)</span>求偏导，并令其等于0。</p><p><span class="math display">\[\nabla_{w} L(w, b, \xi, \alpha, \mu)=w-\sum_{i=1}^{N} \alpha_{i} y_{i} x_{i}=0\]</span></p><p><span class="math display">\[\nabla_{b} L(w, b, \xi, \alpha, \mu)=-\sum_{i=1}^{N} \alpha_{i} y_{i}=0\]</span></p><p><span class="math display">\[\nabla_{\xi_{i}} L(w, b, \xi, \alpha, \mu)=C-\alpha_{i}-\mu_{i}=0\]</span></p><p>得</p><p><span class="math display">\[w=\sum_{i=1}^{N} \alpha_{i} y_{i} x_{i}\]</span></p><p><span class="math display">\[\sum_{i=1}^{N} \alpha_{i} y_{i}=0\]</span></p><p><span class="math display">\[C-\alpha_{i}-\mu_{i}=0\]</span></p><p>因此代入拉格朗日函数得到</p><p><span class="math display">\[\min _{w, b, \xi} L(w, b, \xi, \alpha, \mu)=-\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)+\sum_{i=1}^{N} \alpha_{i}\]</span></p><blockquote><p>第二步：求<span class="math inline">\(\underset{w, b, \xi}{min} L(w, b, \xi, \alpha, \mu)\)</span>对<span class="math inline">\(\alpha\)</span>的极大值，即是对偶问题</p></blockquote><p><span class="math display">\[\begin{aligned} \underset{\alpha}{max} &amp; \quad -\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)+\sum_{i=1}^{N} \alpha_{i} \\ \text { s.t. } &amp; \quad \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \\ &amp; \quad C-\alpha_{i}-\mu_{i}=0 \\ &amp; \quad \alpha_{i} \geqslant 0, \quad i=1,2, \cdots, N \\ &amp; \quad \mu_{i} \geqslant 0, \quad i=1,2, \cdots, N \end{aligned}\]</span></p><p>将约束进行变换，消去<span class="math inline">\(\mu_{i}\)</span>，从而只留下<span class="math inline">\(\alpha_{i}\)</span>，因此对偶问题就写为</p><p><span class="math display">\[\begin{aligned} \underset{\alpha}{max} &amp; \quad -\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)+\sum_{i=1}^{N} \alpha_{i} \\ \text { s.t. } &amp; \quad \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \\ &amp; \quad 0 \leqslant \alpha_{i} \leqslant C \end{aligned}\]</span></p><p>此时就把原始问题转化为了求极小值的对偶问题，得到<span class="math inline">\(\alpha^{*}=\left(\alpha_{1}^{*}, \alpha_{2}^{*}, \cdots, \alpha_{l}^{*}\right)^{\mathrm{T}}\)</span>的最优解。从而解出<span class="math inline">\(w^{*}\)</span>和<span class="math inline">\(b^{*}\)</span>：</p><p><span class="math display">\[w^{*}=\sum_{i=1}^{N} \alpha_{i}^{*} y_{i} x_{i}\]</span></p><p><span class="math display">\[b^{*}=y_{j}-\sum_{i=1}^{N} \alpha_{i}^{*} y_{i}\left(x_{i} \cdot x_{j}\right)\]</span></p><p>因此分离超平面可以写为</p><p><span class="math display">\[\sum_{i=1}^{N} \alpha_{i}^{*} y_{i}\left(x \cdot x_{i}\right)+b^{*}=0\]</span></p><p>因此分类决策函数就可以写成</p><p><span class="math display">\[f(x)=\operatorname{sign}\left(\sum_{i=1}^{N} \alpha_{i}^{*} y_{i}\left(x \cdot x_{i}\right)+b^{*}\right)\]</span></p><p>该式称为线性可分支持向量机的对偶形式。</p><h2 id="支持向量没懂">支持向量（没懂）</h2><table><thead><tr class="header"><th style="text-align:center"><span class="math inline">\(\alpha_{i}^{*}\)</span></th><th style="text-align:center"><span class="math inline">\(\xi_{i}\)</span></th><th style="text-align:center"><span class="math inline">\(x_{i}\)</span></th></tr></thead><tbody><tr class="odd"><td style="text-align:center"><span class="math inline">\(\alpha_{i}^{*}&lt;C\)</span></td><td style="text-align:center"><span class="math inline">\(\xi_{i}=0\)</span></td><td style="text-align:center"><span class="math inline">\(x_{i}\)</span>落在间隔边界上</td></tr><tr class="even"><td style="text-align:center"><span class="math inline">\(\alpha_{i}^{*}=C\)</span></td><td style="text-align:center"><span class="math inline">\(0&lt;\xi_{i}&lt;1\)</span></td><td style="text-align:center">分类正确，<span class="math inline">\(x_{i}\)</span>落在间隔边界与分离超平面之间</td></tr><tr class="odd"><td style="text-align:center"><span class="math inline">\(\alpha_{i}^{*}=C\)</span></td><td style="text-align:center"><span class="math inline">\(\xi_{i}=1\)</span></td><td style="text-align:center"><span class="math inline">\(x_{i}\)</span>落在分离超平面上</td></tr><tr class="even"><td style="text-align:center"><span class="math inline">\(\alpha_{i}^{*}=C\)</span></td><td style="text-align:center"><span class="math inline">\(\xi_{i}&gt;1\)</span></td><td style="text-align:center"><span class="math inline">\(x_{i}\)</span>落在分离超平面误分类一侧</td></tr></tbody></table><h1 id="非线性支持向量机与核函数">非线性支持向量机与核函数</h1><p>给定训练数据集<span class="math inline">\(T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right\}\)</span>，其中<span class="math inline">\(x_{i}\)</span>属于输入空间，<span class="math inline">\(x_{i} \in \mathcal{X}=\mathbf{R}^{n}\)</span>，对应的标记有两类<span class="math inline">\(y_{i} \in \mathcal{Y}=\{-1,+1\}\)</span>，<span class="math inline">\(i=1,2, \cdots, N\)</span>。如果能用<span class="math inline">\(\mathbf{R}^{n}\)</span>中的一个超曲面将正负例正确分开，则称该问题为非线性可分问题。</p><p><strong>要解决非线性可分问题，就是将非线性问题转化为线性问题，通过解变换后的线性问题方法来解决原来的非线性问题。</strong></p> <img src="/slm-svm/nonlinear.png" class title="非线性分类问题与核技巧示例"><p>图1：非线性分类问题与核技巧示例</p><p>假设<span class="math inline">\(\mathcal{X} \subset \mathbf{R}^{2}\)</span>，<span class="math inline">\(x=\left(x^{(1)}, x^{(2)}\right)^{\mathrm{T}} \in \mathcal{X}\)</span>，新空间<span class="math inline">\(\mathcal{Z} \subset \mathbf{R}^{2}\)</span>，<span class="math inline">\(z=\left(z^{(1)}, z^{(2)}\right)^{\mathrm{T}} \in \mathcal{Z}\)</span>，定义从原空间到新空间的变换（映射）：</p><p><span class="math display">\[z=\phi(x)=\left(\left(x^{(1)}\right)^{2},\left(x^{(2)}\right)^{2}\right)^{\mathrm{T}}\]</span></p><p>经过变换<span class="math inline">\(z=\phi(x)\)</span>，把原空间<span class="math inline">\(\mathcal{X} \subset \mathbf{R}^{2}\)</span>变换为新空间<span class="math inline">\(\mathcal{Z} \subset \mathbf{R}^{2}\)</span>，原空间中的点就变换为新空间中的点，原空间中的椭圆</p><p><span class="math display">\[w_{1}\left(x^{(1)}\right)^{2}+w_{2}\left(x^{(2)}\right)^{2}+b=0\]</span></p><p>变换为新空间中的直线</p><p><span class="math display">\[w_{1} z^{(1)}+w_{2} z^{(2)}+b=0\]</span></p><p>求解非线性分类问题分两步（核技巧）：</p><ol type="1"><li>使用一个变换将原空间数据映射到新空间。</li><li>在新空间里用线性分类学习方法从训练数据中学习分类模型。</li></ol><p>由于线性支持向量机的对偶问题中，无论目标函数还是决策函数都只涉及输入实例与实例之间的内积<span class="math inline">\(x_{i} \cdot x_{j}\)</span>，因此可以使用核函数<span class="math inline">\(K\left(x_{i}, x_{j}\right)=\phi\left(x_{i}\right) \cdot \phi\left(x_{j}\right)\)</span>来代替，此时的对偶目标函数为</p><p><span class="math display">\[W(\alpha)=\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} K\left(x_{i}, x_{j}\right)-\sum_{i=1}^{N} \alpha_{i}\]</span></p><p>分类决策函数中的内积也可以使用核函数代替</p><p><span class="math display">\[\begin{aligned} f(x) &amp;=\operatorname{sign}\left(\sum_{i=1}^{N_{s}} a_{i}^{*} y_{i} \phi\left(x_{i}\right) \cdot \phi(x)+b^{*}\right) \\ &amp;=\operatorname{sign}\left(\sum_{i=1}^{N_{s}} a_{i}^{*} y_{i} K\left(x_{i}, x\right)+b^{*}\right) \end{aligned}\]</span></p><p>在实际应用中，直接计算<span class="math inline">\(K(x, z)\)</span>比较容易，而通过<span class="math inline">\(\phi(x)\)</span>和<span class="math inline">\(\phi(z)\)</span>计算<span class="math inline">\(K(x, z)\)</span>比较难。</p><p><span class="math display">\[K(x, z)=\phi(x) \cdot \phi(z)\]</span></p><p>由于学习过程是隐式的在特征空间进行，并不需要显示的定义特征空间和映射函数，所以该方法称为核技巧。</p><h2 id="正定核">正定核</h2><p><span class="math inline">\(K(x, z)\)</span>为正定核函数的充要条件是<span class="math inline">\(x_{i} \in \mathcal{X}\)</span>，<span class="math inline">\(i=1,2, \cdots, m\)</span>，<span class="math inline">\(K(x, z)\)</span>对应的Gram矩阵</p><p><span class="math display">\[K=\left[K\left(x_{i}, x_{j}\right)\right]_{m \times m}\]</span></p><p>是半正定矩阵。</p><p>等价于设<span class="math inline">\(\mathcal{X} \subset \mathbf{R}^{n}\)</span>，<span class="math inline">\(K(x, z)\)</span>是定义在<span class="math inline">\(\mathcal{X} \times \mathcal{X}\)</span>上的对称函数，如果对任意<span class="math inline">\(x_{i} \in \mathcal{X}\)</span>，<span class="math inline">\(i=1,2, \cdots, m\)</span>，<span class="math inline">\(K(x, z)\)</span>对应的Gram矩阵</p><p><span class="math display">\[K=\left[K\left(x_{i}, x_{j}\right)\right]_{m \times m}\]</span></p><p>是半正定矩阵，那么称<span class="math inline">\(K(x, z)\)</span>是正定核。</p><h2 id="常用核函数">常用核函数</h2><ol type="1"><li>多项式核函数（polynomial kernel function）</li></ol><p><span class="math display">\[K(x, z)=(x \cdot z+1)^{p}\]</span></p><ol start="2" type="1"><li>高斯核函数（Gaussian kernel function）</li></ol><p><span class="math display">\[K(x, z)=\exp \left(-\frac{\|x-z\|^{2}}{2 \sigma^{2}}\right)\]</span></p><ol start="3" type="1"><li>字符串核函数（string kernel function）<strong>没懂</strong></li></ol><p><span class="math display">\[\begin{aligned} k_{n}(s, t) &amp;=\sum_{u \in \Sigma^{n}}\left[\phi_{n}(s)\right]_{u}\left[\phi_{n}(t)\right]_{u} \\ &amp;=\sum_{u \in \Sigma^{n}} \sum_{(i, j): s(i)=t(j)=u} \lambda^{l(i)} \lambda^{l(j)} \end{aligned}\]</span></p><h1 id="smo序列最小最优化算法没懂">SMO序列最小最优化算法（<strong>没懂</strong>）</h1><p>SMO（序列最小最优化）算法要解如下凸二次规划的对偶问题</p><p><span class="math display">\[\begin{aligned} \underset{\alpha}{max} &amp; \quad -\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}K(x, z)+\sum_{i=1}^{N} \alpha_{i} \\ \text { s.t. } &amp; \quad \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \\ &amp; \quad 0 \leqslant \alpha_{i} \leqslant C, \quad i=1,2, \cdots, N \end{aligned}\]</span></p><h1 id="工程实践">工程实践</h1><h2 id="函数原型">函数原型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sklearn.svm.SVC(</span><br><span class="line">            C=<span class="number">1.0</span>, kernel=<span class="string">'rbf'</span>, degree=<span class="number">3</span>, gamma=<span class="string">'scale'</span>,</span><br><span class="line">            coef0=<span class="number">0.0</span>, shrinking=<span class="literal">True</span>, probability=<span class="literal">False</span>,</span><br><span class="line">            tol=<span class="number">1e-3</span>, cache_size=<span class="number">200</span>, class_weight=<span class="literal">None</span>,</span><br><span class="line">            verbose=<span class="literal">False</span>, max_iter=<span class="number">-1</span>, decision_function_shape=<span class="string">'ovr'</span>,</span><br><span class="line">            break_ties=<span class="literal">False</span>, random_state=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><ol type="1"><li>C（类型：flot，默认值：1.0，<code>超参：是</code>）：正则化参数，即惩罚项的系数。</li><li>kernel（类型：str，默认值：rbf，<code>超参：是</code>）：核函数的选择。scikit-learn中支持4种核函数，分别是：<ul><li>linear：线性核函数<span class="math inline">\(K\left(x, x^{\prime}\right)=\left\langle x, x^{\prime}\right\rangle\)</span></li><li>poly：多项式核函数<span class="math inline">\(K\left(x, x^{\prime}\right)=\left(\gamma\left\langle x, x^{\prime}\right\rangle+ r\right)^{d}\)</span></li><li>rbf：径向核函数/高斯核函数<span class="math inline">\(K\left(x, x^{\prime}\right)=\exp \left(-\gamma\left\|x-x^{\prime}\right\|^{2}\right)\)</span></li><li>sigmoid：sigmoid核函数<span class="math inline">\(K\left(x, x^{\prime}\right)=\tanh \left(\gamma\left\langle x, x^{\prime}\right\rangle+ r\right)\)</span></li><li>precomputed：核矩阵，使用自定义转换矩阵</li></ul></li><li>degree（类型：int，默认值：3，<code>超参：是</code>）：设置多项式的阶数。<strong>只有kernel函数为poly才有效，即公式中的<span class="math inline">\(d\)</span>。</strong></li><li>gamma（类型：int或str，默认值：scale，<code>超参：是</code>）：核函数的系数。<strong>只有kernel函数为poly、rbf、sigmod才有效，即公式中的<span class="math inline">\(\gamma\)</span>。</strong><ul><li>当gamma为scale时，<span class="math inline">\(\text {gamma}=\frac{1}{\text {n_features} \times \text {X.var()}}\)</span></li><li>当gamma为auto时，<span class="math inline">\(\text {gamma}=\frac{1}{\text {n_features}}\)</span></li></ul></li><li>coef0（类型：float，默认值：0.0，<code>超参：是</code>）：核函数的独立项。<strong>只有kernel函数为poly、sigmod才有效，即公式中的<span class="math inline">\(r\)</span>。</strong></li><li>shrinking（类型：boolean，默认值：True）：是否使用启发式的最优算法。</li><li>probability（类型：boolean，默认值：False）：是否使用概率估计。</li><li>tol（类型：float，默认值：1e-3）：停止训练的误差值大小。</li><li>cache_size（类型：float，默认值：200）：训练时所需要的内存大小，单位MB。</li><li>class_weight（类型：dict或str，默认值：None）：给每个类分别设置不同的惩罚参数C，即<span class="math inline">\(\text {class_weight}\left [ i\right ] \times C\)</span>。如果参数是“balanced”，那么类别权重自动调整为与输入数据中的类频率成反比的权重<span class="math inline">\(\frac{\text{n_samples}}{\text{n_classes} \times \text{np.bincount}\left ( y\right )}\)</span>。</li><li>verbose（类型：boolean，默认值：False）：是否启用详细输出。</li><li>max_iter（类型：int，默认值：-1）：最大迭代次数，如果为-1，那么不受限制。</li><li>decision_function_shape（类型：str，默认值：ovr）：决策函数类型。<ul><li>ovo：返回形状为<span class="math inline">\(\left (\text{n_samples}, \frac{\text{n_classes} \times \left ( \text{n_classes} - 1\right )}{2} \right )\)</span>的决策函数，该决策函数一般用于多类别策略。</li><li>ovr：返回形状为<span class="math inline">\(\left (\text{n_samples}, \text{n_classes} \right )\)</span>的决策函数。</li></ul></li><li>break_ties（类型：boolean，默认值：False）：当该值为True时，decision_function_shape为ovr，且类别数量大于2时，预测结果将根据decision_function_shape的置信度来来打破平局，否则就返回所有类别中的第一个。要注意的是打破平局相比简单的预测所需代价更高。</li><li>random_state（类型：int，默认值：None）：随机数种子，如果不设置则默认为<code>np.random</code>。</li></ol><h2 id="程序流程">程序流程</h2><ol type="1"><li>将原始数据转化为SVM算法软件或包所能识别的数据格式；</li><li>将数据标准化；（防止样本中不同特征数值大小相差较大影响分类器性能）</li><li>不知使用什么核函数，考虑使用RBF；</li><li>利用交叉验证网格搜索寻找最优参数（C, γ）；（交叉验证防止过拟合，网格搜索在指定范围内寻找最优参数）</li><li>使用最优参数来训练模型；</li><li>测试。</li></ol><h2 id="核函数选择">核函数选择</h2><ol type="1"><li>样本数量远小于特征数量：这种情况，利用情况利用linear核效果会高于RBF核。</li><li>样本数量和特征数量一样大：linear核合适，且速度也更快。</li><li>样本数量远大于特征数量：非线性核RBF等合适。</li></ol><h2 id="gridsearchcv小技巧">GridSearchCV小技巧</h2><p>网格大小如果设置范围大且步长密集的话难免耗时，但是不这样的话又可能找到的参数不是很好，针对这解决方法是，先在大范围，大步长的粗糙网格内寻找参数。在找到的参数左右在设置精细步长找寻最优参数比如：</p><ol type="1"><li>一开始寻找范围是<span class="math inline">\(C=2^{-5},2^{-3}, \cdots ,2^{15}\)</span>和<span class="math inline">\(\gamma = 2^{-15},2^{-13}, \cdots ,2^{3}\)</span>，由此找到的最优参数是<span class="math inline">\(\left ( 2^{3}, 2^{-5}\right )\)</span>；</li><li>然后设置更小一点的步长，参数范围变为<span class="math inline">\(C=2^{1},2^{1.25}, \cdots ,2^{5}\)</span>和<span class="math inline">\(\gamma = 2^{−7},2^{−6.75}, \cdots ,2^{-3}\)</span></li></ol><p>这样既可以避免一开始就使用大范围，小步长而导致分类器进行过于多的计算而导致计算时间的增加。</p><h2 id="常用函数">常用函数</h2><ol type="1"><li>样本到分离超平面的距离</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.svm.SVC.decision_function(X)</span><br></pre></td></tr></table></figure><ol start="2" type="1"><li>根据给定的训练数据拟合SVM模型</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.svm.SVC.fit(X, y[, sample_weight=<span class="literal">None</span>])</span><br></pre></td></tr></table></figure><ol start="3" type="1"><li>获取此估算器的参数并以字典形式显示</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.svm.SVC.get_params([deep=<span class="literal">True</span>])</span><br></pre></td></tr></table></figure><ol start="4" type="1"><li>根据测试数据集进行预测</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.svm.SVC.predict(X)</span><br></pre></td></tr></table></figure><ol start="5" type="1"><li>返回给定测试数据和标签的平均精确度</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.svm.SVC.score(X, y)</span><br></pre></td></tr></table></figure><ol start="6" type="1"><li>返回样本的对数概率以及普通概率，注意该函数只有在SVM构建时的probability参数为True时才有效。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sklearn.svm.SVC.predict_log_proba(X_test)</span><br><span class="line">sklearn.svm.SVC.predict_proba(X_test)</span><br></pre></td></tr></table></figure><h1 id="作业">作业</h1><p>1、完成习题7.2：已知正例点<span class="math inline">\(x_{1}=(1,2)^{\top}\)</span>，<span class="math inline">\(x_{2}=(2,3)^{\top}\)</span>，<span class="math inline">\(x_{3}=(3,3)^{\top}\)</span>，负例点<span class="math inline">\(x_{4}=(2,1)^{\top}\)</span>，<span class="math inline">\(x_{5}=(3,2)^{\top}\)</span>，试求最大间隔分离超平面和分类决策函数，并在图上画出分离超平面、间隔边界及支持向量</p><p>解：根据下式</p><p><span class="math display">\[\begin{aligned} \underset{\alpha}{min} &amp; \quad \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} \\ \text { s.t. } &amp; \quad \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \\ &amp; \quad \alpha_{i} \geqslant 0, \quad i=1,2, \cdots, N \end{aligned}\]</span></p><p>可以得到</p><p><span class="math display">\[\begin{aligned} \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} &amp; = \frac{1}{2}\left(5\alpha_{1}^{2}+13\alpha_{2}^{2}+18\alpha_{3}^{2}+5\alpha_{4}^{2}+13\alpha_{5}^{2}+16\alpha_{1}\alpha_{2}+18\alpha_{1}\alpha_{3}-8\alpha_{1}\alpha_{4}-14\alpha_{1}\alpha_{5}+30\alpha_{2}\alpha_{3}-14\alpha_{2}\alpha_{4}-24\alpha_{2}\alpha_{5}-18\alpha_{3}\alpha_{4}-30\alpha_{3}\alpha_{5}-16\alpha_{4}\alpha_{5}\right)-\alpha_{1}-\alpha_{2}-\alpha_{3}-\alpha_{4}-\alpha_{5} \\ \sum_{i=1}^{N} \alpha_{i} y_{i} &amp; = \alpha_{1}+\alpha_{2}+\alpha_{3}-\alpha_{4}-\alpha_{5}=0 \\ &amp; \quad \alpha_{i} \geqslant 0, \quad i=1,2, \cdots, N \end{aligned}\]</span></p><p>把<span class="math inline">\(\alpha_{5} = \alpha_{1}+\alpha_{2}+\alpha_{3}-\alpha_{4}\)</span>代入上式消去<span class="math inline">\(\alpha_{5}\)</span>，再分别对上式<span class="math inline">\(\alpha_{1}\)</span>、<span class="math inline">\(\alpha_{2}\)</span>、<span class="math inline">\(\alpha_{3}\)</span>、<span class="math inline">\(\alpha_{4}\)</span>求偏导，并令其为0</p><p><span class="math display">\[\begin{aligned} \nabla_{\alpha_{1}} L(\alpha_{1}, \alpha_{2},\alpha_{3},\alpha_{4}) &amp; = 4\alpha_{1}+2\alpha_{2}-2\alpha_{4}-2=0 \\ \nabla_{\alpha_{2}} L(\alpha_{1}, \alpha_{2},\alpha_{3},\alpha_{4}) &amp; = 2\alpha_{1}+2\alpha_{2}+\alpha_{3}-2=0 \\ \nabla_{\alpha_{3}} L(\alpha_{1}, \alpha_{2},\alpha_{3},\alpha_{4}) &amp; = \alpha_{2}+\alpha_{3}+\alpha_{4}-2=0 \\ \nabla_{\alpha_{4}} L(\alpha_{1}, \alpha_{2},\alpha_{3},\alpha_{4}) &amp; = -2\alpha_{1}+\alpha_{3}+2\alpha_{4}=0 \end{aligned}\]</span></p><p>由于上式无解，所以通过观测，由于<span class="math inline">\(x_{2}\)</span>不在间隔边界，因此<span class="math inline">\(\alpha_{2}=0\)</span>，因此上式的求导就变为</p><p><span class="math display">\[\begin{aligned} \nabla_{\alpha_{1}} L(\alpha_{1}, \alpha_{2},\alpha_{3},\alpha_{4}) &amp; = 4\alpha_{1}+2\alpha_{2}-2\alpha_{4}-2=0 \\ \nabla_{\alpha_{3}} L(\alpha_{1}, \alpha_{2},\alpha_{3},\alpha_{4}) &amp; = \alpha_{2}+\alpha_{3}+\alpha_{4}-2=0 \\ \nabla_{\alpha_{4}} L(\alpha_{1}, \alpha_{2},\alpha_{3},\alpha_{4}) &amp; = -2\alpha_{1}+\alpha_{3}+2\alpha_{4}=0 \end{aligned}\]</span></p><p>代入<span class="math inline">\(\alpha_{2}=0\)</span>得到</p><p><span class="math display">\[\begin{aligned} \nabla_{\alpha_{1}} L(\alpha_{1}, \alpha_{2},\alpha_{3},\alpha_{4}) &amp; = 2\alpha_{1}-\alpha_{4}-1=0 \\ \nabla_{\alpha_{3}} L(\alpha_{1}, \alpha_{2},\alpha_{3},\alpha_{4}) &amp; = \alpha_{3}+\alpha_{4}-2=0 \\ \nabla_{\alpha_{4}} L(\alpha_{1}, \alpha_{2},\alpha_{3},\alpha_{4}) &amp; = -2\alpha_{1}+\alpha_{3}+2\alpha_{4}=0 \end{aligned}\]</span></p><p>由于上式依然无解，因此分三种情况进行讨论，分别是：</p><blockquote><p>情况1：<span class="math inline">\(\alpha_{1}=0,\alpha_{2}=0\)</span></p></blockquote><p>求得<span class="math inline">\(\alpha_{1}=0\)</span>、<span class="math inline">\(\alpha_{2}=0\)</span>、<span class="math inline">\(\alpha_{3}=2\)</span>、<span class="math inline">\(\alpha_{4}=0\)</span>、<span class="math inline">\(\alpha_{5}=2\)</span></p><blockquote><p>情况2：<span class="math inline">\(\alpha_{3}=0,\alpha_{2}=0\)</span></p></blockquote><p>求得<span class="math inline">\(\alpha_{1}=1\)</span>、<span class="math inline">\(\alpha_{2}=0\)</span>、<span class="math inline">\(\alpha_{3}=0\)</span>、<span class="math inline">\(\alpha_{4}=1\)</span>、<span class="math inline">\(\alpha_{5}=0\)</span></p><blockquote><p>情况3：<span class="math inline">\(\alpha_{4}=0,\alpha_{2}=0\)</span></p></blockquote><p>求得<span class="math inline">\(\alpha_{1}=0.5\)</span>、<span class="math inline">\(\alpha_{2}=0\)</span>、<span class="math inline">\(\alpha_{3}=2\)</span>、<span class="math inline">\(\alpha_{4}=0\)</span>、<span class="math inline">\(\alpha_{5}=2.5\)</span></p><p>把所有解分别代入，求最小值</p><p><span class="math display">\[L(\alpha)=\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i}\]</span></p><p>求得</p><p><span class="math inline">\(L_{1}(\alpha)=-2\)</span>，<span class="math inline">\(L_{2}(\alpha)=-1\)</span>，<span class="math inline">\(L_{3}(\alpha)=-2.5\)</span></p><p>所以<span class="math inline">\(L_{3}(\alpha)\)</span>最小，所以<span class="math inline">\(\alpha\)</span>的最优解为<span class="math inline">\(\alpha_{1}=0.5\)</span>、<span class="math inline">\(\alpha_{2}=0\)</span>、<span class="math inline">\(\alpha_{3}=2\)</span>、<span class="math inline">\(\alpha_{4}=0\)</span>、<span class="math inline">\(\alpha_{5}=2.5\)</span>。</p><p>再把所有<span class="math inline">\(\alpha\)</span>代入下式，可以求得<span class="math inline">\(w^{*}\)</span>和<span class="math inline">\(b^{*}\)</span></p><p><span class="math display">\[w^{*}=\sum_{i=1}^{N} \alpha_{i}^{*} y_{i} x_{i}=\left(-1,2\right)\]</span></p><p><span class="math display">\[b^{*}=y_{j}-\sum_{i=1}^{N} \alpha_{i}^{*} y_{i}\left(x_{i} \cdot x_{j}\right)=-2\]</span></p><p>所以分离超平面为</p><p><span class="math display">\[-x^{(1)}+2 x^{(2)}-2=0\]</span></p><p>决策函数为</p><p><span class="math display">\[f(x)=\operatorname{sign}\left(-x^{(1)}+2 x^{(2)}-2\right)\]</span></p><p>所以，<span class="math inline">\(x_{1}=(1,2)^{\top}\)</span>，<span class="math inline">\(x_{3}=(3,3)^{\top}\)</span>，<span class="math inline">\(x_{5}=(3,2)^{\top}\)</span>是支持向量。</p><p>2、完成习题7.3：线性支持向量机还可以定义成以下形式：</p><p><span class="math display">\[\begin{aligned} \underset{w,b,\xi}{min} &amp; \quad \frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{N} \xi_{i}^{2} \\ \text { s.t. } &amp; \quad y_{i}\left(w \cdot x_{i}+b\right) \geqslant 1-\xi_{i}, i=1,2, \cdots, N \\ &amp; \quad \xi_{i} \geqslant 0, \quad i=1,2, \cdots, N \end{aligned}\]</span></p><p>试求其对偶形式。</p><p>3、试调用sklearn.svm中的SVC模块求解习题7.2，长时改变参数，如C、kernel，比较结果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment"># 计算开始时间</span></span><br><span class="line">    star = time.time()</span><br><span class="line"></span><br><span class="line">    data = np.array([</span><br><span class="line">        [<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>], [<span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">1</span>, <span class="number">-1</span>], [<span class="number">3</span>, <span class="number">2</span>, <span class="number">-1</span>]</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">    train_x = data[:, <span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line">    train_y = data[:, <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">    clf = SVC(C=<span class="number">100</span>, kernel=<span class="string">'linear'</span>)</span><br><span class="line">    clf.fit(train_x, train_y)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取w</span></span><br><span class="line">    w = clf.coef_[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 获取截距</span></span><br><span class="line">    b = clf.intercept_</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 打印超平面</span></span><br><span class="line">    print(clf.support_vectors_)</span><br><span class="line">    print(w, b)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算结束事时间</span></span><br><span class="line">    end = time.time()</span><br><span class="line">    print(<span class="string">'用时：&#123;:.3f&#125;s'</span>.format(end - star))</span><br></pre></td></tr></table></figure></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> 陶文寅</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="/bearcoding.cn/slm-svm/" title="S7-支持向量机">bearcoding.cn/slm-svm/</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/slm-logistic-regression/" rel="next" title="S6-逻辑斯谛回归与最大熵模型"><i class="fa fa-chevron-left"></i> S6-逻辑斯谛回归与最大熵模型</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"> <a href="/slm-adaboost/" rel="prev" title="S8-提升方法">S8-提升方法<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#线性可分支持向量机与硬间隔最大化"><span class="nav-number">1.</span> <span class="nav-text">线性可分支持向量机与硬间隔最大化</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#线性可分支持向量机"><span class="nav-number">1.1.</span> <span class="nav-text">线性可分支持向量机</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#函数间隔和几何间隔"><span class="nav-number">1.2.</span> <span class="nav-text">函数间隔和几何间隔</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#函数间隔"><span class="nav-number">1.2.1.</span> <span class="nav-text">函数间隔</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#几何间隔"><span class="nav-number">1.2.2.</span> <span class="nav-text">几何间隔</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#间隔最大化"><span class="nav-number">1.3.</span> <span class="nav-text">间隔最大化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#最大间隔分离超平面"><span class="nav-number">1.3.1.</span> <span class="nav-text">最大间隔分离超平面</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#支持向量和间隔边界"><span class="nav-number">1.3.2.</span> <span class="nav-text">支持向量和间隔边界</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#学习的对偶算法"><span class="nav-number">1.4.</span> <span class="nav-text">学习的对偶算法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#线性支持向量机与软间隔最大化"><span class="nav-number">2.</span> <span class="nav-text">线性支持向量机与软间隔最大化</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#线性支持向量机"><span class="nav-number">2.1.</span> <span class="nav-text">线性支持向量机</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#学习的对偶算法-1"><span class="nav-number">2.2.</span> <span class="nav-text">学习的对偶算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#支持向量没懂"><span class="nav-number">2.3.</span> <span class="nav-text">支持向量（没懂）</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#非线性支持向量机与核函数"><span class="nav-number">3.</span> <span class="nav-text">非线性支持向量机与核函数</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#正定核"><span class="nav-number">3.1.</span> <span class="nav-text">正定核</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#常用核函数"><span class="nav-number">3.2.</span> <span class="nav-text">常用核函数</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#smo序列最小最优化算法没懂"><span class="nav-number">4.</span> <span class="nav-text">SMO序列最小最优化算法（没懂）</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#工程实践"><span class="nav-number">5.</span> <span class="nav-text">工程实践</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#函数原型"><span class="nav-number">5.1.</span> <span class="nav-text">函数原型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#程序流程"><span class="nav-number">5.2.</span> <span class="nav-text">程序流程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#核函数选择"><span class="nav-number">5.3.</span> <span class="nav-text">核函数选择</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#gridsearchcv小技巧"><span class="nav-number">5.4.</span> <span class="nav-text">GridSearchCV小技巧</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#常用函数"><span class="nav-number">5.5.</span> <span class="nav-text">常用函数</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#作业"><span class="nav-number">6.</span> <span class="nav-text">作业</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><p class="site-author-name" itemprop="name">陶文寅</p><div class="site-description" itemprop="description"></div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">42</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/"><span class="site-state-item-count">14</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">16</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="/mailto:wenyin.tao@163.com" title="E-Mail &amp;rarr; mailto:wenyin.tao@163.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i> E-Mail</a></span></div><div class="cc-license motion-element" itemprop="license"> <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright"> &copy; <span itemprop="copyrightYear">2020</span><span class="with-love"><i class="fa fa-user"></i></span> <span class="author" itemprop="copyrightHolder">陶文寅</span></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.0.0</div> <span class="post-meta-divider">|</span><div class="theme-info">主题 – <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.5.0</div></div></footer></div><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script><script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script></body></html>