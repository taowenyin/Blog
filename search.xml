<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Multi-Label Learning with Deep Forest</title>
    <url>/ml-df/</url>
    <content><![CDATA[<h1 id="摘要">摘要</h1><p>在对标签学习中，每个实例都都与多个标签相关，因此多标签学习的关键就是在构建模型中如何利用标签之间的相关性。深度神经网络方法经常把特征和标签信息进行联合放到一个潜在的空间，从而挖掘出标签之间的相关性。然而，这些方法的成功高度依赖于模型深度的精确选择。深度森林是一个基于树模型集成的深度学习框架，并且不依赖于反向传播。作者认为深度森林模型的优势在于非常适合解决多标签问题。MLDF方法具有两个机制：</p><a id="more"></a>

<blockquote>
<ul>
<li>度量感知特征重用机制：以置信度作为依据，重用前一层的较好标签。</li>
<li>度量感知层增长机制：保证了MLDF模型复杂度随性能的度量而逐步增加。</li>
</ul>
</blockquote>
<p>MLDF将同时面对两个问题的挑战：</p>
<blockquote>
<ul>
<li>通过限制模型的复杂度来防止过拟合的问题。</li>
<li>因为在多标签任务中有许多不同的指标，所以需要根据用户需求优化性能指标。</li>
</ul>
</blockquote>
<p>实验表明，MLDF不仅优于在9个基准数据集上使用的6个方法，并且还能够在多标签学习中发现标签之间的相关性和其他相关特性。</p>
<h1 id="介绍">介绍</h1>
<p>在多标签学习中，每个实例都具有多个标签，并且多标签学习的任务就是为一个未知实例预测一组相关标签集。多标签学习被广泛的用于多种问题中，如文本分类、场景分类、基因组功能分类、视频分类、化学品分类等。多标签学习任务在现实问题中无处不在，引起了越来越多的研究关注。</p>
<h1 id="参考资料">参考资料</h1>
<p>[1] <a href="https://www.zhuanzhi.ai/document/430049ace146346a4859fa4c111b1a16" target="_blank" rel="noopener">周志华团队：深度森林挑战多标签学习，9大数据集超越传统方法</a></p>
]]></content>
      <categories>
        <category>论文</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>多标签学习</tag>
      </tags>
  </entry>
  <entry>
    <title>基于事件分析的告知学生综合评价模型构建与研究</title>
    <url>/base-on-event-analysis/</url>
    <content><![CDATA[<p><strong>期刊：</strong>《职业技术教育》，2019，40(21)：38-43</p><p><strong>作者：</strong>陶文寅，潘嘹</p><h1 id="摘要">摘要</h1><p>以事件数据分析理论为切入点，通过评估学生在校事件分值，建立学生在校事件影响力估算公式，结合高职学生的行为特点，初步构建高职学生综合评价模型，并通过事件模拟评估该模型的有效性。结果表明，基于事件分析的高职学生综合评价模型能够准确、客观地评价高职学生的在校表现，有效克服了主观评价体系中的缺点，为定量分析高职学生的在校表现提供了理论基础。</p><a id="more"></a>



<p><strong>关键字：</strong>学生评价；事件数据分析；定量分析</p>
<p><a href="https://pan.baidu.com/s/1lYCjL8NGLF-gUN0G9inPag" target="_blank" rel="noopener">下载地址</a></p>
<p>提取码: che7</p>
]]></content>
      <categories>
        <category>发表</category>
      </categories>
      <tags>
        <tag>教育教学</tag>
      </tags>
  </entry>
  <entry>
    <title>高等工程应用数学</title>
    <url>/math/</url>
    <content><![CDATA[<h1 id="考试题目">考试题目</h1><h2 id="第一题">第一题</h2><p>设<span class="math inline">\(Z\)</span>表示整数集</p><blockquote>
<ul>
<li>1、在<span class="math inline">\(Z\)</span>上定义运算：<span class="math inline">\(a \circ b = a+b+4\)</span>，求证<span class="math inline">\(\mathbb{Z}\)</span>按<span class="math inline">\(\circ\)</span>运算作为一个群。</li>
<li>2、如果在<span class="math inline">\(Z\)</span>上定义运算<span class="math inline">\(a \odot b = a^{k}\)</span>，说明<span class="math inline">\(\mathbb{Z}\)</span>在<span class="math inline">\(\odot\)</span>运算上不是一个群。</li>
</ul>
</blockquote><a id="more"></a>



<p>（注：题目中的“<span class="math inline">\(+\)</span>”以及“<span class="math inline">\(a^{k}\)</span>”都是指通常意义下数的加法和乘方）</p>
<p>1、证明群：运算封闭、结合律、有单位元、有逆元。</p>
<blockquote>
<ul>
<li>证明运算封闭</li>
</ul>
</blockquote>
<p><span class="math inline">\(\because \forall a, b \in z\)</span></p>
<p><span class="math inline">\(\therefore a \circ b=a+b+4 \in z\)</span></p>
<blockquote>
<ul>
<li>证明结合律</li>
</ul>
</blockquote>
<p><span class="math inline">\(\forall a, b, c \in z\)</span></p>
<p><span class="math inline">\(\because (a \cdot b) \cdot c=(a+b+4) \cdot c=a+b+4+c+4=a+b+c+8\)</span></p>
<p><span class="math inline">\(a \cdot (b \cdot c)=a \cdot (b+c+4)=a+b+c+4+4=a+b+c+8\)</span></p>
<p><span class="math inline">\(\therefore (a \cdot b) \cdot c=a \cdot (b \cdot c)\)</span></p>
<blockquote>
<ul>
<li>证明有单位元</li>
</ul>
</blockquote>
<p>令单位元为<span class="math inline">\(e\)</span>，<span class="math inline">\(\forall a \in \mathbb{Z}\)</span>，证明<span class="math inline">\(a \circ e=e \circ a=a\)</span></p>
<p>则<span class="math inline">\(a \circ e=a+e+4=a\)</span>，所以<span class="math inline">\(e=-4\)</span></p>
<p><span class="math inline">\(\because e \circ a=-4+a+4=a\)</span></p>
<p><span class="math inline">\(\therefore a \circ e=e \circ a=a\)</span></p>
<blockquote>
<ul>
<li>证明有逆元</li>
</ul>
</blockquote>
<p>令逆元为<span class="math inline">\(b\)</span>，<span class="math inline">\(\forall a \in \mathbb{Z}\)</span>，证明<span class="math inline">\(a \circ b=b \circ a=e\)</span></p>
<p>则<span class="math inline">\(a \circ b=a+b+4=e=-4\)</span>，所以<span class="math inline">\(b=-a-8\)</span></p>
<p><span class="math inline">\(\because b \circ a=b+a+4=-a-8+a+4=-4\)</span></p>
<p><span class="math inline">\(\because a \circ b\)</span>在<span class="math inline">\(\mathbb{Z}\)</span>上构成群。</p>
<p>2、证明群：运算封闭、结合律、有单位元、有逆元只要有一个不符合就不是群。</p>
<p>令<span class="math inline">\(a=2,b=-2\)</span>，那么<span class="math inline">\(a \odot b=a^{b}=2^{-2}=\frac{1}{4} \notin \mathbb{Z}\)</span></p>
<p>所以<span class="math inline">\(a \odot b = a^{k}\)</span>在<span class="math inline">\(\mathbb{Z}\)</span>不是一个群</p>
<h2 id="第二题">第二题</h2>
<p>设<span class="math inline">\(R\)</span>是一个环，若<span class="math inline">\(\forall a \in R\)</span>，都有<span class="math inline">\(a^{2} = a\)</span>，则称R为布尔环</p>
<blockquote>
<ul>
<li>1、求证：若<span class="math inline">\(R\)</span>是布尔环，则<span class="math inline">\(\forall a \in R\)</span>，<span class="math inline">\(a+a=0\)</span>。</li>
<li>2、设<span class="math inline">\(X\)</span>是一个集合，<span class="math inline">\(\Gamma\)</span>表示由<span class="math inline">\(X\)</span>的全部子集作为元素得到的集合，定义<span class="math inline">\(\Gamma\)</span>中的两种运算，<span class="math inline">\(A+B=(A \setminus B) \cup (B \setminus A)\)</span>和<span class="math inline">\(A \ast B = A \cap B\)</span>，求证<span class="math inline">\(\Gamma\)</span>在这两个运算下构成一个环，并且是布尔环。（注：“<span class="math inline">\(\setminus\)</span>”表示两个集合做差集）</li>
</ul>
</blockquote>
<p>1、<span class="math inline">\(\forall a,b \in R\)</span>，则有<span class="math inline">\(a+b \in R\)</span></p>
<p><span class="math inline">\(\because R\)</span>是一个布尔环</p>
<p><span class="math inline">\(\therefore (a+b)^{2}=a+b\)</span></p>
<p>又<span class="math inline">\(\because (a+b)^{2}=a^{2}+ab+ba+b^{2}\)</span>，并且<span class="math inline">\(R\)</span>是布尔环</p>
<p><span class="math inline">\(\therefore a^{2}=a,b^{2}=b\)</span></p>
<p><span class="math inline">\(\therefore (a+b)^{2}=a^{2}+ab+ba+b^{2}=a+ab+ba+b=a+b\)</span></p>
<p><span class="math inline">\(\therefore ab+ba=0\)</span></p>
<p><span class="math inline">\(\therefore ab=-ba\)</span></p>
<p>令<span class="math inline">\(b=a\)</span>，则<span class="math inline">\(a^{2}=-a^{2}\)</span></p>
<p><span class="math inline">\(\therefore 2a^{2}=0\)</span></p>
<p><span class="math inline">\(\therefore 2a=0\)</span></p>
<p><span class="math inline">\(\therefore a+a=0\)</span></p>
<p>2、证明布尔环：加法是交换群、乘法是半群、所有元素都是幂等元。</p>
<p>（1）<span class="math inline">\(\left ( R,+ \right )\)</span>加法运算是交换群</p>
<blockquote>
<ul>
<li>证明运算封闭</li>
</ul>
</blockquote>
<p><span class="math inline">\(\because \forall A,B \in \Gamma\)</span></p>
<p><span class="math inline">\(\therefore (A \setminus B) \in \Gamma,(B \setminus A) \in \Gamma,(A \setminus B) \cup (B \setminus A) \in \Gamma\)</span></p>
<p><span class="math inline">\(\therefore \left ( A \cap B^{\mathrm{C}} \right ) \cup \left ( B \cap A^{\mathrm{C}} \right )=\left ( A \cup B \right ) \cap \left ( A \cap B \right )^{\mathrm{C}} \in \Gamma\)</span></p>
<blockquote>
<ul>
<li>证明结合律</li>
</ul>
</blockquote>
<p><span class="math inline">\(\forall A,B,C \in \Gamma\)</span></p>
<p><span class="math inline">\(\because \left ( A+B \right )+C=(A \setminus B) \cup (B \setminus A)+C=\left ( A \cup B \cup C \right ) \cap \left ( A \cap B \cap C \right )^{\mathrm{C}}\)</span></p>
<p><span class="math inline">\(\because A+\left ( B+C \right )=A+(B \setminus C) \cup (C \setminus B)=\left ( A \cup B \cup C \right ) \cap \left ( A \cap B \cap C \right )^{\mathrm{C}}\)</span></p>
<p><span class="math inline">\(\therefore \left ( A+B \right )+C=A+\left ( B+C \right )\)</span></p>
<blockquote>
<ul>
<li>证明有单位元</li>
</ul>
</blockquote>
<p>令<span class="math inline">\(\varnothing=e\)</span>，且<span class="math inline">\(\varnothing \in \Gamma\)</span></p>
<p><span class="math inline">\(\because A+\varnothing=(A \setminus \varnothing) \cup (\varnothing \setminus A)=A\)</span></p>
<p>又<span class="math inline">\(\because \varnothing+A=(\varnothing \setminus A) \cup (A \setminus \varnothing)=A\)</span></p>
<p><span class="math inline">\(\therefore A+\varnothing=\varnothing+A\)</span></p>
<p><span class="math inline">\(\therefore \varnothing\)</span>为单位元</p>
<blockquote>
<ul>
<li>证明有逆元</li>
</ul>
</blockquote>
<p>令逆元为<span class="math inline">\(B\)</span>，<span class="math inline">\(\forall A \in \mathbb{X}\)</span>，证明<span class="math inline">\(A \circ B=B \circ A=e=\varnothing\)</span></p>
<p>则<span class="math inline">\(A \circ B=A+B=(A \setminus B) \cup (B \setminus A)=\varnothing\)</span></p>
<p><span class="math inline">\(\therefore B=A^{\mathrm{C}}\)</span></p>
<p>同理可证<span class="math inline">\(B \circ A=\varnothing\)</span></p>
<p><span class="math inline">\(\therefore A \circ B=B \circ A=\varnothing\)</span></p>
<p>同理可证<span class="math inline">\(A \ast B=A \ast A^{\mathrm{C}}=A \cap A^{\mathrm{C}}=\varnothing\)</span></p>
<p>（2）<span class="math inline">\(\left ( R,\ast \right )\)</span>乘法运算是半群</p>
<p><span class="math inline">\(\because \forall A,B \in \Gamma\)</span></p>
<p><span class="math inline">\(\therefore A \ast B=A \cap B \in \Gamma\)</span></p>
<p><span class="math inline">\(\because \forall A,B,C \in \Gamma\)</span></p>
<p><span class="math inline">\(\therefore \left ( A \ast B \right ) \ast C=\left ( A \cap B \right ) \cap C=A \cap B \cap C\)</span></p>
<p><span class="math inline">\(\therefore A \ast \left ( B \ast C \right )=A \cap \left ( B \cap C \right ) =A \cap B \cap C\)</span></p>
<p><span class="math inline">\(\therefore A \ast \left ( B \ast C \right )=A \ast \left ( B \ast C \right )\)</span></p>
<p><span class="math inline">\(\therefore\)</span>乘法运算是半群</p>
<p>（3）所有元素都是幂等元</p>
<p><span class="math inline">\(\because \forall A \in \Gamma\)</span></p>
<p><span class="math inline">\(\therefore A \ast A=A \cap A=A\)</span></p>
<p><span class="math inline">\(\therefore \Gamma\)</span>中所有元素都是幂等元</p>
<p>综上所述，<span class="math inline">\(R\)</span>为布尔环。</p>
<h2 id="第三题">第三题</h2>
<p>设<span class="math inline">\(V\)</span>是<span class="math inline">\(n\)</span>维线性空间，<span class="math inline">\(W\)</span>是<span class="math inline">\(V\)</span>的一个线性真子空间。</p>
<blockquote>
<ul>
<li>1、写出商空间<span class="math inline">\(V/W\)</span>的定义（只要写出它的元素形式，加法、数乘的定义）。</li>
<li>2、若<span class="math inline">\(\alpha_{1}, \alpha_{2}, \cdots, \alpha_{m}\)</span>是<span class="math inline">\(W\)</span>的一组基，把它延拓为<span class="math inline">\(V\)</span>的一组基<span class="math inline">\(\alpha_{1}, \alpha_{2}, \cdots, \alpha_{m}, \alpha_{m+1}, \cdots, \alpha_{n}(n&gt;m)\)</span>，试写出商空间<span class="math inline">\(V/W\)</span>一组基，并验证它的线性无关性。</li>
</ul>
</blockquote>
<p>1、商空间<span class="math inline">\(V/W\)</span>的定义</p>
<p><span class="math inline">\(V/W=\left \{ \alpha +W \mid \alpha \in V \right \}\)</span></p>
<p>令<span class="math inline">\(\alpha +W=\hat{\alpha}\)</span>，对<span class="math inline">\(\forall \alpha ,\beta \in V\)</span></p>
<p>有<span class="math inline">\(\hat{\alpha}+\hat{\beta}=\left ( \alpha+W \right )+\left ( \beta+W \right )=\left ( \alpha+\beta \right )+W\)</span></p>
<p><span class="math inline">\(k\hat{\alpha}=k\left ( \alpha +W \right )=k\alpha +W\)</span></p>
<p>2、获取商空间<span class="math inline">\(V/W\)</span>一组基</p>
<p>对<span class="math inline">\(\forall \beta +W \in V/W\)</span>，有<span class="math inline">\(\beta=x_{1} \alpha_{1}+x_{2} \alpha_{2}+\cdots+x_{n} \alpha_{n}\)</span>，则</p>
<p><span class="math display">\[\begin{equation}
    \begin{matrix}
        \beta+W=\left(x_{1} \alpha_{1}+x_{2} \alpha_{2}+\cdots+x_{n} \alpha_{n}\right)+W \\
        =\left(x_{1} \alpha_{1}+W\right)+\cdots+\left(x_{m} \alpha_{m}+W\right)+\left(x_{m+1} \alpha_{m+1}+W\right)+\cdots+\left(x_{n} \alpha_{n}+W\right) \\
        =W+\cdots+W+x_{m+1}\left(\alpha_{m+1}+W\right)+\cdots+x_{n}\left(\alpha_{n}+W\right) \\
        =x_{m+1}\left(\alpha_{m+1}+W\right)+\cdots+x_{n}\left(\alpha_{n}+W\right) \\
        =x_{m+1}\hat{\alpha_{m+1}}+\cdots+x_{n}\hat{\alpha_{n}}
    \end{matrix}
\end{equation}\]</span></p>
<p>因此<span class="math inline">\(V/W\)</span>中任意向量都可以用<span class="math inline">\(\hat{\alpha_{m+1}},\cdots ,\hat{\alpha_{n}}\)</span>线性表示。</p>
<p>3、验证这组基的线性无关性<span class="math inline">\(k_{m+1}\hat{\alpha_{m+1}}+\cdots+k_{n}\hat{\alpha_{n}}=\hat{0}\)</span></p>
<p>设<span class="math inline">\(k_{m+1}\left(\alpha_{m+1}+W\right)+\cdots+k_{n}\left(\alpha_{n}+W\right)=W\)</span></p>
<p>则<span class="math inline">\(\left ( k_{m+1}\alpha_{m+1}+\cdots+k_{n}\alpha_{n} \right )+W=W\)</span></p>
<p><span class="math inline">\(\therefore k_{m+1}\alpha_{m+1}+\cdots+k_{n}\alpha_{n} \in W\)</span></p>
<p><span class="math inline">\(\therefore k_{m+1}\alpha_{m+1}+\cdots+k_{n}\alpha_{n} = -k_{1}\alpha_{1}-\cdots-k_{m}\alpha_{m}\)</span></p>
<p><span class="math inline">\(\therefore k_{1}\alpha_{1}+\cdots+k_{m}\alpha_{m}+k_{m+1}\alpha_{m+1}+\cdots+k_{n}\alpha_{n}=0\)</span></p>
<p>因为<span class="math inline">\(\alpha_{1}, \cdots, \alpha_{n}\)</span>是一组线性无关的基</p>
<p>所以<span class="math inline">\(k_{1}=k_{2}=\cdots=k_{n}=0\)</span></p>
<p>因此<span class="math inline">\(\hat{\alpha_{m+1}},\cdots,\hat{\alpha_{n}}\)</span>是<span class="math inline">\(V/W\)</span>的一组线性无关基。</p>
<h2 id="第四题">第四题</h2>
<p>设<span class="math inline">\(\left ( X, \rho \right )\)</span>是距离空间，令</p>
<p><span class="math display">\[\begin{equation}
    d(x, y)=\frac{\rho(x, y)}{1+\rho(x, y)}, \forall x, y \in X
\end{equation}\]</span></p>
<blockquote>
<ul>
<li>1、求证：<span class="math inline">\(\left ( X, d \right )\)</span>也是距离空间。</li>
<li>2、若<span class="math inline">\(X\)</span>中的点列<span class="math inline">\(\left\{x_{n}\right\}_{n=1}^{\infty}\)</span>按距离<span class="math inline">\(\rho\)</span>收敛到<span class="math inline">\(x\)</span>，求证<span class="math inline">\(\left\{x_{n}\right\}_{n=1}^{\infty}\)</span>也按距离<span class="math inline">\(d\)</span>收敛到<span class="math inline">\(x\)</span></li>
<li>3、若<span class="math inline">\(A\)</span>是<span class="math inline">\(\left ( X, d \right )\)</span>中的闭集，求证：<span class="math inline">\(A\)</span>也是<span class="math inline">\(\left ( X, \rho \right )\)</span>中的闭集。</li>
</ul>
</blockquote>
<p>1、证明<span class="math inline">\(\left ( X, d \right )\)</span>也是距离空间：非负性、对称性、三角不等式</p>
<p>（1）非负性</p>
<p><span class="math inline">\(\because \rho\)</span>是距离空间</p>
<p><span class="math inline">\(\therefore \rho(x, y) \geq 0\)</span></p>
<p><span class="math inline">\(\therefore d(x, y)=\frac{\rho(x, y)}{1+\rho(x, y)} \geq 0\)</span></p>
<p>（2）对称性</p>
<p><span class="math inline">\(\because \rho\)</span>是距离空间</p>
<p><span class="math inline">\(\therefore \rho(x, y) = \rho(y, x)\)</span></p>
<p><span class="math inline">\(\therefore d(x, y)=\frac{\rho(x, y)}{1+\rho(x, y)} = \frac{\rho(y, x)}{1+\rho(y, x)} = d(y, x)\)</span></p>
<p>（3）三角不等式</p>
<p><span class="math inline">\(\because \rho\)</span>是距离空间</p>
<p><span class="math inline">\(\therefore \rho(x, z) \leq \rho(x, y) + \rho(y, z)\)</span></p>
<p><span class="math inline">\(\therefore d(x, z)=\frac{\rho(x, z)}{1+\rho(x, z)} \leq \frac{\rho(x, y)+\rho(y, z)}{1+\rho(x, y)+\rho(y, z)}\)</span></p>
<p><span class="math inline">\(\therefore \frac{\rho(x, y)+\rho(y, z)}{1+\rho(x, y)+\rho(y, z)}=\frac{\rho(x, y)}{1+\rho(x, y)+\rho(y, z)}+\frac{\rho(y, z)}{1+\rho(x, y)+\rho(y, z)}\)</span></p>
<p><span class="math inline">\(\therefore \frac{\rho(x, y)}{1+\rho(x, y)+\rho(y, z)}+\frac{\rho(y, z)}{1+\rho(x, y)+\rho(y, z)} \leq \frac{\rho(x, y)}{1+\rho(x, y)}+\frac{\rho(y, z)}{1+\rho(y, z)}=d(x,y)+d(y,z)\)</span></p>
<p><span class="math inline">\(\therefore d(x, z) \leq d(x, y)+d(y, z)\)</span></p>
<p>因此<span class="math inline">\(\left ( X, d \right )\)</span>是距离空间</p>
<p>2、证明<span class="math inline">\(d\)</span>收敛到<span class="math inline">\(x\)</span>：随着<span class="math inline">\(x_{n}\)</span>的增加，距离接近于0</p>
<p><span class="math inline">\(\because X\)</span>中的点列<span class="math inline">\(\left\{x_{n}\right\}_{n=1}^{\infty}\)</span>按距离<span class="math inline">\(\rho\)</span>收敛到<span class="math inline">\(x\)</span></p>
<p><span class="math inline">\(\therefore \forall \varepsilon &gt; 0,\exists N\)</span>时，当<span class="math inline">\(n&gt;N\)</span>是<span class="math inline">\(\rho \left ( x_{n},x \right ) &lt; \varepsilon\)</span></p>
<p><span class="math inline">\(\therefore \forall \varepsilon &gt; 0,\exists N\)</span>时，当<span class="math inline">\(n&gt;N\)</span>是<span class="math inline">\(d \left ( x_{n},x \right ) &lt; \frac{\varepsilon}{1+\varepsilon}\)</span></p>
<p><span class="math inline">\(\therefore \frac{\varepsilon}{1+\varepsilon}\)</span>任意小，并且<span class="math inline">\(\frac{\varepsilon}{1+\varepsilon}&gt;0\)</span></p>
<p><span class="math inline">\(\therefore \left\{x_{n}\right\}_{n=1}^{\infty}\)</span>也按距离<span class="math inline">\(d\)</span>收敛到<span class="math inline">\(x\)</span></p>
<p>3、<span class="math inline">\(A\)</span>也是<span class="math inline">\(\left ( X, \rho \right )\)</span>中的闭集</p>
<p><span class="math inline">\(\because A\)</span>是<span class="math inline">\(\left ( X, d \right )\)</span>中的闭集的充要条件是<span class="math inline">\(A\)</span>中任意一个收敛点列必收敛于<span class="math inline">\(A\)</span>中的一点。</p>
<p><span class="math inline">\(\therefore X\)</span>中点列<span class="math inline">\(\left\{x_{n}\right\}_{n=1}^{\infty}\)</span>按距离<span class="math inline">\(d\)</span>收敛到<span class="math inline">\(X\)</span>中的一个点<span class="math inline">\(x_{0}\)</span>。</p>
<p>有第2问结果知道点列<span class="math inline">\(\left\{x_{n}\right\}_{n=1}^{\infty}\)</span>在距离<span class="math inline">\(\rho\)</span>也收敛于<span class="math inline">\(x_{0}\)</span>，并且<span class="math inline">\(x_{0} \in X\)</span>。</p>
<p><span class="math inline">\(\therefore A\)</span>是<span class="math inline">\(\left ( X, \rho \right )\)</span>中的闭集。</p>
<h2 id="第五题">第五题</h2>
<p>设<span class="math inline">\(V\)</span>是数域<span class="math inline">\(P\)</span>上的赋范线性空间，证明：</p>
<blockquote>
<ul>
<li>1、<span class="math inline">\(V\)</span>中的收敛点列<span class="math inline">\(\left\{x_{n}\right\}\)</span>的极限是唯一的。</li>
<li>2、<span class="math inline">\(V\)</span>中收敛序列<span class="math inline">\(\left\{x_{n}\right\}\)</span>必有界，即存在正数<span class="math inline">\(M\)</span>，使得<span class="math inline">\(\left\|x_{n}\right\| \leq M(n=1,2, \cdots)\)</span>;</li>
<li>3、如果<span class="math inline">\(V\)</span>中序列<span class="math inline">\(\left\{x_{n}\right\}\)</span>收敛到<span class="math inline">\(x \in V\)</span>，则<span class="math inline">\(\left\{x_{n}\right\}\)</span>的任意一个子序列{<span class="math inline">\(\left\{x_{n_{k}}\right\}\)</span>}也收敛到<span class="math inline">\(x\)</span>；</li>
<li>4、如果<span class="math inline">\(V\)</span>中序列<span class="math inline">\(\left\{x_{n}\right\}\)</span>，<span class="math inline">\(\left\{y_{n}\right\}\)</span>分别收敛到<span class="math inline">\(x,y \in V\)</span>，则对任意<span class="math inline">\(a,b \in P\)</span>，有</li>
</ul>
</blockquote>
<p><span class="math display">\[\begin{equation}
    \underset{n \rightarrow \infty}{lim}\left(a x_{n}+b y_{n}\right)=ax+by
\end{equation}\]</span></p>
<p>1、证明<span class="math inline">\(\left\{x_{n}\right\}\)</span>极限的唯一性</p>
<p>设<span class="math inline">\(x,\tilde{x}\)</span>都是<span class="math inline">\(x_{n}\)</span>的极限，则得到<span class="math inline">\(0\leq \|x+\tilde{x} \| \leq \|x+x_{n}\|+\|x_{n}+\tilde{x}\|\)</span></p>
<p><span class="math inline">\(\therefore\)</span>当<span class="math inline">\(n \rightarrow \infty\)</span>时，<span class="math inline">\(\|x+x_{n} \| \rightarrow 0,\|x_{n}+\tilde{x}\| \rightarrow 0\)</span></p>
<p><span class="math inline">\(\therefore \|x+\tilde{x} \|=0\)</span></p>
<p><span class="math inline">\(\therefore x=\tilde{x}\)</span></p>
<p>2、证明<span class="math inline">\(\left\{x_{n}\right\}\)</span>必有界</p>
<p>由<span class="math inline">\(x_{n} \rightarrow x \left ( n \rightarrow \infty \right )\)</span>知，存在正数<span class="math inline">\(K\)</span>，使得当<span class="math inline">\(k &gt; K\)</span>时，<span class="math inline">\(d\left ( x_{k},x \right )&lt; 1\)</span></p>
<p><span class="math display">\[\begin{equation}
    r=max\left \{ d\left ( x_{1},x \right ),\cdots ,d\left ( x_{K-1},x \right ),1 \right \}
\end{equation}\]</span></p>
<p>则<span class="math inline">\(\left\{x_{n}\right\}\subseteq U\left ( x,r \right )\)</span></p>
<p>3、证明任意一个子序列{<span class="math inline">\(\left\{x_{n_{k}}\right\}\)</span>}也收敛到<span class="math inline">\(x\)</span></p>
<p><span class="math inline">\(\because x_{n} \rightarrow x\)</span></p>
<p><span class="math inline">\(\therefore \exists N\)</span>，使得<span class="math inline">\(n &gt; N\)</span>时有<span class="math inline">\(\left \| x_{n}-x \right \| &lt; \varepsilon\)</span></p>
<p><span class="math inline">\(\because x_{n_{k}}\)</span>是<span class="math inline">\(x_{n}\)</span>的子列</p>
<p><span class="math inline">\(\therefore \exists k\)</span>，使得<span class="math inline">\(k&gt;K\)</span>时，有<span class="math inline">\(n_{k}&gt;N\)</span></p>
<p>4、证明<span class="math inline">\(\underset{n \rightarrow \infty}{lim} \left(a x_{n}+b y_{n}\right)=ax+by\)</span></p>
<p><span class="math inline">\(\because \left \|\left(a x_{n}+b y_{n}\right)-\left ( ax+by\right ) \right \|=\left \| a\left ( x_{n}-x\right )+b\left ( y_{n}-y\right )\right \|\)</span></p>
<p><span class="math inline">\(\therefore \left \| a\left ( x_{n}-x\right )+b\left ( y_{n}-y\right )\right \| \leq \left \| a\left ( x_{n}-x\right )\right \|+\left \| b\left ( y_{n}-y\right )\right \|\)</span></p>
<p><span class="math inline">\(\because \left \| a\left ( x_{n}-x\right )\right \|+\left \| b\left ( y_{n}-y\right )\right \|=\left | a\right |\left \| x_{n}-x\right \|+\left | b\right |\left \| y_{n}-y\right \|\)</span></p>
<p><span class="math inline">\(\therefore \left \|\left(a x_{n}+b y_{n}\right)-\left ( ax+by\right ) \right \| \leq \left | a\right |\left \| x_{n}-x\right \|+\left | b\right |\left \| y_{n}-y\right \|\)</span></p>
<p>又<span class="math inline">\(\because x_{n}\)</span>收敛到<span class="math inline">\(x\)</span>，并且<span class="math inline">\(y_{n}\)</span>也收敛到<span class="math inline">\(y\)</span></p>
<p><span class="math inline">\(\therefore \underset{n \rightarrow \infty}{lim} \left(a x_{n}+b y_{n}\right)=ax+by\)</span></p>
<h2 id="第六题">第六题</h2>
<p>证<span class="math inline">\(U_{n}\)</span>是循环群，<span class="math inline">\(U_{n}=span\left ( e^{i\frac{2\pi k}{n}} \right )\left ( i=0,1,\cdots,n-1 \right )\)</span></p>
<p>先证群，再证循环群</p>
<p>设<span class="math inline">\(\varepsilon ^{k}=e^{\frac{i \cdot 2\pi \cdot k}{n}}\)</span></p>
<p>当<span class="math inline">\(k=n\)</span>时，<span class="math inline">\(\varepsilon ^{n}=e^{i \cdot 2\pi}=1,\varepsilon ^{0}=e^{0}=1\)</span></p>
<p><span class="math inline">\(\forall \varepsilon ^{a},\varepsilon ^{a},0\leq a,b\leq n-1\)</span></p>
<blockquote>
<ul>
<li>证明运算封闭</li>
</ul>
</blockquote>
<p><span class="math inline">\(\because \varepsilon ^{a}\cdot \varepsilon ^{b}=e^{i\frac{2\pi a}{n}}\cdot e^{i\frac{2\pi b}{n}}=e^{i\frac{2\pi \left ( a+b \right )}{n}}\)</span></p>
<p><span class="math inline">\(\therefore \varepsilon ^{a}\cdot \varepsilon ^{b} \in U_{n}\)</span></p>
<blockquote>
<ul>
<li>证明结合律</li>
</ul>
</blockquote>
<p><span class="math inline">\(\because \left ( \varepsilon ^{a}\cdot \varepsilon ^{b} \right )\cdot \varepsilon ^{c}=e^{i\frac{2\pi \left ( a+b \right )}{n}}\cdot e^{i\frac{2\pi c}{n}}=e^{i\frac{2\pi \left ( a+b+c \right )}{n}}\)</span></p>
<p><span class="math inline">\(\because \varepsilon ^{a} \cdot \left ( \varepsilon ^{b}\cdot \varepsilon ^{c} \right )=e^{i\frac{2\pi a}{n}} \cdot e^{i\frac{2\pi \left ( b+c \right )}{n}}=e^{i\frac{2\pi \left ( a+b+c \right )}{n}}\)</span></p>
<p><span class="math inline">\(\therefore \left ( \varepsilon ^{a}\cdot \varepsilon ^{b} \right )\cdot \varepsilon ^{c}=\varepsilon ^{a} \cdot \left ( \varepsilon ^{b}\cdot \varepsilon ^{c} \right )\)</span></p>
<blockquote>
<ul>
<li>证明有单位元</li>
</ul>
</blockquote>
<p><span class="math inline">\(\because \varepsilon ^{0}=e ^{0}=1\)</span></p>
<p>并且<span class="math inline">\(\because \varepsilon ^{k}\cdot \varepsilon ^{0}=\varepsilon ^{k}\)</span></p>
<p><span class="math inline">\(\therefore \varepsilon ^{0}\)</span>为单位元</p>
<blockquote>
<ul>
<li>证明有逆元</li>
</ul>
</blockquote>
<p><span class="math inline">\(\forall \varepsilon ^{k} \in U_{n}\)</span></p>
<p>则<span class="math inline">\(\exists \varepsilon ^{n-k}\)</span>使得</p>
<p><span class="math inline">\(\varepsilon ^{k} \cdot \varepsilon ^{n-k}=\varepsilon ^{n}=1=\varepsilon ^{0}\)</span></p>
<p><span class="math inline">\(\varepsilon ^{n-k} \cdot \varepsilon ^{k}=\varepsilon ^{0}\)</span></p>
<p>综上所述<span class="math inline">\(U_{n}\)</span>是群</p>
<p><span class="math inline">\(\because \varepsilon ^{k}=e^{\frac{i \cdot 2\pi \cdot k}{n}}\)</span>，即生成元是<span class="math inline">\(e^{\frac{i \cdot 2\pi}{n}}\)</span></p>
<p><span class="math inline">\(\therefore U_{n}=span\left ( e^{i\frac{2\pi k}{n}} \right )\)</span>是循环群</p>
]]></content>
      <categories>
        <category>课程</category>
      </categories>
  </entry>
  <entry>
    <title>最优化理论和应用</title>
    <url>/optimization-theory/</url>
    <content><![CDATA[<h1 id="向量空间与矩阵">向量空间与矩阵</h1><h2 id="向量与矩阵">向量与矩阵</h2><h3 id="线性相关">线性相关</h3><p>下式中，只有当所有系数<span class="math inline">\(\alpha_{i}(i=1,2,\cdots,k)\)</span>都等于零的前提下才成立，那么就称向量机<span class="math inline">\({\mathbf{a_{1}},\mathbf{a_{2}},\cdots,\mathbf{a_{k}}}\)</span>是线性无关的，否则只要有一个<span class="math inline">\(\mathbf{a_{k}}\)</span>不等于零，那么就是线性相关。</p><a id="more"></a>



<p><span class="math display">\[\begin{equation}
    \alpha_{1} \mathbf{a_{1}}+\alpha_{2} \mathbf{a_{2}}+\cdots+\alpha_{k} \mathbf{a_{k}}=\mathbf{0}
\end{equation}\]</span></p>
<p>对于所有包含<span class="math inline">\(\mathbf{0}\)</span>向量的集合都是线性相关。</p>
<p>给定了向量<span class="math inline">\(\mathbf{a}\)</span>，如果存在标量<span class="math inline">\(a_{1}, a_{2}, \cdots, a_{k}\)</span>，使得</p>
<p><span class="math display">\[\begin{equation}
    \mathbf{a}=\alpha_{1} \mathbf{a_{1}}+\alpha_{2} \mathbf{a_{2}}+\cdots+\alpha_{k} \mathbf{a_{k}}
\end{equation}\]</span></p>
<p>那么就称向量<span class="math inline">\(\mathbf{a}\)</span>是<span class="math inline">\({\mathbf{a_{1}},\mathbf{a_{2}},\cdots,\mathbf{a_{k}}}\)</span>的线性组合。</p>
<h3 id="生成子空间">生成子空间</h3>
<p>假定<span class="math inline">\(\mathbf{a_{1}},\mathbf{a_{2}},\cdots,\mathbf{a_{k}}\)</span>是<span class="math inline">\(\mathbb{R}^n\)</span>中的任意向量，那么它们所有线性组合的集合称为生成子空间，记为:</p>
<p><span class="math display">\[\begin{equation}
    span\left [ \mathbf{a_{1}},\mathbf{a_{2}},\cdots,\mathbf{a_{k}} \right ]=\left \{ \sum_{i=1}^{k}\alpha_{i}\mathbf{a_{i}}:\alpha_{1},\cdots,\alpha_{k},\alpha_{k}\in\mathbb{R} \right \}
\end{equation}\]</span></p>
<p>给定子空间<span class="math inline">\(\mathcal{V}\)</span>，如果存在<strong>线性无关</strong>的向量集合<span class="math inline">\(\left\{\mathbf{a_{1}}, \mathbf{a_{2}}, \cdots, \mathbf{a_{k}}\right\}\subset \mathcal{V}\)</span>，使得<span class="math inline">\(\mathcal{V}=span\left [\mathbf{a_{1}}, \mathbf{a_{2}}, \cdots, \mathbf{a_{k}} \right]\)</span>，那么就称<span class="math inline">\(\left \{ \mathbf{a_{1}}, \mathbf{a_{2}}, \cdots, \mathbf{a_{k}} \right \}\)</span>是子空间<span class="math inline">\(\mathcal{V}\)</span>的一组基，并且<span class="math inline">\(\mathcal{V}\)</span>中所有基都具有相同数量的向量，该数量称为<span class="math inline">\(\mathcal{V}\)</span>的维数，记为<span class="math inline">\(dim\mathcal{V}\)</span>。</p>
<p>如果<span class="math inline">\(\left |\mathbf{a_{1}}, \mathbf{a_{2}}, \cdots, \mathbf{a_{k}}\right |\)</span>是<span class="math inline">\(\mathcal{V}\)</span>的基，那么<span class="math inline">\(\mathcal{V}\)</span>中任意的向量<span class="math inline">\(\mathbf{a}\)</span>都可以通过下式进行表示，其中<span class="math inline">\(\mathbf{a_{i}} \in \mathbb{R},\ i=1,2,\cdots,k\)</span>。</p>
<p><span class="math display">\[\begin{equation}
    \mathbf{a}=\alpha_{1}\mathbf{a_{1}}+\alpha_{2}\mathbf{a_{2}}+\cdots+\alpha_{k}\mathbf{a_{k}}
\end{equation}\]</span></p>
<h2 id="矩阵的秩">矩阵的秩</h2>
<p><span class="math display">\[\begin{equation}
    \mathbf{A} = 
        \begin{bmatrix}
            a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n}\\ 
            a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n}\\ 
            \vdots &amp; \cdots &amp; \ddots &amp; \vdots\\ 
            a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn}
        \end{bmatrix}
\end{equation}\]</span></p>
<p><span class="math inline">\(\mathbf{A}\)</span>的第<span class="math inline">\(k\)</span>列用<span class="math inline">\(\mathbf{a_{k}}\)</span>表示</p>
<p><span class="math display">\[\begin{equation}
    \mathbf{a_{k}} = 
        \begin{bmatrix}
            a_{1k}\\ 
            a_{2k}\\ 
            \vdots\\ 
            a_{mk}
        \end{bmatrix}
\end{equation}\]</span></p>
<p>矩阵<span class="math inline">\(\mathbf{A}\)</span>中线性无关列的最大数目称为<span class="math inline">\(\mathbf{A}\)</span>的<strong>秩</strong>，记为<span class="math inline">\(rank\mathbf{A}\)</span>，并且<span class="math inline">\(rank\mathbf{A}\)</span>就是<span class="math inline">\(span\left[\mathbf{a_{1}}, \mathbf{a_{2}}, \cdots, \mathbf{a_{k}}\right]\)</span>的维数。</p>
<p>给定<span class="math inline">\(m\times n\)</span>矩阵<span class="math inline">\(\mathbf{A}\)</span>，那么<span class="math inline">\(p\)</span><strong>阶子式</strong>是一个<span class="math inline">\(p\times p\)</span>矩阵的行列式，该<span class="math inline">\(p\times p\)</span>矩阵由矩阵<span class="math inline">\(\mathbf{A}\)</span>减去<span class="math inline">\(m-p\)</span>行和<span class="math inline">\(n-p\)</span>列得到，并且<span class="math inline">\(p\leq min\left \{ m,n \right \}\)</span>。同时，如果<span class="math inline">\(m\times n\left ( m \geq n \right )\)</span>矩阵<span class="math inline">\(\mathbf{A}\)</span>具有<span class="math inline">\(n\)</span>阶子式，那么<span class="math inline">\(rank\mathbf{A}=n\)</span>。</p>
<p>假定<span class="math inline">\(\mathbf{A}\)</span>是<span class="math inline">\(n \times n\)</span>的方阵，如果存在<span class="math inline">\(n \times n\)</span>方阵<span class="math inline">\(\mathbf{B}\)</span>，使得<span class="math inline">\(\mathbf{A}\mathbf{B}=\mathbf{B}\mathbf{A}=\mathbf{I}_{n}\)</span>，其中<span class="math inline">\(\mathbf{I}_{n}\)</span>为<span class="math inline">\(n \times n\)</span>的单位矩阵，<span class="math inline">\(\mathbf{B}\)</span>为<span class="math inline">\(\mathbf{A}\)</span>的逆矩阵，记为<span class="math inline">\(\mathbf{B}=\mathbf{A}^{-1}\)</span></p>
<h2 id="线性方程">线性方程</h2>
<p>假设方程写成矩阵形式</p>
<p><span class="math display">\[\begin{equation}
    \mathbf{A}\mathbf{x}=\mathbf{b}
\end{equation}\]</span></p>
<p>其中<span class="math inline">\(\mathbf{A}\)</span>为系数矩阵，<span class="math inline">\(\left [ \mathbf{A},\mathbf{b} \right ]\)</span>为增广矩阵，那么方程<span class="math inline">\(\mathbf{A}\mathbf{x}=\mathbf{b}\)</span>要有解，那么<span class="math inline">\(rank\mathbf{A}=rank\left [ \mathbf{A},\mathbf{b} \right ]\)</span>。</p>
<p>如果<span class="math inline">\(\mathbf{A} \in \mathbb{R}^{m \times n}\)</span>，且<span class="math inline">\(rank\mathbf{A}=m\)</span>，那么只要<span class="math inline">\(n-m\)</span>个未知数就可以求解<span class="math inline">\(\mathbf{A}\mathbf{x}=\mathbf{b}\)</span>。</p>
<h2 id="内积和范数">内积和范数</h2>
<p>对于<span class="math inline">\(\mathbf{x},\mathbf{y} \in \mathbb{R}^{n}\)</span>，其欧式内积为：</p>
<p><span class="math display">\[\begin{equation}
    \left \langle \mathbf{x},\mathbf{y} \right \rangle=\sum_{i=1}^{n}x_{i}y_{i}=\mathbf{x}^{\top}\mathbf{y}
\end{equation}\]</span></p>
<p>如果向量<span class="math inline">\(\mathbf{x}\)</span>和<span class="math inline">\(\mathbf{y}\)</span>，使得<span class="math inline">\(\left \langle \mathbf{x},\mathbf{y} \right \rangle=0\)</span>，那么<span class="math inline">\(\mathbf{x}\)</span>和<span class="math inline">\(\mathbf{y}\)</span><strong>正交</strong>。</p>
<p>向量<span class="math inline">\(\mathbf{x}\)</span>的欧式范数为<span class="math inline">\(\left \| \mathbf{x} \right \|=\sqrt{\left \langle \mathbf{x},\mathbf{x} \right \rangle}=\sqrt{\mathbf{x}^{\top}\mathbf{x}}\)</span>。</p>
<p><strong>柯西-施瓦茨不等式：</strong><span class="math inline">\(\left | \left \langle \mathbf{x},\mathbf{y} \right \rangle \right |=\left \| \mathbf{x} \right \|\left \| \mathbf{y} \right \|\)</span></p>
<p><span class="math inline">\(\mathbb{R}^{n}\)</span><strong>中的毕达哥拉斯定理：</strong>如果<span class="math inline">\(\mathbf{x}\)</span>和<span class="math inline">\(\mathbf{y}\)</span>正交，则<span class="math inline">\(\left \langle \mathbf{x},\mathbf{y} \right \rangle=0\)</span>，就可以得到<span class="math inline">\(\left \| \mathbf{x}+\mathbf{y} \right \|^{2}=\left \| \mathbf{x} \right \|^{2} + 2\left \langle \mathbf{x},\mathbf{y} \right \rangle + \left \| \mathbf{y} \right \|^{2}=\left \| \mathbf{x} \right \|^{2}+\left \| \mathbf{y} \right \|^{2}\)</span>。</p>
<h1 id="变换">变换</h1>
<h2 id="线性变换">线性变换</h2>
<p><strong>相似矩阵：</strong>给定两个<span class="math inline">\(n \times n\)</span>的矩阵<span class="math inline">\(\mathbf{A}\)</span>和<span class="math inline">\(\mathbf{B}\)</span>，如果存在一个非奇异矩阵（可逆）<span class="math inline">\(\mathbf{T}\)</span>，使得<span class="math inline">\(\mathbf{A}=\mathbf{T}^{-1}\mathbf{B}\mathbf{T}\)</span>，那么称阵<span class="math inline">\(\mathbf{A}\)</span>和<span class="math inline">\(\mathbf{B}\)</span>相似，在不同的基下，相似矩阵对应的线性变换相同。</p>
<h2 id="特征值与特征向量">特征值与特征向量</h2>
<p>特征值：令<span class="math inline">\(\mathbf{A}\)</span>为<span class="math inline">\(n \times n\)</span>的方阵，如果存在<span class="math inline">\(\lambda\)</span>和非零向量<span class="math inline">\(\mathbf{v}\)</span>满足等式<span class="math inline">\(\mathbf{A}\mathbf{v}=\lambda\mathbf{v}\)</span>，那么称<span class="math inline">\(\lambda\)</span>为特征值，<span class="math inline">\(\mathbf{v}\)</span>为特征向量。<span class="math inline">\(\lambda\)</span>为<span class="math inline">\(\mathbf{A}\)</span>的充要条件为矩阵<span class="math inline">\(\lambda\mathbf{I}-\mathbf{A}\)</span>是<strong>奇异矩阵，</strong>即多项式<span class="math inline">\(\textbf{det}\left [ \lambda\mathbf{I}-\mathbf{A} \right ]=0\)</span>，其中<span class="math inline">\(\mathbf{I}\)</span>是<span class="math inline">\(n \times n\)</span>的单位矩阵，多项式为特征多项式，下面的方程为特征方程。</p>
<p><span class="math display">\[\begin{equation}
    \textbf{det}[\lambda \mathbf{I}-\mathbf{A}]=\lambda^{n}+a_{n-1} \lambda^{n-1}+\cdots+a_{1} \lambda+a_{0}=0
\end{equation}\]</span></p>
<p>1、假定方程<span class="math inline">\(\textbf{det}\left [ \lambda\mathbf{I}-\mathbf{A} \right ]=0\)</span>存在<span class="math inline">\(n\)</span>个相异的根<span class="math inline">\(\lambda_{1},\lambda_{2},\cdots,\lambda_{n}\)</span>，那么就存在<span class="math inline">\(n\)</span>个线性无关的向量<span class="math inline">\(\mathbf{\mathcal{v}_{1}},\mathbf{\mathcal{v}_{2}},\cdots,\mathbf{\mathcal{v}_{n}}\)</span>。当矩阵<span class="math inline">\(\mathbf{A}=\mathbf{A}^{\top}\)</span>，则称<span class="math inline">\(\mathbf{A}\)</span>为对称矩阵。</p>
<p>2、对于<span class="math inline">\(n \times n\)</span>实数对称矩阵，其<span class="math inline">\(n\)</span>个特征向量是相互正交的。</p>
<h2 id="正投影">正投影</h2>
<h2 id="矩阵范数">矩阵范数</h2>
<p>矩阵<span class="math inline">\(\mathbf{A}\)</span>的范数记为<span class="math inline">\(\left \| \mathbf{A} \right \|\)</span>，是满足以下条件的任意函数<span class="math inline">\(\left \| \cdot \right \|\)</span></p>
<p>1、如果<span class="math inline">\(\mathbf{A} \neq \mathbf{0}\)</span>，那么就有<span class="math inline">\(\left \| \mathbf{A} \right \| &gt; 0\)</span>，<span class="math inline">\(\left \| \mathbf{0} \right \| = 0\)</span>。</p>
<p>2、对于任意<span class="math inline">\(c \in \mathbb{R}\)</span>，就有<span class="math inline">\(\left \| c\mathbf{A} \right \|=\left | c \right |\left \| \mathbf{A} \right \|\)</span>。</p>
<p>3、<span class="math inline">\(\left \| \mathbf{A}+\mathbf{B} \right \| \leq \left \| \mathbf{A} \right \|+\left \| \mathbf{B} \right \|\)</span></p>
<p>4、<span class="math inline">\(\left \| \mathbf{A} \mathbf{B} \right \| \leq \left \| \mathbf{A} \right \|\left \| \mathbf{B} \right \|\)</span></p>
<p>令</p>
<p><span class="math display">\[\begin{equation}
    \left \| \mathbf{x} \right \|=\sqrt{\left(\sum_{k=1}^{n}\left|x_{k}\right|^{2}\right)}=\sqrt{\langle\mathbf{x}, \mathbf{x}\rangle}
\end{equation}\]</span></p>
<p>则由该向量函数导出矩阵范数为</p>
<p><span class="math display">\[\begin{equation}
    \left \| \mathbf{A} \right \|=\sqrt{\lambda_{1}}
\end{equation}\]</span></p>
<p>其中，<span class="math inline">\(\lambda_{1}\)</span>是矩阵<span class="math inline">\(\mathbf{A}^{\top}\mathbf{A}\)</span>的最大特征矩阵。</p>
<p><strong>瑞利不等式：</strong>如果<span class="math inline">\(n \times n\)</span>矩阵<span class="math inline">\(\mathbf{P}\)</span>是一个实数对称正定矩阵，则有</p>
<p><span class="math display">\[\begin{equation}
    \lambda_{\min }(\mathbf{P})\|\mathbf{x}\|^{2} \leq \mathbf{x}^{\top} \mathbf{P} \mathbf{x} \leq \lambda_{\max }(\mathbf{P})\|\mathbf{x}\|^{2}
\end{equation}\]</span></p>
<p>其中，<span class="math inline">\(\lambda_{\min }(\mathbf{P})\)</span>表示<span class="math inline">\(\mathbf{P}\)</span>的最小特征值，<span class="math inline">\(\lambda_{\max }(\mathbf{P})\)</span>表示<span class="math inline">\(\mathbf{P}\)</span>的最大特征值。</p>
<h1 id="有关几何概念">有关几何概念</h1>
<h2 id="线段">线段</h2>
<p><span class="math inline">\(\mathbf{x}\)</span>和<span class="math inline">\(\mathbf{y}\)</span>是空间<span class="math inline">\(\mathbb{R}^{n}\)</span>中的两个点，<span class="math inline">\(\mathbf{z}\)</span>是两点之间连线上的点，那么就可以得到</p>
<p><span class="math display">\[\begin{equation}
    \mathbf{z}-\mathbf{y}=\alpha \left ( \mathbf{x}-\mathbf{y} \right )
\end{equation}\]</span></p>
<p>其中<span class="math inline">\(\alpha\)</span>是区间<span class="math inline">\(\left [ 0,1 \right ]\)</span>之间的实数，因此上式可写为</p>
<p><span class="math display">\[\begin{equation}
    \mathbf{z}=\alpha \mathbf{x}+\left ( 1-\alpha  \right )\mathbf{y}
\end{equation}\]</span></p>
<p>并且表示为</p>
<p><span class="math display">\[\begin{equation}
    \left \{ \alpha \mathbf{x}+\left ( 1-\alpha  \right )\mathbf{y}:\alpha \in \left [ 0,1 \right ] \right \}
\end{equation}\]</span></p>
<h2 id="超平面和线性簇">超平面和线性簇</h2>
<p>令<span class="math inline">\(u_{1}, u_{2}, \dots, u_{n}, v \in \mathbb{R}\)</span>，其中至少存在一个<span class="math inline">\(u_{i}\)</span>不为零，由所有满足线性方程</p>
<p><span class="math display">\[\begin{equation}
    u_{1} x_{1}+u_{2} x_{2}+\cdots+u_{n} x_{n}=v
\end{equation}\]</span></p>
<p>的点<span class="math inline">\(\mathbf{x}=\left[x_{1}, x_{2}, \cdots, x_{n}\right]^{\top}\)</span>组成的集合成为空间<span class="math inline">\(\mathbb{R}^{n}\)</span>的超平面，可写为</p>
<p><span class="math display">\[\begin{equation}
    \left\{\mathbf{x} \in \mathbb{R}^{n}: \mathbf{u}^{\top} \mathbf{x}=v\right\}
\end{equation}\]</span></p>
<p>当<span class="math inline">\(n\)</span>为2时，即二维空间时，超平面为一条直线方程<span class="math inline">\(u_{1} x_{1}+u_{2} x_{2}=v\)</span>，当<span class="math inline">\(n\)</span>为3时，即三维空间时，超平面为一个面。</p>
<h2 id="凸集">凸集</h2>
<p>已知两点<span class="math inline">\(\mathbf{u},\mathbf{v} \in \mathbb{R}^{n}\)</span>之间的线段可表示为集合</p>
<p><span class="math display">\[\begin{equation}
    \left \{ \mathbf{w}=\mathbb{R}^{n}: \mathbf{w}=\alpha \mathbf{u}+(1-\alpha) \mathbf{v}, \alpha \in[0,1] \right \}
\end{equation}\]</span></p>
<p>如果点<span class="math inline">\(\mathbf{w}=\alpha \mathbf{u}+(1-\alpha) \mathbf{v}\left ( \alpha \in \left [ 0,1 \right ] \right )\)</span>称为点<span class="math inline">\(\mathbf{u}\)</span>和点<span class="math inline">\(\mathbf{v}\)</span>的凸组合。如果<span class="math inline">\(\mathbf{u},\mathbf{v} \in \Theta\)</span>，并且<span class="math inline">\(\mathbf{u}\)</span>和<span class="math inline">\(\mathbf{v}\)</span>之间的线段都在<span class="math inline">\(\Theta\)</span>内，那么<span class="math inline">\(\Theta \in \mathbb{R}^{n}\)</span>为凸集。</p>
<p>凸集的性质：</p>
<p>1、如果<span class="math inline">\(\Theta\)</span>是一个凸集，且<span class="math inline">\(\beta\)</span>是一个实数，那么<span class="math inline">\(\beta \Theta=\{\mathbf{x}: \mathbf{x}=\beta \mathbf{v}, \mathbf{v} \in \Theta\}\)</span>也是凸集。</p>
<p>2、如果<span class="math inline">\(\Theta_{1}\)</span>和<span class="math inline">\(\Theta_{2}\)</span>都是凸集，那么集合<span class="math inline">\(\Theta_{1}+\Theta_{2}=\left\{\mathbf{x}: \mathbf{x}=\mathbf{v}_{1}+\mathbf{v}_{2}, \mathbf{v}_{1} \in \Theta_{1}, \mathbf{v}_{2} \in \Theta_{2}\right\}\)</span>也是凸集。</p>
<p>3、任意多个凸集的交集都是凸集。</p>
]]></content>
      <categories>
        <category>课程</category>
      </categories>
  </entry>
  <entry>
    <title>决策树</title>
    <url>/dt/</url>
    <content><![CDATA[<img src="/dt/dt.png" class title="决策树"><h1 id="原理">原理</h1><p>决策树是一种<strong>非参数监督</strong>学习方法，用于分类和回归任务。对于离散值的构建的树模型，一般为分类树，而用连续值构建的树模型，一般为回归树。</p><blockquote>
<ul>
<li>决策树的本质：从训练集中归纳出一组分类规则，或者说是由训练数据集估计条件概率模型。</li>
<li>决策树学习的损失函数：正则化的极大似然函数。</li>
<li>决策树学习的测试：最小化损失函数。</li>
</ul>
</blockquote><a id="more"></a>



<p>决策树的构造通常分为两个阶段，分别是<code>构造和剪枝</code>。</p>
<p>1、构造：<code>生成一颗完整的树</code>，即在构造的过程总选择什么属性作为节点，树中的节点有三个，分别是：</p>
<p>（1）根节点：树的顶端，即最开始的节点，如上图中的“天气”。</p>
<p>（2）内部节点：树内部的分叉节点，如上图中的“温度”、“湿度”等</p>
<p>（3）叶节点：树最底部的节点，即决策树最终的结果，并且叶节点没有子节点。</p>
<p>在进行树构造时，需要解决三个主要问题：</p>
<p>（1）选择哪个属性作为根节点？</p>
<p>（2）选择哪个属性作为子节点？</p>
<p>（3）停止并获得目标状态的条件？即产生一个叶节点</p>
<p>2、剪枝：<code>为防止过拟合</code>，就需要给决策树瘦身（并不需要精确的判断所有属性）。</p>
<img src="/dt/overfitting.png" class title="过拟合">
<p>上图中的1表示欠拟合，2表示具有较好泛化性，3表示过拟合。</p>
<p><strong>泛化能力：</strong>指分类器是通过训练集获得的分类能力。如果对训练集分类的过于精准，即过度依赖于训练数据，那么得到的决策树其容错率就会比较低，泛化能力差。因为训练集只是全部数据的抽样，并不能体现全部数据的特点。</p>
<p><strong>预剪枝：</strong>在决策树构造时就进行剪枝。即在构造的过程中对节点进行评估，如果对某个节点进行划分，在验证集中不能带来准确性的提升，那么对这个节点进行划分就没有意义，这时就会把当前节点作为<code>叶节点</code>，不对其进行划分。预剪枝降低了过拟合的风险，显著减少了决策树的训练时间开销和测试时间开销，但可能带来欠拟合的风险 。</p>
<p><strong>后剪枝：</strong>在生成决策树之后再进行剪枝。通常会从决策树的叶节点开始，逐层向上对每个节点进行评估。如果剪掉这个节点子树，与保留该节点子树在分类准确性上差别不大，或者剪掉该节点子树，能在验证集中带来准确性的提升，那么就可以把该节点子树进行剪枝。<code>方法是：用该节点子树中使用最频繁的哪个节点来代替该节点。</code>后剪枝的欠拟合风险很小，泛化性能往往优于预剪枝决策树，但其训练时间开销比未剪枝和预剪枝都要大得多。</p>
<h2 id="特点">特点</h2>
<blockquote>
<ul>
<li>优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。</li>
<li>缺点：可能会产生过度匹配的问题。</li>
<li>适用数据类型：数值型和标称型（结果只在有限目标集中取值，如列表值）。</li>
</ul>
</blockquote>
<p>创建分支的伪代码createBranch()如下所示：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">If so <span class="keyword">return</span> 类标签：</span><br><span class="line">Else</span><br><span class="line">    寻找划分数据集的最好特征</span><br><span class="line">    划分数据集</span><br><span class="line">    创建分支节点</span><br><span class="line">        <span class="keyword">for</span> 每个划分的子集</span><br><span class="line">            调用函数createBranch()并增加返回结果到分支节点中</span><br><span class="line">        <span class="keyword">return</span> 分支节点</span><br></pre></td></tr></table></figure>
<h2 id="示例">示例</h2>
<img src="/dt/example1.png" class title="示例">
<p>该示例是用于构造是否去打篮球的决策树，其中的属性有天气、温度、湿度、刮风。</p>
<h1 id="纯度与信息熵">纯度与信息熵</h1>
<h2 id="纯度">纯度</h2>
<p>决策树的构造过程可以理解为寻找纯净划分的过程，纯度则是让目标变量的分歧最小。</p>
<p>例如下面的三个集合：</p>
<blockquote>
<ul>
<li>集合 1：6 次都去打篮球；</li>
<li>集合 2：4 次去打篮球，2 次不去打篮球；</li>
<li>集合 3：3 次去打篮球，3 次不去打篮球。</li>
</ul>
</blockquote>
<p>按照纯度来分，集合 1 &gt; 集合 2 &gt; 集合 3。因为集合 1的分歧最小，集合 3的分歧最大。</p>
<h2 id="信息熵">信息熵</h2>
<p>设<span class="math inline">\(X\)</span>是一个取有限个值的离散随机变量，其概率分布为：</p>
<p>\begin{matrix} P(X=x_{i})=p_{i} &amp; i=1,2,,n \end{matrix}</p>
<p><strong>信息熵：</strong>表示信息的不确定度。由于随机离散事件的出现概率存在着不确定性，因此为了衡量这种信息的不确定性，就是用信息熵来表示。同时，<strong>信息熵也用于在构建树的每个步骤决定要拆分的特征</strong>。</p>
<p>1、经验熵：熵中的概率由数据估计(特别是最大似然估计)得到。</p>
<p><span class="math display">\[\begin{equation}
    Entropy(D)=-\sum_{i=1}^{n}p_{i}\log_{2}p_{i}=-\sum_{i=1}^{n}\frac{\left | c_{k} \right |}{\left | D \right |}\log_{2}\frac{\left | c_{k} \right |}{\left | D \right |}
\end{equation}\]</span></p>
<p>其中<span class="math inline">\(\frac{\left | c_{k} \right |}{\left | D \right |}\)</span>为<span class="math inline">\(\left | c_{k} \right |\)</span>样本数量和总样本数量<span class="math inline">\(\left | D \right |\)</span>的比，当<span class="math inline">\(\log\)</span>底为2时，成为比特熵，以<span class="math inline">\(e\)</span>为底时称为纳特熵。</p>
<p>2、条件熵：已知在随机事件<span class="math inline">\(X\)</span>的条件下随机事件<span class="math inline">\(Y\)</span>的不确定性。</p>
<p><span class="math display">\[\begin{equation}
    Entropy(Y | X)=-\sum_{i=1}^{n}p_{i}H\left ( Y | X = x_{i} \right )
\end{equation}\]</span></p>
<p>其中<span class="math inline">\(p_{i} = P\left (X = x_{i} \right )\)</span>表示条件<span class="math inline">\(x_{i}\)</span>出现的概率，<span class="math inline">\(H\left ( Y | X = x_{i} \right )\)</span>表示事件<span class="math inline">\(x_{i}\)</span>发生时，发生事件<span class="math inline">\(Y\)</span>的概率。</p>
<p><strong>当不确定性越大时，它所包含的信息量也就越大，信息熵也就越高。</strong>以示例中的集合为例：</p>
<blockquote>
<ul>
<li>集合1：<span class="math inline">\(Entropy(t)=-\frac{1}{6}\log_{2}(\frac{6}{6})=0\)</span></li>
<li>集合2：<span class="math inline">\(Entropy(t)=-\frac{4}{6}\log_{2}(\frac{4}{6})-\frac{2}{6}\log_{2}(\frac{2}{6})=0.9\)</span></li>
<li>集合3：<span class="math inline">\(Entropy(t)=-\frac{3}{6}\log_{2}(\frac{3}{6})-\frac{3}{6}\log_{2}(\frac{3}{6})=1\)</span></li>
</ul>
</blockquote>
<p>从结果可以看出，<strong>信息熵越大，纯度越低。当集合中的所有样本均匀混合时，信息熵最大，纯度最低。</strong></p>
<p>因此，在构建决策树时，会基于纯度来进行构建，典型的“不纯度”指标有三种，分别是<strong>信息增益（ID3 算法）、信息增益率（C4.5 算法）以及基尼指数（Cart 算法）。</strong></p>
<h1 id="信息增益id3-算法">信息增益（ID3 算法）</h1>
<h2 id="原理-1">原理</h2>
<p>父亲节点的信息熵减去所有子节点的信息熵。在计算的过程中会计算每个子节点的归一化信息熵，<strong>即按照每个子节点在父节点中出现的概率</strong>来计算这些子节点的信息熵。</p>
<p><span class="math display">\[\begin{equation}
    Gain(D, A)=H(D)-H(D|A)=Entropy(D)-\sum_{i=1}^{k} \frac{\left|D_{i}\right|}{|D|} Entropy\left(D_{i}\right)
\end{equation}\]</span></p>
<p>其中<span class="math inline">\(D\)</span>表示父节点，<span class="math inline">\(D_{i}\)</span>是子节点，<span class="math inline">\(A\)</span>是在父节点<span class="math inline">\(D\)</span>中选择的属性，<span class="math inline">\(\frac{\left|D_{i}\right|}{|D|}\)</span>表示以父节点的分叉属性<span class="math inline">\(A\)</span>为例，子节点在父节点出现的概率。<strong>该式可以理解为由于特征<span class="math inline">\(A\)</span>而使得对数据集<span class="math inline">\(D\)</span>的分类的不确定减少的程度。</strong></p>
<img src="/dt/infogain.png" class title="信息增益">
<p><span class="math display">\[\begin{equation}
    Gain(D, A)=Entropy(D)-\left(\frac{3}{10} Entropy\left(D_{1}\right)+\frac{7}{10} Entropy\left(D_{2}\right)\right)
\end{equation}\]</span></p>
<h2 id="实现步骤">实现步骤</h2>
<blockquote>
<ul>
<li>Step1：根据要分类目标计算其信息熵。</li>
<li>Step2：根据分类目标，为每个属性计算信归一化信息熵。</li>
<li>Step3：根据分类目标和式子（2）计算每个属性的信息增益，并把<strong>信息增益最大的作为根节点</strong>。</li>
<li>Step4：根据根节点的属性值进行分类，计算每一类中信息增益最大的属性，并把该属性作为该节点的分割属性，如下图。</li>
<li>Step5：循环Step4从而得到整棵树。</li>
</ul>
</blockquote>
<img src="/dt/dt-split.png" class title="节点分裂">
<h2 id="缺陷">缺陷</h2>
<p>1、ID3没有剪枝策略，容易过拟合。</p>
<p>2、ID3算法中倾向于选择出现概率比较多的属性。例如，“编号”属性容易将会被选为最优属性。</p>
<p>3、只能用于处理离散分布的特征。</p>
<p>4、没有考虑缺失值。</p>
<p>所以，ID3的缺陷为<strong>当有些属性可能对分类任务没有太大作用，但是他们仍然可能会被选为最优属性</strong>，即对噪声敏感，并且当训练数据如果有少量错误，可能会产生决策树分类错误。</p>
<h1 id="信息增益率c4.5算法">信息增益率（C4.5算法）</h1>
<h2 id="改进">改进</h2>
<p>1、引入悲观剪枝策略进行后剪枝。</p>
<p>2、引入信息增益率作为划分标准。</p>
<p>3、离散化处理连续值。</p>
<p>4、可以处理缺省数据。</p>
<h2 id="原理-2">原理</h2>
<p>因为 ID3 在计算的时候，倾向于选择取值多的属性。为了避免这个问题，C4.5 采用信息增益率的方式来选择属性。</p>
<p><span class="math display">\[\begin{equation}
    \text { 信息增益率 }=\frac{\text { 信息增益 }}{\text { 父节点熵 }}=\frac{H(D)-H(D|A)}{\text { H(D) }}
\end{equation}\]</span></p>
<p>当属性有很多值的时候，相当于被划分成了许多份，<strong>虽然信息增益变大了，但是对于C4.5来说，属性熵也会变大，</strong>所以整体的信息增益率并不大。</p>
<h2 id="剪枝">剪枝</h2>
<p>ID3构造决策树时，容易产生过拟合。而在C4.5中，会在决策树构造之后采用悲观剪枝（PEP），这样可以提升决策树的泛化能力。</p>
<p>悲观剪枝是后剪枝技术中的一种，通过递归估算每个内部节点的损失函数，比较剪枝前后这个节点的损失函数来决定是否对其进行剪枝。<strong>这种剪枝方法的优势是不再需要一个单独的测试数据集。</strong></p>
<p>决策树的损失函数为：</p>
<p><span class="math display">\[\begin{equation}
    C_{\alpha}(T)=\sum_{t=1}^{|T|} N_{t} H_{t}(T)+\alpha|T|=\sum_{t=1}^{|T|} \sum_{k=1}^{K} N_{tk} \log\frac{N_{tk}}{N_{t}}+\alpha|T|=C(T)+\alpha|T|
\end{equation}\]</span></p>
<p>其中<span class="math inline">\(T\)</span>表示子树的叶节点；<span class="math inline">\(H_{t}(T)\)</span>表示第<span class="math inline">\(t\)</span>个叶子的熵；<span class="math inline">\(N_{t}\)</span>表示该叶子所含的训练样例的个数；<span class="math inline">\(\alpha\)</span>表示惩罚系数，并控制模型和训练数据之间拟合程度，当<span class="math inline">\(\alpha\)</span>较大时，树模型较为简单，反之则树模型较为复杂，为0时只考虑模型与训练数据的拟合程度，不考虑模型的复杂度；C(T)表示模型对训练数据的预测误差，即模型与训练数据的拟合程度。</p>
<h3 id="实现步骤-1">实现步骤</h3>
<blockquote>
<ul>
<li>Step1：计算每个结点的经验熵。</li>
<li>Step2：递归的从树的叶结点向上回缩。</li>
</ul>
</blockquote>
<p>设一组叶结点回缩到其父结点之前与之后的整体树分别为<span class="math inline">\(T_{B}\)</span>和<span class="math inline">\(T_{A}\)</span>，其对应的损失函数值分别是<span class="math inline">\(C_{\alpha}(T_{B})\)</span>和<span class="math inline">\(C_{\alpha}(T_{A})\)</span>，如果<span class="math inline">\(C_{\alpha}(T_{A}) \leq C_{\alpha}(T_{B})\)</span>，则进行剪枝，即将父结点变为新的叶结点。</p>
<blockquote>
<ul>
<li>Step3：返回Step3，直到不能继续为止，从而得到损失函数最小的子树<span class="math inline">\(T\)</span>。</li>
</ul>
</blockquote>
<h2 id="离散化处理连续值">离散化处理连续值</h2>
<p>对于处理连续值时，C4.5选择具有最高信息增益属性的属性值作为阈值。</p>
<h2 id="处理缺省数据">处理缺省数据</h2>
<p>当某个属性由于数据缺失，使得只有<span class="math inline">\(B\)</span>条样本，而总样本数为<span class="math inline">\(A\)</span>，那么最后算出的信息增益率为：</p>
<p><span class="math display">\[\begin{equation}
    \text { 某属性的信息增益率 }=\frac{B}{A}*\text { 某属性的实际信息增益率 }
\end{equation}\]</span></p>
<h2 id="缺陷-1">缺陷</h2>
<p>1、C4.5只能用于分类。</p>
<p>2、熵模型中的对数运算、连续值、排序运算都消耗大量运算。</p>
<p>C4.5在ID3的基础上，用信息增益率代替了信息增益，解决了噪声敏感的问题，并且可以对所构造树进行剪枝，同时还能处理连续数值以及数值缺失等情况。<strong>但由于C4.5需要对数据集进行多次扫描，算法效率相对较低。</strong></p>
<h1 id="基尼指数cart算法">基尼指数（CART算法）</h1>
<h2 id="原理-3">原理</h2>
<p>CART的全称是分类与回归树，即该算法既可以用于分类问题，也可以用于回归问题，并且使用CART生成的树只能是二叉树。</p>
<p>1、回归树：使用<strong>平方误差最小化准则</strong>来选择特征并进行划分。每一个叶节点给出的预测值是划分到该叶子节点的所有样本目标值的均值，这样只是在给定划分的情况下最小化了平方误差。</p>
<p>2、分类树：使用<strong>基尼指数（GINI）最小化准则</strong>来选择特征并进行划分。基尼指数表示集合的不确定性，或者是不纯度。基尼指数越大，集合不确定性越高，不纯度也越大。</p>
<h3 id="算法步骤">算法步骤</h3>
<p><strong>1、决策树生成：</strong>基于训练数据集生成决策树，生成的决策树要尽量大。</p>
<p><strong>2、决策树剪枝：</strong>用于验证数据集对已生成的数进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准。</p>
<h2 id="回归树">回归树</h2>
<p>假设一组训练数据为</p>
<p><span class="math display">\[\begin{equation}
    D=\left \{ \left ( x_{1},y_{1} \right ), \left ( x_{2},y_{2} \right ),\cdots,\left ( x_{N},y_{N} \right ) \right \}
\end{equation}\]</span></p>
<p>所谓回归树就把输入空间进行划分，并且在每个划分的单元上进行输出。如果把空间划分为<span class="math inline">\(M\)</span>个单元<span class="math inline">\(R_{1},R_{2},\cdots,R_{M}\)</span>，并且在每个单元<span class="math inline">\(R_{m}\)</span>上有一个固定的输出<span class="math inline">\(c_{m}\)</span>，那么回归树就可以表示为：</p>
<p><span class="math display">\[\begin{equation}
    f\left ( x \right )=\sum_{m=1}^{M}c_{m}I\left ( x \in R_{m} \right )
\end{equation}\]</span></p>
<p>并且回归树使用平方误差最小准则求每个单元<span class="math inline">\(R_{m}\)</span>上的最有输出。由平方误差最小准则就可以知道单元<span class="math inline">\(R_{m}\)</span>上的<span class="math inline">\(c_{m}\)</span>最优值<span class="math inline">\(\hat{c_{m}}\)</span>为所有输入实例输出的<span class="math inline">\(y_{i}\)</span>的均值，即：</p>
<p><span class="math display">\[\begin{equation}
    \hat{c}_{m}=ave\left(y_{i} | x_{i} \in R_{m}\right)=\frac{1}{N_{m}}\sum_{x_{i} \in R_{m}(j,s)}y_{i},\ x \in R_{m},\ m=1,2
\end{equation}\]</span></p>
<p>在CART回归树中，输入空间的划分采用启发式方法，即选择第<span class="math inline">\(j\)</span>个变量<span class="math inline">\(x^{(j)}\)</span>和空间取值<span class="math inline">\(s\)</span>作为分割变量和分割点，并通过分割点定义两个区域：</p>
<p><span class="math display">\[\begin{equation}
    R_{1}\left ( j,s \right )=\left \{ x|x^{\left ( j \right )} \leq s \right \}\ R_{2}\left ( j,s \right )=\left \{ x|x^{\left ( j \right )} &gt; s \right \}
\end{equation}\]</span></p>
<p>因此，最优的分割变量<span class="math inline">\(j\)</span>和分割点<span class="math inline">\(s\)</span>选择就是求解下面的式子：</p>
<p><span class="math display">\[\begin{equation}
    \min_{j, s}\left[\min_{c_{1}} \sum_{x_{i} \in R_{1}(j,s)}\left(y_{i}-c_{1}\right)^{2}+\min_{c_{2}} \sum_{x_{i} \in R_{2}(j,s)}\left(y_{i}-c_{2}\right)^{2}\right]
\end{equation}\]</span></p>
<p>并且对于固定输入<span class="math inline">\(j\)</span>，可以球的最优切点<span class="math inline">\(s\)</span>，使得<span class="math inline">\(\hat{c}_{1}\)</span>和<span class="math inline">\(\hat{c}_{2}\)</span>最优。</p>
<p><span class="math display">\[\begin{equation}
    \hat{c}_{1}=ave\left(y_{i} | x_{i} \in R_{1}(j,s)\right)\ \hat{c}_{2}=ave\left(y_{i} | x_{i} \in R_{2}(j,s)\right)
\end{equation}\]</span></p>
<h3 id="损失函数">损失函数</h3>
<p>回归树使用平方误差来表示回归树对训练数据的预测误差。</p>
<p><span class="math display">\[\begin{equation}
    \sum_{x_{i} \in R_{m}}\left(y_{i}-f\left(x_{i}\right)\right)^{2}
\end{equation}\]</span></p>
<h2 id="分类树">分类树</h2>
<p><span class="math display">\[\begin{equation}
    Gini(D)=1-\sum_{k=1}^{K}\left ( \frac{\left | C_{k} \right |}{\left | D \right |} \right )^2
\end{equation}\]</span></p>
<p>其中<span class="math inline">\(\left | C_{k} \right |\)</span>表示<span class="math inline">\(D\)</span>中属于第<span class="math inline">\(k\)</span>类样本的个数，<span class="math inline">\(\left | D \right |\)</span>数据集的个数，<span class="math inline">\(K\)</span>是类的个数。</p>
<p>如果样本集合<span class="math inline">\(D\)</span>中根据特征<span class="math inline">\(A\)</span>的某个取值<span class="math inline">\(a\)</span>来把<span class="math inline">\(D\)</span>分割为<span class="math inline">\(D_{1}\)</span>和<span class="math inline">\(D_{2}\)</span>，那么在特征A的条件下，集合D的基尼指数为：</p>
<p><span class="math display">\[\begin{equation}
    Gini(D, A)=\frac{\left|D_{1}\right|}{D} Gini\left(D_{1}\right)+\frac{\left|D_{2}\right|}{D} Gini\left(D_{2}\right)
\end{equation}\]</span></p>
<p>其中<span class="math inline">\(Gini(D, A)\)</span>表示经<span class="math inline">\(A=a\)</span>分割后集合<span class="math inline">\(D\)</span>的不确定性。基尼指数值越大，样本集合的不确定性就越大。<strong>在树生成时选择基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点。</strong></p>
<h2 id="剪枝-1">剪枝</h2>
<p>决策树的损失函数为：</p>
<p><span class="math display">\[\begin{equation}
    C_{\alpha}(T)=C(T)+\alpha|T|
\end{equation}\]</span></p>
<p>其中<span class="math inline">\(T\)</span>为任意子树；<span class="math inline">\(C(T)\)</span>为预测误差，用来衡量模型与训练数据的拟合程度；<span class="math inline">\(|T|\)</span>为子树<span class="math inline">\(T\)</span>的叶子节点个数，即树的复杂度；<span class="math inline">\(\alpha\)</span>为惩罚系数，用来控制模型和训练数据之间拟合程度，当<span class="math inline">\(\alpha\)</span>较大时，树模型较为简单，反之则树模型较为复杂。并且当<span class="math inline">\(\alpha\)</span>从0开始不断增加到<span class="math inline">\(+\infty\)</span>时，会产生一系列的子树序列：<span class="math inline">\(T_{0},T_{1},\cdots,T_{n}\)</span>，并且每一个<span class="math inline">\(T_{i+1}\)</span>子树都是由前一个子树<span class="math inline">\(T_{i}\)</span>剪掉某一个内部节点来生成的。</p>
<p>对于任意一个内部节点<span class="math inline">\(t\)</span>，剪枝前有<span class="math inline">\(|T_{t}|\)</span>个叶子节点，并且预测误差为<span class="math inline">\(C(T_{t})\)</span>，剪枝后因为只有一个叶节点<span class="math inline">\(t\)</span>，因此预测误差为<span class="math inline">\(C(t)\)</span>，，因此损失函数的变化为：</p>
<p>剪枝前：</p>
<p><span class="math display">\[\begin{equation}
    C_{\alpha}(T)=C(T)+\alpha|T|
\end{equation}\]</span></p>
<p>剪枝后：</p>
<p><span class="math display">\[\begin{equation}
    C_{\alpha}(t)=C(t)+\alpha \times 1=C(t)+\alpha
\end{equation}\]</span></p>
<p>要是剪枝后的损失函数与剪枝前的损失函数相同<span class="math inline">\(C(T)+\alpha|T|=C(t)+\alpha\)</span>，就可以得到：</p>
<p><span class="math display">\[\begin{equation}
    \alpha=\frac{C(t)-C\left(T_{t}\right)}{\left|T_{t}\right|-1}
\end{equation}\]</span></p>
<p>其中，<span class="math inline">\(\alpha\)</span>的意义在于在<span class="math inline">\(\left [ \alpha_{i}, \alpha_{i+1} \right )\)</span>中惩罚系数的临界值，如果比该<span class="math inline">\(\alpha\)</span>大，那么一定有<span class="math inline">\(C_{\alpha}(T) &gt; C_{\alpha}(t)\)</span>，即剪掉这个节点后都比不剪掉要更优。</p>
<p>对于分类树来说采用叶子节点里概率最大的类别作为当前对象的预测类别。而在回归树中，则是采用最终叶子的均值或者中位数作为预测结果输出。</p>
<h2 id="优点">优点</h2>
<blockquote>
<ul>
<li>基尼指数的计算不需要对数运算，更加高效。</li>
<li>基尼指数更偏向于连续属性，熵更偏向于离散属性。</li>
</ul>
</blockquote>
<h1 id="参考资料">参考资料</h1>
<p>[1] <a href="https://www.cnblogs.com/molieren/articles/10664954.html" target="_blank" rel="noopener">决策树</a></p>
<p>[2] <a href="https://blog.csdn.net/jiaoyangwm/article/details/79525237#3__680" target="_blank" rel="noopener">机器学习实战（三）——决策树</a></p>
<p>[3] <a href="https://zhuanlan.zhihu.com/p/85731206" target="_blank" rel="noopener">【机器学习】决策树</a></p>
]]></content>
      <categories>
        <category>基础</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>决策树</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习资料</title>
    <url>/mlresource/</url>
    <content><![CDATA[<h1 id="国际顶级会议">国际顶级会议</h1>
<h2 id="aaai"><a href="http://www.aaai.org/Conferences/AAAI/aaai.php" target="_blank" rel="noopener">AAAI</a></h2>
<h2 id="cikm-2010"><a href="http://www.yorku.ca/cikm10/papers.php" target="_blank" rel="noopener">CIKM 2010</a></h2>
<h2 id="cikm-2011"><a href="http://www.cikm2011.org/" target="_blank" rel="noopener">CIKM 2011</a></h2>
<a id="more"></a>
<h2 id="colt-2010"><a href="http://www.colt2010.org/" target="_blank" rel="noopener">COLT 2010</a></h2>
<h2 id="colt-2011"><a href="http://colt2011.sztaki.hu/" target="_blank" rel="noopener">COLT 2011</a></h2>
<h2 id="computer-vision-resource"><a href="http://www.cvpapers.com/index.html" target="_blank" rel="noopener">Computer Vision Resource</a></h2>
<h2 id="icjia"><a href="http://ijcai.org/" target="_blank" rel="noopener">ICJIA</a></h2>
<h2 id="icml"><a href="http://www.machinelearning.org/icml.html" target="_blank" rel="noopener">ICML</a></h2>
<h2 id="nips"><a href="http://books.nips.cc/" target="_blank" rel="noopener">NIPS</a></h2>
<h2 id="sigir-2010"><a href="http://www.sigir2010.org/doku.php?id=program:sessions" target="_blank" rel="noopener">SIGIR 2010</a></h2>
<h2 id="sigir-2011"><a href="http://www.sigir2011.org/" target="_blank" rel="noopener">SIGIR 2011</a></h2>
<h2 id="sigkdd"><a href="http://www.kdd.org/kdd2011/" target="_blank" rel="noopener">SIGKDD</a></h2>
<h2 id="sigkdd2010"><a href="http://www.kdd.org/kdd2010/papers.shtml" target="_blank" rel="noopener">SIGKDD2010</a></h2>
<h1 id="论文搜索">论文搜索</h1>
<h2 id="cv顶级会议论文下载"><a href="http://cvpapers.com/" target="_blank" rel="noopener">CV顶级会议论文下载</a></h2>
<h2 id="google-学术搜索"><a href="http://scholar.google.com.hk/schhp?hl=zh-CN&amp;tab=ws" target="_blank" rel="noopener">google 学术搜索</a></h2>
<h2 id="超全计算机视觉资源汇总"><a href="http://www.visionbib.com/bibliography/contents.html" target="_blank" rel="noopener">超全计算机视觉资源汇总</a></h2>
<h2 id="联合参考文献"><a href="http://www.ucdrs.net/admin/union/index.do" target="_blank" rel="noopener">联合参考文献</a></h2>
<h1 id="学术牛人主页">学术牛人主页</h1>
<h2 id="feifei-li--computer-vision"><a href="http://vision.stanford.edu/" target="_blank" rel="noopener">feifei li -computer vision</a></h2>
<h2 id="googlers-in-machine-learning"><a href="http://research.google.com/pubs/MachineLearning.html" target="_blank" rel="noopener">Googlers in Machine Learning</a></h2>
<h2 id="michael-i.-jordan"><a href="http://www.cs.berkeley.edu/~jordan/" target="_blank" rel="noopener">Michael I. Jordan</a></h2>
<h2 id="microsoft-research"><a href="http://research.microsoft.com/apps/dp/areas.aspx" target="_blank" rel="noopener">Microsoft Research</a></h2>
<h2 id="mit-leozhu-cv"><a href="http://people.csail.mit.edu/leozhu/" target="_blank" rel="noopener">mit leozhu cv</a></h2>
<h2 id="pff-cv"><a href="http://www.cs.brown.edu/~pff/" target="_blank" rel="noopener">pff cv</a></h2>
<h2 id="yahoo-research"><a href="http://research.yahoo.com/publication" target="_blank" rel="noopener">Yahoo! Research</a></h2>
<h2 id="zhangzhang-si"><a href="http://www.stat.ucla.edu/~zzsi/" target="_blank" rel="noopener">zhangzhang si</a></h2>
<h2 id="国外人工智能界牛人主页"><a href="http://caiqi1123.blog.163.com/blog/static/5736178120080213836366/" target="_blank" rel="noopener">国外人工智能界牛人主页</a></h2>
<h2 id="计算机视觉相关资源"><a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/" target="_blank" rel="noopener">计算机视觉相关资源</a></h2>
<h2 id="牛人周志华推荐的人工智能网站"><a href="http://blog.csdn.net/huzhyi21/archive/2009/09/20/4573476.aspx" target="_blank" rel="noopener">牛人（周志华）推荐的人工智能网站</a></h2>
<h2 id="数据挖掘牛人-一览"><a href="http://blog.csdn.net/dnnyyq/archive/2010/01/24/5250935.aspx" target="_blank" rel="noopener">数据挖掘牛人 一览</a></h2>
<h2 id="谈机器学习machine-learning大牛"><a href="http://blog.sina.com.cn/s/blog_591e979d0100kds5.html" target="_blank" rel="noopener">谈机器学习(Machine Learning)大牛</a></h2>
<h2 id="周志华"><a href="http://cs.nju.edu.cn/zhouzh/" target="_blank" rel="noopener">周志华</a></h2>
<h1 id="学术期刊">学术期刊</h1>
<h2 id="ieee-transactions-pattern-analysis-and-machine-intelligence"><a href="http://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34" target="_blank" rel="noopener">IEEE Transactions Pattern Analysis and Machine Intelligence</a></h2>
<h2 id="acm-transactions-on-knowledge-discovery-from-data"><a href="http://portal.acm.org/citation.cfm?id=J1054&amp;picked=prox&amp;cfid=16966702&amp;cftoken=61328876" target="_blank" rel="noopener">ACM Transactions on Knowledge Discovery from Data</a></h2>
<h2 id="american-statistical-association---journal-of-the-american-statistical-association"><a href="http://pubs.amstat.org/loi/jasa" target="_blank" rel="noopener">American Statistical Association - Journal of the American Statistical Association</a></h2>
<h2 id="annals-of-statistics"><a href="http://www.imstat.org/aos/" target="_blank" rel="noopener">Annals of Statistics</a></h2>
<h2 id="artificial-intelligent"><a href="http://www.sciencedirect.com/science/journal/00043702" target="_blank" rel="noopener">Artificial Intelligent</a></h2>
<h2 id="computer-vision-and-image-understanding"><a href="http://www.sciencedirect.com/science/journal/10773142/116" target="_blank" rel="noopener">Computer Vision and Image Understanding</a></h2>
<h2 id="data-mining-and-knowledge-discovery"><a href="http://springer.lib.tsinghua.edu.cn/content/1384-5810/" target="_blank" rel="noopener">Data Mining and Knowledge Discovery</a></h2>
<h2 id="ieee-knowledge-and-data-engineering"><a href="http://ieeexplore.ieee.org/xpl/tocresult.jsp?isnumber=4358933" target="_blank" rel="noopener">IEEE Knowledge and Data Engineering</a></h2>
<h2 id="ieee-t.-on-information-theory"><a href="http://ieeexplore.ieee.org/xpl/tocresult.jsp?isnumber=6126885" target="_blank" rel="noopener">IEEE T. on Information Theory</a></h2>
<h2 id="ieee-t.-on-neural-networks"><a href="http://ieeexplore.ieee.org/xpl/tocresult.jsp?isnumber=6099846" target="_blank" rel="noopener">IEEE T. on Neural Networks</a></h2>
<h2 id="ieee-t.-on-systems-machine-and-cybernetics"><a href="http://ieeexplore.ieee.org/xpl/tocresult.jsp?isnumber=10230" target="_blank" rel="noopener">IEEE T. on Systems Machine and Cybernetics</a></h2>
<h2 id="image-processing-ieee-transactions-on"><a href="http://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83" target="_blank" rel="noopener">Image Processing, IEEE Transactions on</a></h2>
<h2 id="image-vision-computing"><a href="http://www.sciencedirect.com/science/journal/02628856" target="_blank" rel="noopener">Image Vision Computing</a></h2>
<h2 id="international-journal-of-computer-vision"><a href="http://springer.lib.tsinghua.edu.cn/content/0920-5691/" target="_blank" rel="noopener">International Journal of Computer Vision</a></h2>
<h2 id="journal-of-machine-learning-research"><a href="http://jmlr.csail.mit.edu/" target="_blank" rel="noopener">Journal of Machine Learning Research</a></h2>
<h2 id="journal-of-the-royal-statistical-society-series-b-statistical-methodology"><a href="http://onlinelibrary.wiley.com/journal/10.1111/(ISSN)1467-9868" target="_blank" rel="noopener">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</a></h2>
<h2 id="machine-learning"><a href="http://springer.lib.tsinghua.edu.cn/content/0885-6125/" target="_blank" rel="noopener">Machine Learning</a></h2>
<h2 id="neural-computation"><a href="http://www.mitpressjournals.org/toc/neco/24/3" target="_blank" rel="noopener">Neural Computation</a></h2>
<h2 id="pattern-analysis-and-applications"><a href="http://www.springerlink.com/content/1433-7541" target="_blank" rel="noopener">Pattern Analysis and Applications</a></h2>
<h2 id="pattern-recognition"><a href="http://www.sciencedirect.com/science/journal/00313203" target="_blank" rel="noopener">Pattern Recognition</a></h2>
<h1 id="牛人主页主页有很多论文代码">牛人主页（主页有很多论文代码）</h1>
<h2 id="serge-belongie-at-uc-san-diego"><a href="http://cseweb.ucsd.edu/~sjb/" target="_blank" rel="noopener">Serge Belongie at UC San Diego</a></h2>
<h2 id="antonio-torralba-at-mit"><a href="http://web.mit.edu/torralba/www/" target="_blank" rel="noopener">Antonio Torralba at MIT</a></h2>
<h2 id="alexei-ffros-at-cmu"><a href="http://www.cs.cmu.edu/~efros/" target="_blank" rel="noopener">Alexei Ffros at CMU</a></h2>
<h2 id="ce-liu-at-microsoft-research-new-england"><a href="http://people.csail.mit.edu/celiu/" target="_blank" rel="noopener">Ce Liu at Microsoft Research New England</a></h2>
<h2 id="vittorio-ferrari-at-univ.of-edinburgh"><a href="http://www.vision.ee.ethz.ch/~calvin/" target="_blank" rel="noopener">Vittorio Ferrari at Univ.of Edinburgh</a></h2>
<h2 id="kristen-grauman-at-ut-austin"><a href="http://www.cs.utexas.edu/~grauman/" target="_blank" rel="noopener">Kristen Grauman at UT Austin</a></h2>
<h2 id="devi-parikh-at-tti-chicago-marr-prize-at-iccv2011"><a href="http://ttic.uchicago.edu/~dparikh/index.html" target="_blank" rel="noopener">Devi Parikh at TTI-Chicago (Marr Prize at ICCV2011)</a></h2>
<h2 id="john-wright-at-columbia-univ."><a href="http://www.columbia.edu/~jw2966/" target="_blank" rel="noopener">John Wright at Columbia Univ.</a></h2>
<h2 id="piotr-dollar-at-caltech"><a href="http://vision.ucsd.edu/~pdollar/" target="_blank" rel="noopener">Piotr Dollar at CalTech</a></h2>
<h2 id="boris-babenko-at-uc-san-diego"><a href="http://vision.ucsd.edu/~bbabenko/" target="_blank" rel="noopener">Boris Babenko at UC San Diego</a></h2>
<h2 id="david-ross-at-googleyoutube"><a href="http://www.cs.toronto.edu/~dross/" target="_blank" rel="noopener">David Ross at Google/Youtube</a></h2>
<h2 id="david-donoho-at-stanford-univ."><a href="http://www-stat.stanford.edu/~donoho/index.html" target="_blank" rel="noopener">David Donoho at Stanford Univ.</a></h2>
]]></content>
      <categories>
        <category>资源</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>多标签学习算法的综述</title>
    <url>/review-multilabel-alg/</url>
    <content><![CDATA[<h1 id="介绍">介绍</h1><p>在传统的单标签监督学习中，<span class="math inline">\(x\)</span>表示一个实体空间，<span class="math inline">\(y\)</span>表示一个标签空间，而传统的监督学习就是通过训练集</p><p><span class="math display">\[\begin{align}
    \left \{ \left ( x_{i} | y_{i} \right ) | 1 \leq i \leq m \right \}
\end{align}\]</span></p><a id="more"></a>


<p>练一个方法使得<span class="math inline">\(f:\mathcal{X} \rightarrow \mathcal{Y}\)</span>，并且<span class="math inline">\(x_{i} \in \mathcal{X}\)</span>表示一个特征属性的对象，而<span class="math inline">\(y_{i} \in \mathcal{Y}\)</span>这事该特征对应的语义标签。</p>
<p>与传统的单标签的监督学习相比，多标签学习中每一个对象都是单独的一个实体，并且这个对象具有一组标签集，而不是一个标签。因此，多标签学习的任务就是构造能够预测实体所具有的标签集的方法。早期的多标签学习主要应用于文本分类，然而在过去十年间，多标签学习已经被广泛的应用于图像、生物信息等领域。</p>
<p>这篇文章对多标签学习进行了回顾，主要分为三个部分：</p>
<p>1、多标签学习的原理（包括学习框架、关键挑战、阈值校准）和评价指标（包括基于实例的评价、基于标签的评价、理论结果）</p>
<p>2、对八种最具代表性的算法进行总结和讨论</p>
<p>3、简要总结了几种相关的学习设定</p>
<h1 id="范式">范式</h1>
<h2 id="形式定义">形式定义</h2>
<p>变量及意义</p>
<table>
<colgroup>
<col style="width: 16%">
<col style="width: 83%">
</colgroup>
<thead>
<tr class="header">
<th>符号</th>
<th style="text-align: left;">数学意义</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\mathcal{X}\)</span></td>
<td style="text-align: left;">表示<span class="math inline">\(d\)</span>维的实体空间<span class="math inline">\(\mathbb{R}^{d}\)</span>（或<span class="math inline">\(\mathbb{Z}^{d}\)</span>）</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\mathcal{Y}\)</span></td>
<td style="text-align: left;">表示带有<span class="math inline">\(q\)</span>个可能标签的标签空间<span class="math inline">\(\left \{ y_{1}, y_{2}, \cdots , y_{q} \right \}\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(x\)</span></td>
<td style="text-align: left;">表示具有<span class="math inline">\(d\)</span>维的特征向量实体<span class="math inline">\(x\)</span>，<span class="math inline">\(\left ( x_{1}, x_{2}, \cdots , x_{d} \right )^{T} \left ( x \in \mathcal{X} \right )\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(Y\)</span></td>
<td style="text-align: left;">表示与某个实体<span class="math inline">\(x\)</span>的相关标签集<span class="math inline">\(\left ( Y \in \mathcal{Y} \right )\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\bar{Y}\)</span></td>
<td style="text-align: left;">表示在<span class="math inline">\(\mathcal{Y}\)</span>中<span class="math inline">\(Y\)</span>的补集</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\mathcal{D}\)</span></td>
<td style="text-align: left;">表示多标签训练集<span class="math inline">\(\left \{ \left ( x_{i}, Y_{i} \right ) \mid 1 \leq i \leq m \right \}\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\mathcal{S}\)</span></td>
<td style="text-align: left;">表示多标签测试集<span class="math inline">\(\left \{ \left ( x_{i}, Y_{i} \right ) \mid 1 \leq i \leq p \right \}\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(f\left ( \cdot , \cdot \right )\)</span></td>
<td style="text-align: left;">表示一个实值函数<span class="math inline">\(f: \mathcal{X} \times \mathcal{Y} \rightarrow \mathbb{R}\)</span>, <span class="math inline">\(f\left ( x, y \right )\)</span>返回<span class="math inline">\(x\)</span>每个可能标签<span class="math inline">\(y\)</span>的置信度。简单说，当与<span class="math inline">\(x\)</span>相关的标签<span class="math inline">\({y}&#39; \in Y\)</span>时，那么置信度函数<span class="math inline">\(f\)</span>返回最大值，当与<span class="math inline">\(x\)</span>不相关的标签<span class="math inline">\({y}&#39;&#39; \notin Y\)</span>时，那么置信度函数<span class="math inline">\(f\)</span>返回最小值，即<span class="math inline">\(f\left ( x,{y}&#39; \right ) &gt; f\left ( x, {y}&#39;&#39; \right )\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(t\left ( \cdot \right )\)</span></td>
<td style="text-align: left;">表示阈值函数<span class="math inline">\(t: \mathcal{X} \rightarrow \mathbb{R}\)</span>, 即该函数能够把<span class="math inline">\(\mathcal{X}\)</span>映射到<span class="math inline">\(\mathbb{R}\)</span>上，并且对于分类器<span class="math inline">\(h\left ( \cdot \right )\)</span>来说只有当实值函数<span class="math inline">\(f\left ( \cdot , \cdot \right )\)</span>的值大于阈值函数<span class="math inline">\(t\left ( \cdot \right )\)</span>时，分类器<span class="math inline">\(h\left ( \cdot \right )\)</span>才能返回可能的标签集，数学表达为： <span class="math inline">\(h\left ( x \right ) = \left \{ y \mid f \left ( x, y \right ) &gt; t \left ( x \right ) , y \in \mathcal{Y} \right \}\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(h\left ( \cdot \right )\)</span></td>
<td style="text-align: left;">表示一个多标签分类器函数<span class="math inline">\(h: \mathcal{X} \rightarrow 2^{\mathcal{Y}}\)</span>, <span class="math inline">\(h\left ( x \right )\)</span>返回<span class="math inline">\(x\)</span>可能的标签集，该函数由置信度函数<span class="math inline">\(f\left ( \cdot , \cdot \right )\)</span>和阈值函数<span class="math inline">\(t\left ( \cdot \right )\)</span>组合得到<span class="math inline">\(h\left ( x \right ) = \left \{ y \mid f \left ( x, y \right ) &gt; t \left ( x \right ) , y \in \mathcal{Y} \right \}\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(rank_{f}\left ( \cdot , \cdot \right )\)</span></td>
<td style="text-align: left;">表示通过<span class="math inline">\(f\left ( x , \cdot \right )\)</span>函数获得每个在相关标签集<span class="math inline">\(\mathcal{Y}\)</span>中的标签<span class="math inline">\(y\)</span>的置信度，并对这些置信度进行降序排名</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\left | \cdot \right |\)</span></td>
<td style="text-align: left;">表示以集合<span class="math inline">\(\mathcal{A}\)</span>为例，<span class="math inline">\(\left | \mathcal{A} \right |\)</span>返回集合<span class="math inline">\(\mathcal{A}\)</span>的基数</td>
</tr>
<tr class="odd">
<td><span class="math inline">\([\![ \cdot ]\!]\)</span></td>
<td style="text-align: left;">表示当<span class="math inline">\(\pi\)</span>成立时<span class="math inline">\([\![ \pi ]\!]\)</span>返回数字1，否则返回0</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\phi \left ( \cdot , \cdot \right )\)</span></td>
<td style="text-align: left;">表示当<span class="math inline">\(y \in Y\)</span>时，<span class="math inline">\(\phi \left ( Y , y \right )\)</span>返回+1，否则返回-1</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\mathcal{D}_{j}\)</span></td>
<td style="text-align: left;">表示从多标签训练集<span class="math inline">\(\mathcal{D}\)</span>中的第<span class="math inline">\(j\)</span>类标签<span class="math inline">\(y_{j}\)</span>得到的二元训练集<span class="math inline">\(\left \{ \left ( x_{i}, \phi \left ( Y_{i}, y_{i} \right ) \right ) \mid 1 \leq i \leq m \right \}\)</span>，即当遍历所有的<span class="math inline">\(x_{i}\)</span>所对应的相关标签集<span class="math inline">\(Y_{i}\)</span>，当<span class="math inline">\(y_{j} \in Y_{i}\)</span>时，那么上式<span class="math inline">\(\left ( x_{i}, \phi \left ( Y_{i}, y_{i} \right ) \right )\)</span>就为1，否则为0</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\psi \left ( \cdot ,\cdot ,\cdot \right )\)</span></td>
<td style="text-align: left;">表示假如<span class="math inline">\(y_{j} \in Y\)</span>，并且<span class="math inline">\(y_{k} \notin Y\)</span>，那么<span class="math inline">\(\psi \left ( Y, y_{j}, y_{k} \right )\)</span>返回+1，而假如<span class="math inline">\(y_{j} \notin Y\)</span>，并且<span class="math inline">\(y_{k} \in Y\)</span>，那么<span class="math inline">\(\psi \left ( Y, y_{j}, y_{k} \right )\)</span>返回-1</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\mathcal{D}_{jk}\)</span></td>
<td style="text-align: left;">表示从多标签训练集<span class="math inline">\(\mathcal{D}\)</span>中的标签对<span class="math inline">\(\left(y_{j}, y_{k}\right)\)</span>得到的二元训练集<span class="math inline">\(\left \{ \left ( x_{i}, \psi \left ( Y_{i}, y_{j}, y_{k} \right ) \right ) \mid \phi \left ( Y_{i},y_{j} \neq \phi \left ( Y_{i}, y_{k} \right ), 1 \leq i \leq m \right ) \right \}\)</span>，即当遍历所有的<span class="math inline">\(x_{i}\)</span>所对应的相关标签集<span class="math inline">\(Y_{i}\)</span>时，<span class="math inline">\(y_{j}\)</span>和<span class="math inline">\(y_{k}\)</span>有且只有一个属于<span class="math inline">\(Y_{i}\)</span>时，那么上式<span class="math inline">\(\left ( x_{i}, \psi \left ( Y_{i}, y_{j}, y_{k} \right ) \right )\)</span>就为1，否则为-1</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\sigma_{\mathcal{Y}}\left ( \cdot \right )\)</span></td>
<td style="text-align: left;">表示一个把<span class="math inline">\(\mathcal{Y}\)</span>的幂集映射到正整数的映射函数<span class="math inline">\(\sigma_{\mathcal{Y}}:2^{\mathcal{Y}} \rightarrow \mathbb{N}\)</span>（<span class="math inline">\(\sigma_{\mathcal{Y}}^{-1}\)</span>表示逆函数）</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\mathcal{D}_{\mathcal{Y}}^{\dagger}\)</span></td>
<td style="text-align: left;">表示从训练集<span class="math inline">\(\mathcal{D}\)</span>获得多类单标签训练集<span class="math inline">\(\left \{ \left ( x_{i}, \sigma_{\mathcal{Y}}\left ( Y_{i} \right ) \right ) \mid 1 \leq i \leq m \right \}\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\mathcal{B}\)</span></td>
<td style="text-align: left;">表示二元学习算法【复杂度：<span class="math inline">\(\mathcal{F}_{\mathcal{B}}\left ( m,d \right )\)</span>为训练集的复杂度，<span class="math inline">\(\mathcal{F}_{\mathcal{B}}^{&#39;}\left ( d \right )\)</span>为每个实例的测试集复杂度】</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\mathcal{M}\)</span></td>
<td style="text-align: left;">表示多类学习算法【复杂度：<span class="math inline">\(\mathcal{F}_{\mathcal{M}}\left ( m,d,q \right )\)</span>为训练集的复杂度，<span class="math inline">\(\mathcal{F}_{\mathcal{M}}^{&#39;}\left ( d,q \right )\)</span>为每个实例的测试集复杂度】</td>
</tr>
</tbody>
</table>
<h3 id="学习框架">学习框架</h3>
<p>有多种有效的多标签指标可以用于描述多标签数据集的特征。其中最直接的方法就是首先测量多标签数据集的方法是平均标签基数：<span class="math inline">\(LCard\left ( \mathcal{D} \right ) = \frac{1}{m} \sum_{i = 1}^{m} \left | Y_{i} \right |\)</span>，即遍历所有<span class="math inline">\(x_{i}\)</span>所对应的相关标签集<span class="math inline">\(Y_{i}\)</span>的基数，并且求和在求平均，从而得到标签集的平均标签基数。然后通过标签基数归一化得到标签密度<span class="math inline">\(LDen\left ( \mathcal{D} \right ) = \frac{1}{\left | \mathcal{Y} \right |} \cdot LCard \left ( \mathcal{D} \right )\)</span>。另一个广泛用于测量多标签数据集多样性的方法是：<span class="math inline">\(LDiv \left ( \mathcal{D} \right ) = \left | \left \{ Y \mid \exists x : \left ( x, Y \right ) \in \mathcal{D} \right \} \right |\)</span>，即在某个多标签数据集中出现不同标签集的数量。同样，标签的多样性可以通过实例的数量来进行归一化，以表示不同标签集的比例：<span class="math inline">\(PLDiv \left ( \mathcal{D} \right ) = \frac{1}{\left | \mathcal{D} \right |} \cdot LDiv\left ( \mathcal{D} \right )\)</span>。</p>
<h3 id="主要挑战">主要挑战</h3>
<p>多标签学习的主要挑战是随着标签种类的增加，标签集的增加呈指数级增长。假如有20种标签（<span class="math inline">\(q = 20\)</span>），那么可能的标签集就有<span class="math inline">\(2^{20}\)</span>个。</p>
<p>为了应对标签集的指数增长，就需要在学习过程中增加标签与标签之间的关系或者依赖。因此，利用标签之间的关系就是多标签学习成功的关键。目前增加标签和标签之间的关联按照所使用方法的顺序，主要包含三种：</p>
<p>1、一阶策略：一阶策略把多标签学习转化为多个不相关的二元分类问题，即对每个标签进行分类。该方法的优点就是简单，当时缺点是由于忽略了标签之间的关系，因此效果次之。</p>
<p>2、二阶策略：二阶策略考虑了标签对内的关系，即某个标签和其他所有标签之间关联度的排序。因此，使用二阶策略具有良好的泛化性。但在实际应用中标签直接的关系往往超过两两之间的二阶关系。</p>
<p>3、高阶策略：高阶策略考虑了一个标签和所有标签之间的关系或者多个随机标签子集的关系。因此，使用高阶策略与采用一阶和二阶策略相比具有更好的泛化性，但计算要求更高，可扩展性更低。</p>
<h3 id="阈值标定">阈值标定</h3>
<p>在多标签学习中通常采用置信度函数<span class="math inline">\(f\left ( \cdot , \cdot \right )\)</span>的返回值作为学习的模型。因此为了获得实例<span class="math inline">\(x\)</span>的正确标签集，即<span class="math inline">\(h\left ( x \right )\)</span>，那么每个标签的置信度函数<span class="math inline">\(f\left ( x , y \right )\)</span>都需要通过阈值函数<span class="math inline">\(t\left ( x \right )\)</span>进行校准。通常阈值标定有两种方法，分别是使用固定的阈值函数<span class="math inline">\(t\left ( \cdot \right )\)</span>和通过训练集中获取动态阈值函数<span class="math inline">\(t\left ( \cdot \right )\)</span>。</p>
<p>1、固定阈值函数：采用这种方法最直接的就是使用0作为标定常量。另一个做法是使用0.5作为标定常量。此外，对于在测试集中未知的实例<span class="math inline">\(x\)</span>，设置标定常量可以最小化训练集和测试集在某个多标签指标上的差异，特别是标签基数。</p>
<p>2、动态阈值函数：采用这种方法通常使用stacking处理（即将训练集拆分为N个部分，当某个模型随机训练拆分后的某个数据集，并把训练后的结果给到下一个新的训练集，以此类推）来获取动态阈值函数。一个常用的获取动态阈值函数<span class="math inline">\(t\left ( \cdot \right )\)</span>的方法是使用线性模型，即当<span class="math inline">\(f^{*}\left ( x \right ) = \left ( f\left ( x, y_{1} \right ), \cdots , f\left ( x, y_{q} \right ) \right )^{T} \in \mathbb{R}^{q}\)</span>时（保存了实例<span class="math inline">\(x\)</span>对于<span class="math inline">\(q\)</span>个标签的置信度），阈值函数模型为<span class="math inline">\(t\left ( x \right ) = \left \langle \omega ^{*}, f^{*}\left ( x \right ) \right \rangle + b^{*}\)</span>，就是说只要能计算出<span class="math inline">\(q\)</span>维向量的权重<span class="math inline">\(w^{*}\)</span>和偏差<span class="math inline">\(b^{*}\)</span>，就可以解决基于训练集<span class="math inline">\(\mathcal{D}\)</span>的线性最小二乘问题<span class="math inline">\(\underset{\left \{ \omega^{*}, b^{*} \right \}}{min}\sum_{i=1}^{m} \left ( \left \langle w^{*}, f^{*}\left ( x_{i} \right ) \right \rangle + b^{*} - s\left ( x_{i} \right ) \right )^{2}\)</span>。其中，<span class="math inline">\(s\left ( x_{i} \right ) = argmin_{a \in \mathbb{R}}\left ( \left | \left \{ y_{j} \mid y_{j} \in Y_{i}, f\left ( x_{i}, y_{i} \right ) \leq a \right \} \right | + \left | \left \{ y_{k} \mid y_{k} \in \bar{Y_{i}}, f\left ( x_{i}, y_{k} \right ) \geq a \right \} \right | \right )\)</span>表示stacking模型处理输出，该模型把每个训练集中可能与<span class="math inline">\(x_{i}\)</span>相关的标签集<span class="math inline">\(\mathcal{Y}\)</span>分为相关和不相关两个部分，从而达到最小的错误分类。 \end{enumerate}</p>
<blockquote>
<p>最小二乘问题：简单理解就是让总误差的平方最小<span class="math inline">\(\varepsilon\)</span>的<span class="math inline">\(y\)</span>就是真值。通过对下式求导就可以得到最小二乘的一个特例算术平均数，即算术平均数可以让误差最小。其中的“二乘”就是指平方。</p>
</blockquote>

<blockquote>
<p>把最小二乘进行推广，就可以到如线性方程<span class="math inline">\(f\left ( x \right ) = ax + b\)</span>，又可以得到总误差<span class="math inline">\(\varepsilon = \sum \left ( f\left ( x_{i} \right ) - y_{i} \right )^{2} = \sum \left ( ax_{i} + b - y_{i} \right )^{2}\)</span>。</p>
</blockquote>
<img src="/review-multilabel-alg/least-squares.png" class title="最小二乘">
<img src="/review-multilabel-alg/linear-least-squares.png" class title="线性最小二乘">
<h2 id="评价指标">评价指标</h2>
<h3 id="评价分类">评价分类</h3>
<p>传统监督学习的泛化性评价指标通常采用精度、F值（F-Measure）、ROC曲线的下面积（AUC）等来表示。由于多标签学习中一个实例会和多个标签相关联，而不是像单标签学习中一个实例和一个标签相关联。因此与传统的但标签学习相比，多标签学习的评价指标要复杂的多，大致可以分为两大类，分别是基于实例的评价和基于标签的评价。</p>
<img src="/review-multilabel-alg/roc.png" class title="ROC和AUC">
<blockquote>
<ul>
<li>1、准确率（Accuracy）：预测准确率的高低，即<span class="math inline">\(\frac{TP+TN}{TP+TN+FP+FN}\)</span>。</li>
<li>2、查准率（Precision，P指标，【宁愿漏掉，不可错杀】）：预测为正例中实际为正例的占比，即<span class="math inline">\(\frac{TP}{TP+FP}\)</span>。</li>
<li>3、查全率（Recall，R指标，【宁愿错杀，不可漏掉】）：，正例预测正确占所有正例预测的比例，即<span class="math inline">\(\frac{TP}{TP+FN}\)</span>。</li>
<li>4、F值：查准率和查全率在非负权重<span class="math inline">\(\beta\)</span>下的加权调和平均值，即<span class="math inline">\(\frac{\left ( 1 + \beta^{2} \right ) \times P \times R}{\beta ^{2}\left ( P+R \right )}\)</span>，<span class="math inline">\(\beta\)</span>一般取0.3。当<span class="math inline">\(\beta\)</span>取1时，那么F值就为P指标和R指标的调和平均，即F1值。</li>
<li>5、ROC曲线：该曲线是由X轴的假正率（FPR，<span class="math inline">\(\frac{FP}{FP+TN}\)</span>）和Y轴的真正率（TPR，<span class="math inline">\(\frac{TP}{TP+FN}\)</span>）组成一个<span class="math inline">\(1 \times 1\)</span>正方形，如图1所示。</li>
<li>6、AUC面积：即ROC曲线的下半部分面积，当<span class="math inline">\(AUC &lt; 0.5\)</span>表示预测没有意义，当<span class="math inline">\(0.5 &lt; AUC &lt; 0.7\)</span>表示预测价值较低，当<span class="math inline">\(0.5 &lt; AUC &lt; 0.9\)</span>表示具有一定的预测价值，当<span class="math inline">\(AUC &gt; 0.9\)</span>表示预测准确度较高。</li>
</ul>
</blockquote>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">项目</th>
<th style="text-align: center;">预测正类（Positive）</th>
<th style="text-align: center;">预测负类（Negative）</th>
<th style="text-align: center;">实际总计</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">实际正类（True）</td>
<td style="text-align: center;">将正类预测为正类（TP）</td>
<td style="text-align: center;">将正类预测为负类（FN）</td>
<td style="text-align: center;">实际为正类（<span class="math inline">\(TP + FN = P\)</span>）</td>
</tr>
<tr class="even">
<td style="text-align: center;">实际负类（False）</td>
<td style="text-align: center;">将负类预测为正类（FP）</td>
<td style="text-align: center;">将负类预测为负类（TN）</td>
<td style="text-align: center;">实际为负类（<span class="math inline">\(FP + TN = N\)</span>）</td>
</tr>
<tr class="odd">
<td style="text-align: center;">预测总计</td>
<td style="text-align: center;">预测为正类（<span class="math inline">\(TP + FP = P^{&#39;}\)</span>）</td>
<td style="text-align: center;">预测为负类（<span class="math inline">\(TN + FN = N^{&#39;}\)</span>）</td>
<td style="text-align: center;"><span class="math inline">\(P+N\)</span></td>
</tr>
</tbody>
</table>
<p>1、基于实例的评价：通过获取每个实例的性能评价，然后取平均。</p>
<p>2、基于标签的评价：通过对每个标签的性能进行评价从而获取整个学习系统的性能，并且返回所有类别标签的宏平均或微平均。</p>
<blockquote>
<ul>
<li>1、宏平均：即在多标签（多类）分类中把每一类指标做算术平均，然后在根据平均后的结果计算F值。</li>
<li>2、微平均：即在多标签（多类）分类中的查准率<span class="math inline">\(\frac{\sum TP}{\sum TP + \sum FP}\)</span>。</li>
</ul>
</blockquote>
<img src="/review-multilabel-alg/evaluation-metrics.png" class title="主要的多标签评价指标">
<p>需要注意的是，除了从分类角度考虑多标签分类系统<span class="math inline">\(h\left ( \cdot \right )\)</span>的泛化性外，也可以通过置信度<span class="math inline">\(f\left ( \cdot , \cdot \right )\)</span>的排序来考虑多标签分类系统的泛化性</p>
<h3 id="基于实例的评价指标">基于实例的评价指标</h3>
<p>1、子集精度（Subset Accuracy）：即分类正确的比例，如预测标签集<span class="math inline">\(h\left ( x_{i} \right )\)</span>与基准标签集<span class="math inline">\(Y_{i}\)</span>相同时，则<span class="math inline">\([\![h\left ( x_{i} \right )=Y_{i}]\!]\)</span>返回1，否则返回0，其中<span class="math inline">\(p\)</span>为实例的个数。但当标签集空间<span class="math inline">\(q\)</span>很大时，由于判定的条件是全等，因此过于严格，造成评价解决并不合理。</p>
<p><span class="math display">\[\begin{align}
    subsetacc(h)=\frac{1}{p}\sum_{i=1}^{p}[\![h\left ( x_{i} \right )=Y_{i}]\!]
\end{align}\]</span></p>
<p>2、汉明损失（Hamming Loss）：即计算每个实例预测得到的标签集与基准标签集的差值的数量，包含没有被预测到的或者被误分类的标签数量，其中<span class="math inline">\(h\left ( x_{i} \right )\)</span>表示实际预测得到的标签集，<span class="math inline">\(\Delta Y_{i}\)</span>表示预测标签集和标准标签集的差值。当属于测试集<span class="math inline">\(\mathcal{S}\)</span>的实例只与一个标签相关，则<span class="math inline">\(hloss\left ( h \right )\)</span>是传统分类误差率的<span class="math inline">\(2 / q\)</span>倍。</p>
<p><span class="math display">\[\begin{align}
    hloss\left ( h \right )=\frac{1}{p}\sum_{i=1}^{p}\left | h\left ( x_{i} \right )\Delta Y_{i} \right |
\end{align}\]</span></p>
<p>3、准确率（Accuracy）、查准率（Precision，P指标）、查全率（Recall，R指标）、F值。</p>
<p><span class="math display">\[\begin{align}
    \begin{matrix}
        Accuracy_{exam}\left ( h \right )=\frac{1}{p}\sum_{i=1}^{p}\frac{\left | Y_{i}\bigcap h\left ( x_{i} \right ) \right |}{\left | Y_{i}\bigcup h\left ( x_{i} \right ) \right |} &amp; Precision_{exam}\left ( h \right )=\frac{1}{p}\sum_{i=1}^{p}\frac{\left | Y_{i}\bigcap h\left ( x_{i} \right ) \right |}{\left | h\left ( x_{i} \right ) \right |} \\ 
        Recall_{exam}\left ( h \right )=\frac{1}{p}\sum_{i=1}^{p}\frac{\left | Y_{i}\bigcap h\left ( x_{i} \right ) \right |}{\left | Y_{i} \right |} &amp; F_{\beta }^{exam}\left ( h \right )=\frac{\left ( 1+\beta ^{2} \right )\cdot Precision_{exam}\left ( h \right )\cdot Recall_{exam}\left ( h \right )}{\beta ^{2}\cdot Precision_{exam}\left ( h \right )+Recall_{exam}\left ( h \right )}
    \end{matrix}
\end{align}\]</span></p>
<p>通过置信度函数<span class="math inline">\(f\left ( \cdot ,\cdot \right )\)</span>可以有四个基于实例的排序指标对多标签实例进行度量。</p>
<p>1、1-错误率（One-error）：即输出结果中排序第一的标签一定不属于当前实例。</p>
<p><span class="math display">\[\begin{align}
    one-error\left ( f \right ) = \frac{1}{p}\sum_{i=1}^{p} [\![\left [ argmax_{y\in \mathcal{Y}}f\left ( x_{i}, y \right ) \right ] \notin Y_{i}]\!]
\end{align}\]</span></p>
<p>2、覆盖率（Coverage）：对获取的每个与实例<span class="math inline">\(x_{i}\)</span>相关标签<span class="math inline">\(y\)</span>的置信度按照降序进行排名后，获得最后一个被正确预测的标签排名与第一个正确预测的标签排名的差值（或者说是距离），即距离越小越好，说明正确预测的标签即排名越高。</p>
<p><span class="math display">\[\begin{align}
    coverage\left ( f \right )=\frac{1}{p}\sum_{i=1}^{p}max_{y\in Y_{i}}rank_{f}\left ( x_{i},y \right )-1
\end{align}\]</span></p>
<img src="/review-multilabel-alg/coverage.png" class title="覆盖率的例子">
<p>3、损失排序（Ranking Loss）：把与<span class="math inline">\(x_{i}\)</span>相关的标签集<span class="math inline">\({y}&#39;\)</span>的置信度与不相关的标签集<span class="math inline">\({y}&#39;&#39;\)</span>的置信度进行两两排序比较，获得不相关标签集出现在相关标签集中的概率，该损失值越小，预测结果越好。</p>
<p><span class="math display">\[\begin{align}
    rloss\left ( f \right )=\frac{1}{p}\sum_{i=1}^{p}\left | \left \{ \left ( {y}&#39;,{y}&#39;&#39; \right ) \mid f\left ( x_{i},{y}&#39; \right ) \leq f\left ( x_{i},{y}&#39;&#39; \right ),\left ( {y}&#39;,{y}&#39;&#39; \right ) \in Y_{i}\times \bar{Y_{i}} \right \} \right |
\end{align}\]</span></p>
<p>4、平均精度（Average Precision）：分母是标签<span class="math inline">\(x_{i}\)</span>的相关标签<span class="math inline">\(y\)</span>的排名，分子表示属于<span class="math inline">\(x_{i}\)</span>相关标签集<span class="math inline">\(Y_{i}\)</span>且排名小于等于相关标签<span class="math inline">\(y\)</span>的个数。该值越大，预测效果越好。</p>
<p><span class="math display">\[\begin{align}
    avgprec\left ( f \right )=\frac{1}{p}\sum_{i=1}^{p}\frac{1}{\left | Y_{i} \right |}\underset{y \in Y_{i}}{\sum} \frac{\left | \left \{ {y}&#39;\mid rank_{f}\left ( x,{y}&#39; \right ) \leq rank_{f}\left ( x_{i},y \right ),{y}&#39;\in Y_{i} \right \} \right |}{rank_{f}\left ( x_{i},y \right )}
\end{align}\]</span></p>
<p>对于1-错误率（One-error）、覆盖率（Coverage）、损失排序（Ranking Loss）而言都是值越小说明学习系统的性能越好，当1-错误率（One-error）和损失排序（Ranking Loss）为0时，覆盖率（Coverage）为<span class="math inline">\(\frac{1}{p}\sum_{i=1}^{p}\left | Y_{i} \right |-1\)</span>时，系统性能最好。而对于其他评价系统来说，值越大说明系统性能越好，1为最佳性能。</p>
<h3 id="基于标签的评价指标">基于标签的评价指标</h3>
<p>对于基于多标签分类器<span class="math inline">\(h\left ( \cdot \right )\)</span>的第<span class="math inline">\(j\)</span>类标签<span class="math inline">\(y_{j}\)</span>有四个基本特征量，这四个基本特征量都是在二元分类上某个标签基本特征量，分别是</p>
<p><span class="math display">\[\begin{align}
    \begin{matrix}
        TP_{j}=\left | \left \{ x_{i} \mid y_{i} \in Y_{i} \wedge y_{j} \in h\left ( x_{i} \right ), 1 \leq i \leq p \right \} \right | &amp; FP_{j}=\left | \left \{ x_{i} \mid y_{i} \notin Y_{i} \wedge y_{j} \in h\left ( x_{i} \right ), 1 \leq i \leq p \right \} \right | \\ 
        TN_{j}=\left | \left \{ x_{i} \mid y_{i} \notin Y_{i} \wedge y_{j} \notin h\left ( x_{i} \right ), 1 \leq i \leq p \right \} \right | &amp; FN_{j}=\left | \left \{ x_{i} \mid y_{i} \in Y_{i} \wedge y_{j} \notin h\left ( x_{i} \right ), 1 \leq i \leq p \right \} \right |
    \end{matrix}
\end{align}\]</span></p>
<p>其中<span class="math inline">\(TP_{j}\)</span>表示正类预测为正类，<span class="math inline">\(FP_{j}\)</span>表示负类预测为正类，<span class="math inline">\(TN_{j}\)</span>表示正类预测为负类，<span class="math inline">\(FN_{j}\)</span>表示负类预测为负类。并且基于上面四个基本特征量可以得到大部分的二元分类指标。假设<span class="math inline">\(B \left(TP_{j}, FP_{j}, TN_{j}, FN_{j}\right)\)</span>表示一个二元分类的评价指标（<span class="math inline">\(B \in \left [ 准确率、查准率、查全率、F^{\beta} \right ]^{4}\)</span>），那么基于标签的分类指标就可以从宏平均或微平均得到。</p>
<p>1、宏平均（Macro-averaging）</p>
<p><span class="math display">\[\begin{align}
    B_{macro}\left ( h \right )=\frac{1}{q}\sum_{j=1}^{q}B\left ( TP_{j},FP_{j},TN_{j},FN_{j} \right )
\end{align}\]</span></p>
<p>2、微平均（Micro-averaging）</p>
<p><span class="math display">\[\begin{align}
    B_{micro}\left ( h \right )=B\left ( \sum_{j=1}^{q}TP_{j},\sum_{j=1}^{q}FP_{j},\sum_{j=1}^{q}TN_{j},\sum_{j=1}^{q}FN_{j} \right )
\end{align}\]</span></p>
<p>同时，还可以得到<span class="math inline">\(Accuracy_{macro}\left ( h \right )=Accuracy_{micro}\left ( h \right )\)</span>，以及<span class="math inline">\(Accuracy_{micro}\left ( h \right )+hloss\left ( h \right )=1\)</span>。</p>
<p>当置信度函数<span class="math inline">\(f\left(\cdot , \cdot \right)\)</span>可用时，可以获得基于标签的指标排序，如宏平均AUC，并且下式也遵守AUC和Wilcoxon-Mann-Whitney统计量之间的关系。</p>
<p><span class="math display">\[\begin{align}
    AUC_{macro}=\frac{1}{q}\sum_{j=1}^{q}AUC_{j}=\frac{1}{q}\sum_{j=1}^{q}\frac{\left | \left \{ \left ( {x}&#39;,{x}&#39;&#39; \right ) \mid f\left ( {x}&#39;,y_{j} \geq f\left ( {x}&#39;&#39;,y_{j} \right ),\left ( {x}&#39;,{x}&#39;&#39; \right ) \in \mathcal{Z}_{j} \times \bar{\mathcal{Z}_{j}} \right ) \right \} \right |}{\left | \mathcal{Z}_{j} \right |\left | \bar{\mathcal{Z}_{j}} \right |}
\end{align}\]</span></p>
<p>其中，<span class="math inline">\(\mathcal{Z}_{j} = \left \{ x_{i} \mid y_{j} \in Y_{i}, 1 \leq i \leq p \right \}\left ( \bar{\mathcal{Z}_{j}} = \left \{ x_{i} \mid y_{j} \notin Y_{i}, 1 \leq i \leq p \right \} \right )\)</span></p>
<p>此外，还可以获得微平均AUC。</p>
<p><span class="math display">\[\begin{align}
    AUC_{micro}=\frac{\left | \left \{ \left ( {x}&#39;,{x}&#39;&#39;,{y}&#39;,{y}&#39;&#39; \right ) \mid f\left ( {x}&#39;,{y}&#39; \right ) \geq f\left ( {x}&#39;&#39;,{y}&#39;&#39; \right ),\left ( {x}&#39;,{y}&#39; \right )\in \mathcal{S}^{+},\left ( {x}&#39;&#39;,{y}&#39;&#39; \right ) \in \mathcal{S}^{-} \right \} \right |}{\left | \mathcal{S}^{+} \right |\left | \mathcal{S}^{-} \right |}
\end{align}\]</span></p>
<p>不论是微平均AUC还是宏平均AUC，都是值越大，系统的性能越好，最优值为1。</p>
<h3 id="理论结果">理论结果</h3>
<p>目前的多标签学习算法都在一个方面进行了优化，并且有研究表明如果只是单纯的提高子集的精度（Subset Accuracy），并且使用汉明损失（Hamming Loss）的话，系统的性能反而更差，因此需要多标签学习算法的性能应该在更为广泛的度量范围内进行测试，而不是仅在优化的算法上进行测试。</p>
<p>由于多标签指标通常是非凸的和不连续的，因此在实践中，大多数学习算法都会使用一些替代方法来优化或者替代多标签指标。最近，研究了多标签学习的一致性，即一个分类器的损失函数是否是否会随着训练集大小的增加而收敛到贝叶斯损失，并且还提出了一种在<span class="math inline">\(\mathcal{X} \times 2^{\mathcal{Y}}\)</span>固定分布上，基于代理损失函数的多标签学习一致性的充分必要条件，即具有最优代理损失函数的分类器集必须落入能够产生最优原始多标签损失函数的分类器集中。</p>
<blockquote>
<p>代理损失函数：即当目标函数非凸、不连续时，数学性质不好，优化起来比较复杂，这时候需要使用其他的性能较好的函数进行替换</p>
</blockquote>
<blockquote>
<p>贝叶斯损失（Bayes loss）：</p>
</blockquote>
<p>通过关注排序损失（ranking loss）可以发现标签对所定义的非双向凸代替损失与排序损失（ranking loss）一致，并且一些近期的多标签学习方法与传统确定的多标签学习方法得到的结果不一致。对于这个负面结果，在最小化排序损失（ranking loss）中得到了与多标签学习具有一致性的补充结果。即通过简化双向排序问题，使得在单个标签上定义具有简单变量的凸代理函数，从而使排序损失（ranking loss）的遗憾界和收敛速度具有一致性。</p>
<blockquote>
<p>遗憾界（regret bounds）：</p>
</blockquote>
<h1 id="学习算法">学习算法</h1>
<h2 id="算法分类">算法分类</h2>
<p>本论文选择八个主要的算法，这八个算法首先具有普适性，即每个算法都有其独特的特点；其次具有原始的影响，是大多数后续算法的基础；最后是具有较高的影响力，这些算法都在多标签学习领域中被大量引用。多标签学习领域中可以把算法大致分为两大类，分别是：</p>
<p>1、问题转换的方法：即核心是把数据拟合到某一个算法上。这类方法是把多标签学习的问题转化为已有的学习方案上。其中代表算法首先是一阶的二元关联方法，其次是把多标签学习转化为标签排序的二阶标签排序校准方法，最后是把多标签学习转化为二元分类问题的高阶分类链方法和把多标签学习转化为多类分类任务的高阶随机k-labelsets方法。</p>
<p>2、算法适配的方法：即核心是把现有的某一个算法拟合到数据上。该类方法是使用目前流行的学习技术来适配多标签学习的问题，从而处理多标签数据。其中代表算法首先是通过适配惰性学习技术的一阶ML-KNN方法和适配了决策树的一阶ML-DT方法，其次是适配了核技术的二阶Rank-SVM方法和适配了信息论技术的二阶CML方法。</p>
<img src="/review-multilabel-alg/multi-label-learning-algorithms.png" class title="多标签学习算法">
<h2 id="问题转换的方法">问题转换的方法</h2>
<h3 id="二元关联">二元关联</h3>
<p>该算法的基本思想是把多标签学习问题转化为<span class="math inline">\(q\)</span>个不相关的二元分类问题，即每个标签的二元分类问题都可能是实例<span class="math inline">\(x_{i}\)</span>上的标签。例如对于第<span class="math inline">\(j\)</span>个标签<span class="math inline">\(y_{j}\)</span>，那么本算法首先考虑标签<span class="math inline">\(y_{j}\)</span>是否属于实例<span class="math inline">\(x_{i}\)</span>的相关标签集<span class="math inline">\(Y_{i}\)</span>来构建相应的二元训练集<span class="math inline">\(\mathcal{D}_{j}\)</span>。</p>
<p><span class="math display">\[\begin{align}
    \mathcal{D}_{j}=\left \{ \left ( x_{i},\phi \left ( Y_{i}, y_{j} \right ) \right ) \mid 1 \leq i \leq m \right \} \nonumber \\
    where \ \phi \left ( Y_{i}, y_{i} \right )=
    \left\{\begin{matrix}
    +1, &amp; y_{j} \in Y_{i} \\ 
    -1, &amp; otherwise
    \end{matrix}\right.
\end{align}\]</span></p>
<p>然后通过二元学习算法<span class="math inline">\(\mathcal{B}\)</span>构建二元分类器<span class="math inline">\(g_{j} : \mathcal{X} \rightarrow \mathbb{R}\)</span>，即<span class="math inline">\(g_{j} \leftarrow \mathcal{B}\left ( \mathcal{D}_{j} \right )\)</span>。因此，对于任意的多标签训练实例<span class="math inline">\(\left( x_{i}, Y_{i} \right)\)</span>，实例<span class="math inline">\(x_{i}\)</span>都将参与<span class="math inline">\(q\)</span>个二元分类器的学习过程。对于相关标签<span class="math inline">\(y_{j} \in Y_{i}\)</span>来说，<span class="math inline">\(x_{i}\)</span>是作为分类器<span class="math inline">\(g_{j}\left ( \cdot \right )\)</span>的一个正例；此外，对于不相关标签<span class="math inline">\(y_{k} \in \bar{Y_{i}}\)</span>来说，<span class="math inline">\(x_{i}\)</span>是作为分类器<span class="math inline">\(g_{j}\left ( \cdot \right )\)</span>的一个负例。这种训练策略被称为交叉训练。</p>
<p>对于未知实例<span class="math inline">\(x\)</span>来说，二元关联预测是通过查询每个独立二元分类器，并结合其他相关标签来构建实例<span class="math inline">\(x\)</span>的相关标签集<span class="math inline">\(Y\)</span>。</p>
<p><span class="math display">\[\begin{align}
    Y = \left \{ y_{j} \mid g_{j}\left ( x \right ) &gt; 0, 1 \leq j \leq q \right \}
\end{align}\]</span></p>
<p>注意，当所有的二元分类器都输出负例时，那么预测得到的标签集<span class="math inline">\(Y\)</span>将为空。因此，为了避免产生预测标签集<span class="math inline">\(Y\)</span>为空，就需要引入T-Criterion规则。该准则通过包含具有最大输出的标签实例或者具有最少负例的实例来对上式进行补充。</p>
<p><span class="math display">\[\begin{align}
    Y = \left \{ y_{j} \mid g_{j}\left ( x \right ) &gt; 0, 1 \leq j \leq q \right \} \bigcup \left \{ y_{j^{*}} = j^{*} = argmax_{1 \leq j \leq q} g_{j}\left ( x \right ) \right \}
\end{align}\]</span></p>
<p>评论：二元关联是许多最先进的多标签学习技术的基础模块。此外，由于二元关联忽略了标签之间的潜在管理，并且当标签数量<span class="math inline">\(q\)</span>很大但标签密度（<span class="math inline">\(LDen\left ( \mathcal{D} \right )\)</span>）较低时，二元分类器就会在每个标签上出现类失衡的问题。</p>
<img src="/review-multilabel-alg/binary-relevance.png" class title="二元关联算法">
<h3 id="分类器链">分类器链</h3>
<p>该算法的基本思想是把多标签学习问题转化为一个二元分类问题链，即在分类器链中的子二元分类器建立在上一个子二元分类器预测的基础上。</p>
<p>假如有<span class="math inline">\(q\)</span>个可能的标签类<span class="math inline">\(\left\{ y_{1}, y_{2}, y_{3}, \cdots , y_{q}\right\}\)</span>，并且有一个全排列函数<span class="math inline">\(\tau : \left ( 1, \cdots ,q \right ) \rightarrow \left ( 1, \cdots ,q \right )\)</span>可以对所有标签进行排序，即<span class="math inline">\(y_{\tau \left ( 1 \right )} \succ y_{\tau \left ( 2 \right )} \succ \cdots \succ y_{\tau \left ( q \right )}\)</span>，其中<span class="math inline">\(\tau \left ( j \right )\left(1 \leq j \leq q\right)\)</span>返回排列好的标签号。对于在一个已排列列表的第<span class="math inline">\(j\)</span>个标签<span class="math inline">\(y_{\tau \left ( j \right )} \left(1 \leq j \leq q\right)\)</span>，通过添加与<span class="math inline">\(y_{\tau \left ( j \right )}\)</span>之前所有标签相关的实例来构建一个二元训练集。</p>
<p><span class="math display">\[\begin{align}
    \mathcal{D}_{\tau \left ( j \right )} = \left \{ \left ( \left [ x_{i}, pre_{\tau \left ( j \right )}^{i} \right ], \phi \left ( Y_{i}, y_{\tau \left ( j \right )} \right ) \right ) \mid 1 \leq i \leq m \right \} \nonumber \\
    where \ pre_{\tau \left ( j \right )}^{i} = \left ( \phi \left ( Y_{i}, y_{\tau \left ( 1 \right )} \right ), \cdots , \phi \left ( Y_{i}, y_{\tau \left ( j-1 \right )} \right ) \right )^{T}
\end{align}\]</span></p>
<p>其中，<span class="math inline">\(\left [ x_{i}, pre_{\tau \left ( j \right )}^{i} \right ]\)</span>由<span class="math inline">\(x_{i}\)</span>和<span class="math inline">\(pre_{\tau \left ( j \right )}^{i}\)</span>两个向量组成。其中<span class="math inline">\(pre_{\tau \left ( j \right )}^{i}\)</span>表示已排序标签<span class="math inline">\(y_{\tau \left ( j \right )}\)</span>之前的所有标签与实例<span class="math inline">\(x_{i}\)</span>的二元相关性赋值（<span class="math inline">\(pre_{\tau \left ( 1 \right )}^{i} = \varnothing\)</span>，即<span class="math inline">\(pre_{\tau \left ( 1 \right )}^{i}\)</span>为空集）。</p>
<p>然后，通过一些二元学习算法<span class="math inline">\(\mathcal{B}\)</span>构建标签<span class="math inline">\(y_{\tau \left ( j \right )}\)</span>的二元分类器<span class="math inline">\(g_{\tau \left ( j \right )} : \mathcal{X} \times \left \{ -1, +1 \right \}^{j-1} \rightarrow \mathbb{R}\)</span>，并且来决定<span class="math inline">\(y_{\tau \left ( j \right )}\)</span>是否是<span class="math inline">\(x_{i}\)</span>的相关标签，即<span class="math inline">\(g_{\tau \left ( j \right )} \leftarrow \mathcal{B}\left ( \mathcal{D}_{\tau \left ( j \right )} \right )\)</span>。</p>
<p>对于一个未知的实例<span class="math inline">\(x\)</span>，通过遍历整个分类器链来预测<span class="math inline">\(x\)</span>的相关标签集<span class="math inline">\(Y\)</span>。假设<span class="math inline">\(\lambda_{\tau \left ( j \right )}^{x} \in \left \{ -1, +1 \right \}\)</span>是标签<span class="math inline">\(y_{\tau \left ( j \right )}\)</span>在实例<span class="math inline">\(x_{i}\)</span>上的预测结果，那么器递归推导如下：</p>
<p><span class="math display">\[\begin{align}
    \begin{matrix}
        \lambda_{\tau \left ( 1 \right )}^{x} = sign\left [ g_{\tau \left ( 1 \right )}\left ( x \right ) \right ] \\ 
        \lambda_{\tau \left ( j \right )}^{x} = sign\left [ g_{\tau \left ( 1 \right )}\left ( \left [ x, \lambda_{\tau \left ( 1 \right )}^{x}, \cdots , \lambda_{\tau \left ( j-1 \right )}^{x} \right ] \right ) \right ]\left ( 2 \leq j \leq q \right )
    \end{matrix}
\end{align}\]</span></p>
<p>其中，<span class="math inline">\(sign\left [ \cdot \right ]\)</span>是一个带符号的函数。因此，实例<span class="math inline">\(x\)</span>相应的预测标签集为：</p>
<p><span class="math display">\[\begin{align}
    Y = \left \{ y_{\tau \left ( j \right )} \mid \lambda_{\tau \left ( j \right )}^{x} = +1, 1 \leq j \leq q \right \}
\end{align}\]</span></p>
<p>对于采用分类器链来解决多标签学习的问题来说，全排列函数<span class="math inline">\(\tau\)</span>的顺序对结果有很大影响。由于排序的影响，因此可以在标签集上通过<span class="math inline">\(n\)</span>个随机排列<span class="math inline">\(\tau\)</span>来构建一个分类器链的集合，即<span class="math inline">\(\tau ^{\left ( 1 \right )}, \tau ^{\left ( 2 \right )}, \cdots , \tau ^{\left ( n \right )}\)</span>。对于每一个排列<span class="math inline">\(\tau ^{\left ( r \right )}\left ( 1 \leq r \leq n \right )\)</span>来说，并不是直接通过原始的训练集<span class="math inline">\(\mathcal{D}\)</span>产生一个分类器链，而是通过对训练集<span class="math inline">\(\mathcal{D}\)</span>进行采样（<span class="math inline">\(\left | \mathcal{D}^{\left( r \right)} \right | = 0.67 \cdot \left | \mathcal{D} \right |\)</span>）或者替换（<span class="math inline">\(\left | \mathcal{D}^{\left( r \right)} \right | = \left | \mathcal{D} \right |\)</span>）的方式来得到一个修改后的训练集<span class="math inline">\(\mathcal{D}^{\left( r \right)}\)</span>。</p>
<p>评论：分类器链以随机方式考虑标签之间的相关性。与二进关联相比，分类器链具有利用标签之间相关性的优点，但是同时由于分类器链自身的特性，因此就无法实现并行处理。在训练阶段，分类器链从参考标记拓展了实例空间的特性，即<span class="math inline">\(pre_{\tau \left ( j \right )}^{i}\)</span>。另一种替代扩展特征值为二元值的可行方案是，当算法<span class="math inline">\(\mathcal{B}\)</span>（如朴素贝叶斯）产生的模型能够返回后验概率时，那么扩展特征值就会输出概率，而不是二元值。</p>
<img src="/review-multilabel-alg/classifier-chains.png" class title="分类器链">
<h3 id="校准标签排序">校准标签排序</h3>
<p>该算法的基本思想是把多标签学习问题转化为标签排序问题，即利用成熟的两两标签比较技术实现标签集中标签的排序。</p>
<p>假如有<span class="math inline">\(q\)</span>个可能的标签类<span class="math inline">\(\left\{ y_{1}, y_{2}, y_{3}, \cdots , y_{q}\right\}\)</span>，那么按照两两成对比较就会有总共<span class="math inline">\(\frac{q\left ( q-1 \right )}{2}\)</span>个二元分类器，其中每一对标签为<span class="math inline">\(\left ( y_{j}, y_{k} \right )\left ( 1\leq j\leq k\leq q \right )\)</span>。要对标签<span class="math inline">\(\left ( y_{j}, y_{k} \right )\)</span>进行两两比较，首先通过考虑每个训练实例<span class="math inline">\(x\)</span>与标签对<span class="math inline">\(y_{j}\)</span>和<span class="math inline">\(y_{k}\)</span>之间的相对相关性才能构建一个基于成对标签的二元训练集。</p>
<p><span class="math display">\[\begin{align}
    \mathcal{D}_{jk}=\left \{ \left ( x_{i},\varphi \left ( Y_{i},y_{j},y_{k} \right ) \right ) \mid \phi \left ( Y_{i},y_{j} \right ) \neq \phi \left ( Y_{i},y_{k} \right ),1\leq i\leq m \right \} \nonumber \\
    where \ \varphi \left ( Y_{i}, y_{j}, y_{k} \right ) = \left\{\begin{matrix}
    +1, &amp; if \ \phi \left ( Y_{i}, y_{j} \right ) = +1 \ and \ \phi \left ( Y_{i}, y_{k} \right ) = -1 \\ 
    -1, &amp; if \ \phi \left ( Y_{i}, y_{j} \right ) = -1 \ and \ \phi \left ( Y_{i}, y_{k} \right ) = +1
    \end{matrix}\right.
\end{align}\]</span></p>
<p>也就是说只有当实例<span class="math inline">\(x_{i}\)</span>与标签<span class="math inline">\(y_{j}\)</span>和标签<span class="math inline">\(y_{k}\)</span>具有上面的显著关系时才会被包含在训练集<span class="math inline">\(\mathcal{D}_{jk}\)</span>中。然后通过一些二元分类算法<span class="math inline">\(\mathcal{B}\)</span>构建一个标签<span class="math inline">\(\left ( y_{j}, y_{k} \right )\)</span>的二元分类器<span class="math inline">\(g_{jk} : \mathcal{X} \rightarrow \mathbb{R}\)</span>，即<span class="math inline">\(g_{jk} \leftarrow \mathcal{B}\left ( \mathcal{D}_{jk} \right )\)</span>。因此对于任意的多标签训练实例<span class="math inline">\(\left ( x_{i}, Y_{i} \right )\)</span>来说，实例<span class="math inline">\(x_{i}\)</span>都将参与到<span class="math inline">\(\left | Y_{i} \right |\left | \bar{Y_{i}} \right |\)</span>二元分类器的学习过程中。对于任意<span class="math inline">\(x \in \mathcal{X}\)</span>的实例，当<span class="math inline">\(g_{jk}\left(x\right) &gt; 0\)</span>时，那么学习系统会投票给<span class="math inline">\(y_{j}\)</span>，否则就投票给<span class="math inline">\(y_{k}\)</span>。</p>
<p>对于一个未知的实例<span class="math inline">\(x\)</span>，校准标签排序算法首先把实例<span class="math inline">\(x\)</span>反馈到<span class="math inline">\(\frac{q\left ( q-1 \right )}{2}\)</span>个已经训练好的二元分类器上，以此来获得所有可能标签的投票结果。</p>
<p><span class="math display">\[\begin{align}
    \zeta \left ( x,y_{j} \right )=\sum_{k=1}^{j-1}[\![ g_{kj}\left ( x \right ) \leq 0 ]\!] + \sum_{k=j+1}^{q}[\![ g_{kj}\left ( x \right ) &gt; 0 ]\!] \ \left ( 1 \leq j \leq q \right )
\end{align}\]</span></p>
<p>根据上面的定义，可以得到<span class="math inline">\(\sum_{j=1}^{q}\zeta \left ( x, y_{i} \right ) = \frac{q\left ( q-1 \right )}{2}\)</span>，并且<span class="math inline">\(\mathcal{Y}\)</span>中的所有标签都可以根据各自的投票进行重新排序（即关系被任意打破）。</p>
<p>因此，还需要使用阈值函数来将排好序的标签列表进行重新分类，即分为相关标签和不相关标签。而对于使用标签对方式的校准标签排序算法而言，需要在每个多标签训练实例<span class="math inline">\(\left ( x_{i},Y_{i} \right )\)</span>中引入了一个虚拟标签<span class="math inline">\(y_{V}\)</span>。因此虚拟标签<span class="math inline">\(y_{V}\)</span>就是区分<span class="math inline">\(x_{i}\)</span>的相关标签和不相关标签的一个分割点。换句话说，比<span class="math inline">\(y_{V}\)</span>排名高的就是相关标签<span class="math inline">\(y_{j} \in Y_{i}\)</span>，而比<span class="math inline">\(y_{V}\)</span>排名低的就是不相关标签<span class="math inline">\(y_{k} \in \bar{Y_{i}}\)</span>。</p>
<p>除了原始的<span class="math inline">\(\frac{q\left ( q-1 \right )}{2}\)</span>个二元分类器外，对于每个新标签对<span class="math inline">\(\left ( y_{j},y_{V} \right )\left ( 1 \leq j \leq q \right )\)</span>来言，又会有<span class="math inline">\(q\)</span>个辅助二元分类器。这个二元训练集将会以以下方式进行构建：</p>
<p><span class="math display">\[\begin{align}
    \mathcal{D}_{jV}=\left \{ \left ( x_{i}, \varphi \left ( Y_{i}, y_{j}, y_{V} \right ) \right ) \mid 1 \leq i \leq m\right \} \nonumber \\
    where \ \varphi \left ( Y_{i}, y_{j}, y_{V} \right ) = \left\{\begin{matrix}
        +1, &amp; if \ y_{j} \in Y_{i} \\ 
        -1, &amp; otherwise
        \end{matrix}\right.
\end{align}\]</span></p>
<p>基于新添加二元训练集，通过一个二元分类算法<span class="math inline">\(\mathcal{B}\)</span>构建了一个虚拟标签的二元分类器<span class="math inline">\(g_{jV} : \mathcal{X} \rightarrow \mathbb{R}\)</span>，即<span class="math inline">\(g_{jV} \leftarrow \mathcal{B}\left ( \mathcal{D}_{jV} \right )\)</span>。然后通过引入新的分类器来更新投票函数<span class="math inline">\(\zeta \left ( x,y_{j} \right )\)</span>，具体如下：</p>
<p><span class="math display">\[\begin{align}
    \zeta^{*} \left ( x,y_{j} \right ) = \zeta \left ( x,y_{j} \right ) + [\![ g_{jV}\left ( x \right ) &gt; 0 ]\!] \left ( 1 \leq j \leq q \right )
\end{align}\]</span></p>
<p>此外，虚拟标签的总投票数可以计算为：</p>
<p><span class="math display">\[\begin{align}
    \zeta^{*} \left ( x,y_{j} \right ) = \sum_{j=1}^{q}[\![ g_{jV}\left ( x \right ) \leq 0 ]\!]
\end{align}\]</span></p>
<p>因此，对于未知实例<span class="math inline">\(x\)</span>的预测标签集可以表示为：</p>
<p><span class="math display">\[\begin{align}
    Y = \left \{ y_{j} \mid \zeta^{*} \left ( x,y_{j} \right ) &gt; \zeta^{*} \left ( x,y_{V} \right ) , 1 \leq j \leq q \right \}
\end{align}\]</span></p>
<p>比较二元关联算法和校准标签排序算法中的训练集创建，可以看出两者的训练集产生相同。因此，校准标签排序算法可以被认为是标签之间两两比较算法的扩展版，即是为了便于学习而把<span class="math inline">\(q\)</span>个二元关联分类器扩展得到<span class="math inline">\(\frac{q\left ( q-1 \right )}{2}\)</span>个二元分类器。</p>
<p>评论：校准标签排序算法是通过两两标签构建分类器的二阶方法。相较于以往一对多算法构建的二元分类器相比，校准标签排序以一对一方式构建二元分类器（除了虚拟标签），具有减轻因标签类不平衡造成的问题的优点。此外，通过校准标签排序算法生成的分类器的数量从原先的随着标签<span class="math inline">\(q\)</span>数量成线性增长变为了随着标签<span class="math inline">\(q\)</span>数量成二次增长。因此，校准标签排序的优化和改进主要集中在通过精确裁剪或近似裁剪从而减少测试阶段的分类器数量，例如通过底层二元学习算法<span class="math inline">\(\mathcal{B}\)</span>的特性（感知机的对偶形式等）可以在训练阶段更为高效的获取二次数量的感知机。</p>
<blockquote>
<p>感知机的对偶形式：</p>
</blockquote>
<img src="/review-multilabel-alg/calibrated-label-ranking.png" class title="校准标签排序">
<h3 id="random-k-labelsets">Random k-Labelsets</h3>
<p>该算法的基本思想是把多标签学习问题转换为多类分类问题的集合，即集合中的每个学习器组件都会针对标签空间<span class="math inline">\(\mathcal{Y}\)</span>中的某个随机子集使用LP（Label Powerset）技术实现一个多类分类器。LP技术是把多标签学习问题转化为多类（单标签）分类问题最直接的方法。假设<span class="math inline">\(\sigma_{\mathcal{Y}}:2^{\mathcal{Y}} \rightarrow \mathbb{N}\)</span>是<span class="math inline">\(\mathcal{Y}\)</span>的幂集映射到自然数的映射函数，而<span class="math inline">\(\sigma_{\mathcal{Y}}^{-1}\)</span>是逆函数。在训练阶段，LP首先把原始的多标签训练集<span class="math inline">\(\mathcal{D}\)</span>中的每个不同标签作为一个新类，从而把在<span class="math inline">\(\mathcal{D}\)</span>转化为多类训练集。</p>
<p><span class="math display">\[\begin{align}
    \mathcal{D}_{\mathcal{Y}}^{\dagger}=\left \{ \left ( x_{i}, \sigma_{\mathcal{Y}}\left ( Y_{i} \right ) \right ) \mid 1\leq i \leq m \right \}
\end{align}\]</span></p>
<p>因此，<span class="math inline">\(\mathcal{D}_{\mathcal{Y}}^{\dagger}\)</span>的新的多类标签集合为：</p>
<p><span class="math display">\[\begin{align}
    \Gamma \left ( \mathcal{D}_{\mathcal{Y}}^{\dagger} \right )=\left \{ \sigma_{\mathcal{Y}} \left ( Y_{i} \right ) \mid 1 \leq i \leq m \right \}
\end{align}\]</span></p>
<p>显然，<span class="math inline">\(\left | \Gamma \left ( \mathcal{D}_{\mathcal{Y}}^{\dagger} \right ) \right | \leq min\left ( m, 2^{\left | \mathcal{Y} \right |} \right )\)</span>，即新的多类标签集一定小于等于训练集中实例的数量或者标签集全排列后的数量中最小的那个。然后，通过一些多类学习算法<span class="math inline">\(\mathcal{M}\)</span>构建一个多类分类器<span class="math inline">\(g_{\mathcal{Y}}^{\dagger }:\mathcal{X}\rightarrow \Gamma \left ( \mathcal{D}_{\dagger }^{\mathcal{Y}} \right )\)</span>，即<span class="math inline">\(g_{\mathcal{Y}}^{\dagger } \leftarrow \mathcal{M} \left ( \mathcal{D}_{\dagger }^{\mathcal{Y}} \right )\)</span>。因此，对于任意多标签训练实例<span class="math inline">\(\left ( x_{i}, Y_{i} \right )\)</span>，实例<span class="math inline">\(x_{i}\)</span>将会被重新分配一个已经映射好的新的单标签<span class="math inline">\(\sigma_{\mathcal{Y}}\left ( Y_{i} \right )\)</span>，并参与到后续的多类分类器中。</p>
<p>对于未知的实例<span class="math inline">\(x\)</span>，LP要预测其相关标签集<span class="math inline">\(Y\)</span>，那么首先就要查询多类分类器的预测，然后把结果通过逆函数得到相关标签集<span class="math inline">\(Y\)</span>的幂集。</p>
<p><span class="math display">\[\begin{align}
    Y=\sigma_{\mathcal{Y}}^{-1}\left ( g_{\mathcal{Y}}^{\dagger} \left ( x \right ) \right )
\end{align}\]</span></p>
<p>不幸的是，LP在可行性方面有两个主要的缺点，分别是：</p>
<p>1、不完备性：根据上式可以知道，LP预测的标签集只能是训练集中的标签集，即训练集外的标签集就不能进行泛化<span class="math inline">\(\left \{ Y_{i} \mid 1 \leq i \leq m\right \}\)</span>。</p>
<p>2、效率较低：当<span class="math inline">\(\mathcal{Y}\)</span>非常大时，就会在<span class="math inline">\(\Gamma \left ( \mathcal{D}_{\mathcal{Y}}^{\dagger} \right )\)</span>有非常多的新的标签类，导致在训练分类器<span class="math inline">\(g_{\mathcal{Y}}^{\dagger } \left(\cdot \right)\)</span>是非常的复杂，并且一些新产生的标签类其对应的实例有非常少。</p>
<p>因此，为了保持LP的简单性，以及克服LP的两个主要缺点，Random k-Labelsets选择在多标签数据的学习中把集成学习与LP相结合。这个方法的核心是仅在随机k-Labelsets（在<span class="math inline">\(\mathcal{Y}\)</span>标签集中大小为<span class="math inline">\(k\)</span>的子集）中使用LP，从而保证计算的效率，然后把多个LP分类器进行组合，从而保证了预测的完备性。</p>
<p>假设<span class="math inline">\(\mathcal{Y}^{k}\)</span>表示在标签集<span class="math inline">\(\mathcal{Y}\)</span>中所有可能的k-Labelsets，例如第<span class="math inline">\(l\)</span>个k-Labelsets表示为<span class="math inline">\(\mathcal{Y}^{k}\left(l\right)\)</span>，即<span class="math inline">\(\mathcal{Y}^{k}\left(l\right) \subseteq \mathcal{Y},\left | \mathcal{Y}^{k}\left(l\right) \right | = k, 1 \leq l \leq \binom{q}{k}\)</span>，类似于<span class="math inline">\(\mathcal{D}_{\mathcal{Y}}^{\dagger}\)</span>，一个多类训练集能够通过把原始标签空间<span class="math inline">\(\mathcal{Y}\)</span>缩小为<span class="math inline">\(\mathcal{Y}^{k}\left(l\right)\)</span>进行构建。</p>
<p><span class="math display">\[\begin{align}
    \mathcal{D}_{\mathcal{Y}^{k}\left ( l \right )}^{\dagger}=\left \{ \left ( x_{i}, \sigma_{\mathcal{Y}^{k}\left ( l \right )}\left ( Y_{i} \cap \mathcal{Y}^{k}\left ( l \right ) \right ) \right ) \mid 1 \leq i \leq m \right \}
\end{align}\]</span></p>
<p>因此，新的多类标签集可以通过<span class="math inline">\(\mathcal{D}_{\mathcal{Y}^{k}\left ( l \right )}^{\dagger}\)</span>表示为：</p>
<p><span class="math display">\[\begin{align}
    \Gamma \left ( \mathcal{D}_{\mathcal{Y}^{k}\left ( l \right )}^{\dagger} \right )=\left \{ \sigma_{\mathcal{Y}^{k}\left ( l \right )}\left ( Y_{i} \cap \mathcal{Y}^{k}\left ( l \right ) \right ) \mid 1 \leq i \leq m \right \}
\end{align}\]</span></p>
<p>然后，通过多类学习算法<span class="math inline">\(\mathcal{M}\)</span>构建一个多类分类器<span class="math inline">\(g_{\mathcal{Y}^{k}\left ( l \right )}^{\dagger}:\mathcal{X} \rightarrow \Gamma \left ( \mathcal{D}_{\mathcal{Y}^{k}\left ( l \right )}^{\dagger} \right )\)</span>，即<span class="math inline">\(g_{\mathcal{Y}^{k}\left ( l \right )}^{\dagger}: \mathcal{M} \left ( \mathcal{D}_{\mathcal{Y}^{k}\left ( l \right )}^{\dagger} \right )\)</span>。</p>
<p>为了创建了由<span class="math inline">\(n\)</span>个分类组件的集合，Random k-Labelsets会在<span class="math inline">\(n\)</span>个子标签集<span class="math inline">\(\mathcal{Y}^{k}\left ( l_{r} \right )\left ( 1 \leq r \leq n \right )\)</span>上使用LP，使得每个子标签集都会产生一个多类分类器<span class="math inline">\(g_{\mathcal{Y}^{k}\left ( l \right )}^{\dagger}\left ( \cdot \right )\)</span>。对于未知的实例<span class="math inline">\(x\)</span>，每类标签都需要计算以下两个数量。</p>
<p><span class="math display">\[\begin{align}
    \begin{matrix}
        \tau \left ( x, y_{j} \right ) = \sum_{r=1}^{n}[\![ y_{j} \in \mathcal{Y}^{k}\left ( l_{r} \right ) ]\!] \ \left ( 1 \leq j \leq q \right ) \\ 
        \mu \left ( x, y_{j} \right ) = \sum_{r=1}^{n} [\![ y_{j} \in \sigma_{\mathcal{Y}^{k}\left ( l_{r} \right )}^{-1} \left ( g_{\mathcal{Y}^{k}\left ( l_{r} \right )}^{\dagger } \left ( x \right )\right ) ) ]\!] \ \left ( 1 \leq j \leq q \right )
    \end{matrix}
\end{align}\]</span></p>
<p>其中，<span class="math inline">\(\tau \left ( x, y_{j} \right )\)</span>表示在集合中标签<span class="math inline">\(y_{j}\)</span>可以获取的最大预期投票数，而<span class="math inline">\(\mu \left ( x, y_{j} \right )\)</span>则表示在集合中<span class="math inline">\(y_{j}\)</span>获取的实际投票数。因此，对于标签集的预测就可以等价为：</p>
<p><span class="math display">\[\begin{align}
    Y=\left \{ y_{j} \mid \frac{\mu \left ( x, y_{j} \right )}{\tau \left ( x, y_{j} \right )} &gt; 0.5, 1 \leq j \leq q \right \}
\end{align}\]</span></p>
<p>换句话说，当实际投标数超过了最大投票数的一半时，那么就认为<span class="math inline">\(y_{j}\)</span>标签与实例<span class="math inline">\(x\)</span>相关。对于有<span class="math inline">\(n\)</span>个k-labelsets来说，其每个标签的最大平均投票数为<span class="math inline">\(\frac{nk}{q}\)</span>。在Random k-Labelsets算法的经验，一般<span class="math inline">\(k = 3，n = 2q\)</span>。</p>
<p>评论：Random k-Labelsets算法是一个高阶方法，即标签相关性的维度，由子标签集的大小决定。此外，在使用Random k-Labelsets算法时，可以通过预设数量阈值从而把在标签数量低于阈值的训练集<span class="math inline">\(\mathcal{D}\)</span>裁剪掉。虽然Random k-Labelsets算法通过将集成学习作为其固有的一部分来修正LP的缺点，但集成学习也可以作为一种元级策略，通过包含同质或异质的多标签学习器来促进多标签学习。</p>
<img src="/review-multilabel-alg/random-k-Labelsets.png" class title="Random k-Labelsets">
<h2 id="算法适配的方法">算法适配的方法</h2>
<h3 id="多标签的k最邻近分类算法ml-knn">多标签的K最邻近分类算法（ML-KNN）</h3>
<p>该算法的基本思想是通过适配KNN技术来处理多标签数据，即利用最大后验概率（MAP）准则对邻近标签内的信息进行推理，从而实现对多标签数据的预测。</p>
<p>对于未知实例<span class="math inline">\(x\)</span>，假设<span class="math inline">\(\mathcal{N}\left(x\right)\)</span>表示实例<span class="math inline">\(x\)</span>在训练集<span class="math inline">\(\mathcal{D}\)</span>中的最邻近数据集。通常，实例之间的相似性使用欧氏距离进行度量。对于第<span class="math inline">\(j\)</span>类标签，在ML-KNN选择计算下面的统计信息：</p>
<p><span class="math display">\[\begin{align}
    C_{j}=\sum_{\left ( x^{*},Y^{*} \right ) \in \mathcal{N}\left ( x \right )}[\![ y_{j} \in Y^{*} ]\!]
\end{align}\]</span></p>
<p>其中<span class="math inline">\(C_{j}\)</span>表示与标签<span class="math inline">\(y_{j}\)</span>相关的实例<span class="math inline">\(x\)</span>的领域数量。</p>
<p>假设<span class="math inline">\(H_{j}\)</span>是实例<span class="math inline">\(x\)</span>有标签<span class="math inline">\(y_{j}\)</span>的事件，并且<span class="math inline">\(\mathbb{P}\left ( H_{j} \mid C_{j} \right )\)</span>表示实例<span class="math inline">\(x\)</span>有标签<span class="math inline">\(y_{j}\)</span>，且正好有<span class="math inline">\(C_{j}\)</span>个领域的情况下的后验概率。而<span class="math inline">\(\mathbb{P}\left (\neg H_{j} \mid C_{j} \right )\)</span>则表示实例<span class="math inline">\(x\)</span>没有标签<span class="math inline">\(y_{j}\)</span>，且正好有<span class="math inline">\(C_{j}\)</span>个领域的情况下的后验概率。因此，按照MAP准则，标签集的预测通过判断<span class="math inline">\(\mathbb{P}\left ( H_{j} \mid C_{j} \right )\)</span>是否大于<span class="math inline">\(\mathbb{P}\left (\neg H_{j} \mid C_{j} \right )\)</span>或者不大于。</p>
<p><span class="math display">\[\begin{align}
    Y = \left \{ y_{j} \mid \frac{\mathbb{P}\left (H_{j} \mid C_{j} \right )}{\mathbb{P}\left (\neg H_{j} \mid C_{j} \right )} &gt; 1, 1 \leq j \leq q\right \}
\end{align}\]</span></p>
<blockquote>
<ul>
<li>1、KNN的说明：即计算实例<span class="math inline">\(x\)</span>和所有训练实例的距离，然后进行排序，取前k个实例，这k个实例的多数属于某个类，实例<span class="math inline">\(x\)</span>就归属于哪个类。虽然KNN简单，但由于要和所有的训练实例计算距离，因此当训练集特别大的时候，这种方式非常耗时，可以采用KD树（K-Dimensional Tree）的方式来减少输入实例和训练实例的计算次数从而优化性能。</li>
<li>2、KD树（K-Dimensional Tree）：以树型方式对<span class="math inline">\(N\)</span>维空间的实例进行存储，以便对其进行快速检索。KD树的创建相当于用不断垂直于坐标轴的超平面将<span class="math inline">\(N\)</span>维空间进行划分，构成一系列的<span class="math inline">\(N\)</span>维超矩阵区域。</li>
<li>3、KD树的生成：首先从<span class="math inline">\(m\)</span>个样本的<span class="math inline">\(N\)</span>维特征中分别计算<span class="math inline">\(N\)</span>个特征取值的方差，用方差最大的第<span class="math inline">\(k\)</span>维特征<span class="math inline">\(n_{k}\)</span>来作为根节点，然后选择特征<span class="math inline">\(n_{k}\)</span>的中间值<span class="math inline">\(n_{kv}\)</span>作为样本的划分点，对于所有第<span class="math inline">\(k\)</span>维特征的实例来说，其值小于<span class="math inline">\(n_{kv}\)</span>时，则该实例划入左分支，对取值大于等于<span class="math inline">\(n_{kv}\)</span>的样本，则划入右分支。然后再对左子树和右子树，采用相同的方法在分支中找到方差最大的特征作为当前分支的根节点，最后递归的生成KD树。</li>
<li>4、KD树的搜索：首先找到包含预测目标点的子节点，以该节点为中心，以目标前到该节点的距离为半径，得到一个超球体，从KNN的原理可以知道最近邻的点一定在该超球体内部。其次，返回该节点的父节点，检查另一个子节点所包含的超矩形是否与球体相交，如果有相交，那么就需要在该节点查询是否有更加近的近邻，如果有就更新最近邻和超球体，如果没有相交就继续返回父节点，在另一个子节点中查找最近邻。最后直到回到了根节点，算法结束，此时保存的最近领就是最终的最近邻。</li>
</ul>
</blockquote>
<img src="/review-multilabel-alg/kd-create.png" class title="KD创建">
<img src="/review-multilabel-alg/kd-structure.png" class title="Random KD结构">
<img src="/review-multilabel-alg/kd-search.png" class title="Random KD搜索">
<blockquote>
<p>欧式距离的说明：即常见的两点之间的距离，对于二维坐标系其距离为：<span class="math inline">\(E\left ( d \right ) = \sqrt{\left ( x_{1} - x_{2} \right )^{2} + \left ( y_{1} - y_{2} \right )^{2}}\)</span>。如果扩展到N维数据，那么两点之间的距离就是各维度数据分别求差后平方，然后再后求和，最后再开平方，即<span class="math inline">\(E\left ( d \right ) = \sqrt{\left ( x_{1} - x_{2} \right )^{2} + \left ( y_{1} - y_{2} \right )^{2} + \left ( z_{1} - z_{2} \right )^{2} + \cdots }\)</span>。</p>
</blockquote>
<p>根据贝叶斯定理，就可以得到：</p>
<p><span class="math display">\[\begin{align}
    \frac{\mathbb{P}\left (H_{j} \mid C_{j} \right )}{\mathbb{P}\left (\neg H_{j} \mid C_{j} \right )} = \frac{\mathbb{P}\left ( H_{j} \right ) \cdot \mathbb{P}\left ( C_{j} \mid H_{j} \right )}{\mathbb{P}\left ( \neg H_{j} \right ) \cdot \mathbb{P}\left ( C_{j} \mid \neg H_{j} \right )}
\end{align}\]</span></p>
<p>这里的<span class="math inline">\(\mathbb{P}\left ( H_{j} \right )\)</span>（<span class="math inline">\(\mathbb{P}\left ( \neg H_{j} \right )\)</span>）表示实例<span class="math inline">\(x\)</span>与标签<span class="math inline">\(y_{j}\)</span>具有相关性的事件<span class="math inline">\(H_{j}\)</span>（或<span class="math inline">\(x\)</span>与标签<span class="math inline">\(y_{j}\)</span>不相关）的先验概率。此外，<span class="math inline">\(\mathbb{P}\left (C_{j} \mid H_{j} \right )\)</span>（<span class="math inline">\(\mathbb{P}\left (C_{j} \mid \neg H_{j} \right )\)</span>）表示当事件<span class="math inline">\(H_{j}\)</span>成立（不成立）的情况下，实例<span class="math inline">\(x\)</span>在标签<span class="math inline">\(y_{j}\)</span>上有<span class="math inline">\(C_{j}\)</span>个邻近的概率。通过上式就可以实现先验概率的估计和执行预测。</p>
<p>为了完成上面的任务，ML-KNN采用频率计数的策略。首先，通过计算与每个标签关联的训练样本的数量来估计先验概率。</p>
<p><span class="math display">\[\begin{align}
    \mathbb{P}\left ( H_{j} \right )=\frac{s + \sum_{i=1}^{m}[\![ y_{j} \in Y_{i} ]\!]}{s \times 2 + m}; \mathbb{P}\left ( \neg H_{j} \right ) = 1 -  \mathbb{P}\left ( H_{j} \right )\ \left ( 1 \leq j \leq q \right )
\end{align}\]</span></p>
<p>其中，<span class="math inline">\(s\)</span>是用于控制平均先验概率对估计影响的平滑参数。在拉普拉斯平滑中，该值一般取值为1。</p>
<blockquote>
<p>拉普拉斯平滑的说明：通过增加平滑参数<span class="math inline">\(s\)</span>解决0概率问题。</p>
</blockquote>
<p>其次，就是对似然性的估计。对于第<span class="math inline">\(j\)</span>个标签<span class="math inline">\(y_{j}\)</span>，ML-KNN维护了两个频率数组<span class="math inline">\(\mathcal{K}_{j}\)</span>和<span class="math inline">\(\tilde{\mathcal{K}}_{j}\)</span>，每个数组都包含了<span class="math inline">\(k+1\)</span>个元素：</p>
<blockquote>
<p>最大似然的说明：似然和概率的区别在于，概率是知道事件发生的概率推算结果，而似然是根据结果推算事件发生的概率，即概率是参数推断结果的过程，而似然是结果推断参数分布的过程。延生到机器学习，就可以知道机器学习也是通过结果进行学习，然后求最大似然，最后得到未知变量的预测结果</p>
</blockquote>
<p><span class="math display">\[\begin{align}
    \begin{matrix}
        \mathcal{K}_{j}\left [ r \right ] = \sum_{i=1}^{m}[\![ y_{i} \in Y_{i} ]\!] \cdot [\![ \delta_{j}\left ( x_{i} \right ) = r ]\!] \ \left ( 0 \leq r \leq k \right ) \\ 
        \tilde{\mathcal{K}_{j}}\left [ r \right ] = \sum_{i=1}^{m}[\![ y_{i} \notin Y_{i} ]\!] \cdot [\![ \delta_{j}\left ( x_{i} \right ) = r ]\!] \ \left ( 0 \leq r \leq k \right ) \\ 
        where \ \delta_{j}\left ( x_{i} \right ) = \sum_{\left ( x^{*}, Y^{*} \right ) \in \mathcal{N}\left ( x_{i} \right )}[\![ y_{j} \in Y^{*} ]\!]
    \end{matrix}
\end{align}\]</span></p>
<p>其中，<span class="math inline">\(\delta_{j}\left ( x_{i} \right )\)</span>记录了与标签<span class="math inline">\(y_{j}\)</span>相关的实例<span class="math inline">\(x_{i}\)</span>邻域的数量。因此，<span class="math inline">\(\mathcal{K}_{j}\left [ r \right ]\)</span>保存了与标签<span class="math inline">\(y_{j}\)</span>相关，并且有<span class="math inline">\(r\)</span>个邻域的训练实例的数量。同样，<span class="math inline">\(\tilde{\mathcal{K}_{j}}\left [ r \right ]\)</span>保存了与标签<span class="math inline">\(y_{j}\)</span>不相关，但有<span class="math inline">\(r\)</span>个邻域的训练实例的数量。随后，似然性就可以通过<span class="math inline">\(\mathcal{K}_{j}\left [ r \right ]\)</span>和<span class="math inline">\(\tilde{\mathcal{K}_{j}}\left [ r \right ]\)</span>这两个数组进行估算。</p>
<p><span class="math display">\[\begin{align}
    \begin{matrix}
        \mathbb{P}\left ( C_{j} \mid H_{j} \right ) = \frac{s + \mathcal{K}_{j}\left [ C_{j} \right ]}{s \times \left ( k+1 \right ) + \sum_{r=0}^{k}\mathcal{K}_{j}\left [ r \right ] } \ \left ( 1 \leq j \leq q,0 \leq C_{j} \leq k \right )\\ 
        \mathbb{P}\left ( C_{j} \mid \neg H_{j} \right ) = \frac{s + \tilde{\mathcal{K}_{j}}\left [ C_{j} \right ]}{s \times \left ( k+1 \right ) + \sum_{r=0}^{k}\tilde{\mathcal{K}_{j}}\left [ r \right ] } \ \left ( 1 \leq j \leq q,0 \leq C_{j} \leq k \right )
    \end{matrix}
\end{align}\]</span></p>
<p>因此，有了先验概率和似然性就可以得到对标签集进行预测。</p>
<p>评价：ML-KNN算法是一个通过单独推理每个标签相关性的一阶算法。ML-KNN继承了惰性学习和贝叶斯推理的优点，具体如下：</p>
<p>1、决策边界可以自适应地调整，用于应对识别每个未知实例的不同领域。</p>
<p>2、由于估算了每类标签的先验概率，因此大大减轻了类别失衡问题。</p>
<p>其实还有其他使用惰性学习来处理多标签数据，如集合了KNN和聚合排名，通过识别一个特定标签来把KNN扩展到整个训练集。由于ML-KNN忽略了标签之间的相关性，因此已经提出了一些方法把标签之间的关系加入到ML-KNN中。</p>
<img src="/review-multilabel-alg/ml-knn.png" class title="ML-kNN">
<h3 id="多标签决策树ml-dt">多标签决策树（ML-DT）}</h3>
<p>该算法的基本思想是通过适配决策树技术来处理多标签数据，即利用多标签熵的信息增益准则来递归构建决策树。</p>
<blockquote>
<ul>
<li>1、决策树的说明：决策树是一种基本的分类与回归方法。决策树的产生通常分为两个部分，分别是构造和剪枝。</li>
<li>2、决策树的构造：即就是对更节点、分支和叶片的选择，并且节点和节点之间存在父子关系。其中要特别注意的是叶片是最底部的节点，是决策的结果。</li>
<li>3、决策树的剪枝：解决过拟合的问题。其中过拟合表示模型训练的太好，使得模型过于死板，分类条件过于严格。欠拟合表示模型训练的效果不理想。剪枝的方法分为两种，分别是预剪枝和后剪枝。</li>
<li>4、预剪枝：在决策树构造时就进行剪枝。即在构造的过程中对节点进行评估，如果对某个分支节点在验证集中不能带来准确性的提升，那么对这个分支节点的划分就没有意义，这时就会把当前分支节点作为叶节点。</li>
<li>5、后剪枝：在生成决策树之后再进行剪枝。通常会从决策树的叶节点开始，逐层向上对每个节点进行评估。如果该节点存在与否对节点子树在分类准确性上差别影响不大，或者剪掉该节点子树，能在验证集中带来准确性的提升，那么就可以把该节点子树进行剪枝。使用这个节点子树的叶片节点来替代，类标记为这个节点子树中最频繁的那个类。</li>
<li>6、在决策树中根节点的选择由两个指标组成：纯度和信息熵。</li>
<li>7、纯度：让目标变量的分歧最小。</li>
<li>8、信息熵：表示信息的不确定性<span class="math inline">\(Entropy\left ( t \right ) = -\sum_{i=0}^{c-1}p\left ( i \mid t \right )\log_{2}p\left ( i \mid t \right )\)</span>。其中<span class="math inline">\(p\left ( i \mid t \right )\)</span>表示节点<span class="math inline">\(t\)</span>为分类<span class="math inline">\(i\)</span>的概率。当不确定性越大时，它所包含的信息量也就越大，信息熵也就越高。并且当信息熵越大，纯度越低，而如果集合中的所有样本均匀混合时，信息熵最大，纯度最低。</li>
</ul>
</blockquote>
<blockquote>
<p>构建决策树时通常会采用纯度来构建，而常用的“不纯度”指标有三种，也对应了三种算法，分别是信息增益（ID3 算法）、信息增益率（C4.5算法）以及基尼指数（Cart 算法）。</p>
</blockquote>
<blockquote>
<ul>
<li>1、信息增益（ID3 算法）：即将信息熵最大的作为每个分支的父节点的属性，该方法可以提高分支的纯度，并降低信息熵。计算方法是父节点的信息熵减去所有子节点信息熵和。在具体计算时计算每个子节点的归一化信息熵，即按照每个子节点在父节点中出现的概率。</li>
</ul>
</blockquote>
<p><span class="math display">\[\begin{align}
    Gain\left (D, a \right )=Entropy\left ( D \right ) - \sum_{i=1}^{k}\frac{\left | D_{i} \right |}{\left | D \right |}Entropy\left ( D_{i} \right )
\end{align}\]</span></p>
<blockquote>
<p>其中<span class="math inline">\(D\)</span>表示父节点，<span class="math inline">\(D_{i}\)</span>表示子节点，<span class="math inline">\(Gain\left ( D, a \right )\)</span>中的<span class="math inline">\(a\)</span>表示选择的属性。ID3算法的缺陷是该算法倾向于选择取值种类比较多的属性，即有些属性可能对分类任务没有太大作用，但是他们仍然可能会被选为最优属性。例如极端情况是如果把“编号”用做一个可选属性，那么ID3就会把编号作为最有属性，但是实际上该属性是无关属性。</p>
</blockquote>
<blockquote>
<ul>
<li>2、信息增益率（C4.5算法）：为了避免ID3算法的问题，C4.5算法使用信息增益率作为属性原则的依据，即当属性有很多值的时候，相当于被划分成了许多份，虽然信息增益变大了，但是对于 C4.5 来说，属性熵也会变大，所以整体的信息增益率并不大。</li>
<li>3、悲观剪枝（PEP）：为了解决ID3容易造成过拟合的问题，在构建完成决策树后采用悲观剪枝的方法来提升决策树的泛化能力。该方法是后剪枝技术中的一种，通过递归估算每个内部节点的分类错误率，比较剪枝前后这个节点的分类错误率来决定是否对其进行剪枝，并且这种剪枝方法不需要单独的测试数据集。</li>
<li>4、离散化处理连续属性：C4.5还可以对连续属性进行离散化处理。其方法是选择具有最高信息增益所对应的阈值。</li>
</ul>
</blockquote>
<img src="/review-multilabel-alg/decision-tree.png" class title="决策树">
<p>假设有<span class="math inline">\(n\)</span>个实例的多标签数据集<span class="math inline">\(\mathcal{T} = \left \{ \left ( x_{i}, Y_{i} \right ) \mid 1 \leq i \leq n \right \}\)</span>，通过沿着分割值<span class="math inline">\(\vartheta\)</span>的第<span class="math inline">\(l\)</span>个特征对数据集<span class="math inline">\(\mathcal{T}\)</span>进行划分，可以得到信息增益为：</p>
<p><span class="math display">\[\begin{align}
    \begin{matrix}
        IG\left ( \mathcal{T} , l, \vartheta \right ) = MLEnt\left ( \mathcal{T}  \right ) - \sum \frac{\left | \mathcal{T}  \right |}{\left | \mathcal{T} ^{\rho } \right |}\cdot MLEnt\left ( \mathcal{T} ^{\rho } \right )\left ( \rho \in \left \{ -,+ \right \} \right )\\ 
        where\ \mathcal{T} ^{-}=\left \{ \left ( x_{i},Y_{i} \right ) \mid x_{il} \leq \vartheta , 1 \leq i \leq n \right \}, \ \mathcal{T} ^{+}=\left \{ \left ( x_{i},Y_{i} \right ) \mid x_{il} &gt; \vartheta , 1 \leq i \leq n \right \}
    \end{matrix}
\end{align}\]</span></p>
<p>其中，<span class="math inline">\(\mathcal{T} ^{-}\left(\mathcal{T} ^{+}\right)\)</span>表示在第<span class="math inline">\(l\)</span>个特征上值小于（大于）<span class="math inline">\(\vartheta\)</span>的所有实例。</p>
<p>从根节点（即<span class="math inline">\(\mathcal{T} = \mathcal{D}\)</span>）开始，ML-DT通过识别特征以及根据上式获取的最大信息增益来确认相应的分割点，然后生成关于<span class="math inline">\(\mathcal{T} ^{-}\)</span>和<span class="math inline">\(\mathcal{T} ^{+}\)</span>的两个子节点。通过将<span class="math inline">\(\mathcal{T} ^{-}\)</span>或<span class="math inline">\(\mathcal{T} ^{+}\)</span>作为新的根节点，递归地调用上述过程，直到满足一些停止准则<span class="math inline">\(\mathcal{C}\)</span>才不进行上述过程（即子节点的大小小于预设的阈值）。</p>
<p>要实例化ML-DT，那么就需要有计算多标签熵（即<span class="math inline">\(MLEnt\left ( \cdot \right )\)</span>）的方法。一个简单的方法是把每个相关子标签集<span class="math inline">\(Y \in \mathcal{Y}\)</span>作为一个新类，然后使用传统的单标签熵来计算多标签熵：</p>
<p><span class="math display">\[\begin{align}
    \begin{matrix}
        \widehat{MLEnt\left ( \mathcal{T} \right )}=-\sum \mathbb{P}\left ( Y \right )\cdot \log_{2}\left ( \mathbb{P}\left ( Y \right ) \right )\left ( Y \subseteq \mathcal{Y} \right )\\ 
        where \ \mathbb{P}\left ( Y \right )=\frac{\sum_{i=1}^{n}\left \| Y_{i} = Y \right \|}{n}
    \end{matrix}
\end{align}\]</span></p>
<p>然而，由于新类数量的增长与<span class="math inline">\(\left | \mathcal{Y} \right |\)</span>的空间大小相关，并且呈指数增长，但其中许多新类甚至不会出现在<span class="math inline">\(\mathcal{T}\)</span>中，因此这些新类就具有可以被忽略的估计概率（即<span class="math inline">\(\mathbb{P}\left ( Y \right ) = 0\)</span>）。为了避免这个问题，ML-DT假设标签之间是相互独立的，并且使用可分解的方式计算多标签的熵：</p>
<p><span class="math display">\[\begin{align}
    \begin{matrix}
        MLEnt\left ( \mathcal{T} \right ) = \sum_{j=1}^{q} - p_{j}\log_{2}p_{j}-\left ( 1-p_{j} \right )\log_{2}\left ( 1 - p_{j} \right )\\ 
        where \ p_{j} = \frac{\sum_{i=1}^{n}\left \| y_{i} \in Y_{i} \right \|}{n}
    \end{matrix}
\end{align}\]</span></p>
<p>其中，<span class="math inline">\(p_{j}\)</span>表示在<span class="math inline">\(\mathcal{T}\)</span>中与标签<span class="math inline">\(y_{j}\)</span>相关的实例数量的占比。需要注意的是<span class="math inline">\(MLEnt\left ( \mathcal{T} \right )\)</span>可以被认为是在标签相互独立假设下的<span class="math inline">\(\widehat{MLEnt\left ( \mathcal{T} \right )}\)</span>简化版，并且<span class="math inline">\(MLEnt\left ( \mathcal{T} \right ) &gt; \widehat{MLEnt\left ( \mathcal{T} \right )}\)</span>。</p>
<p>对于未知的实例<span class="math inline">\(x\)</span>，它验证对应路径遍历学习到的决策树，直到到达与多个训练实例<span class="math inline">\(\mathcal{T} \subseteq \mathcal{D}\)</span>相关的叶节点。相应的预测标签集为：</p>
<p><span class="math display">\[\begin{align}
    Y=\left \{ y_{j} \mid p_{j} &gt; 0.5, 1 \leq j \leq q \right \}
\end{align}\]</span></p>
<p>换句话说，假如进入某个叶节点的训练实例都与标签<span class="math inline">\(y_{j}\)</span>相关，就认为该叶节点与标签<span class="math inline">\(y_{j}\)</span>相关，那么在该叶节点中的所有测试实例就都与标签<span class="math inline">\(y_{j}\)</span>相关。</p>
<p>评论：ML-DT是在计算多标签熵时，假设标签之间相互独立情况下的一阶方法。ML-DT的一个显著特点是它能够高效的从多标签数据中获得决策树模型。未来，ML-DT可以从剪枝策略或集成学习技术方面进行改进。</p>
<img src="/review-multilabel-alg/ml-dt.png" class title="多标签决策树">
<h3 id="rank-support-vector-machinerank-svm">Rank Support Vector Machine（Rank-SVM）</h3>
<p>该算法的基本思想是使用最大边缘策略来处理多标签数据，即其中一组线性分类器被优化为最小化经验排序损失，并且使用核技巧处理非线性问题。</p>
<blockquote>
<p>SVM的说明：</p>
</blockquote>
<p>假设学习系统由<span class="math inline">\(q\)</span>条线性分类器<span class="math inline">\(\mathcal{W}=\left \{ \left ( \omega_{j}, b_{j} \right ) \mid 1 \leq j \leq q \right \}\)</span>组成，其中<span class="math inline">\(\omega_{j} \in \mathbb{R}^{d}\)</span>和<span class="math inline">\(b_{j} \in \mathbb{R}\)</span>是第<span class="math inline">\(j\)</span>类标签<span class="math inline">\(y_{j}\)</span>的权重向量和偏差。因此，Rank-SVM通过考虑实例在相关和不相关标签上的排序能力来定义学习系统在实例<span class="math inline">\(x_{i}\)</span>的相关标签<span class="math inline">\(Y_{i}\)</span>（即<span class="math inline">\(\left ( x_{i},Y_{i} \right )\)</span>）上的边界。</p>
<p><span class="math display">\[\begin{align}
    \underset{\left ( y_{j}, y_{k} \right ) \in Y_{i} \times \bar{Y_{i}}}{min}\frac{\left \langle \omega_{j}-\omega_{k},x_{i} \right \rangle + b_{j} - b_{k}}{\left \| \omega_{j}-\omega_{k} \right \|}
\end{align}\]</span></p>
<p>其中，<span class="math inline">\(\left \langle u, \upsilon \right \rangle\)</span>返回内积<span class="math inline">\(u^{\top}\upsilon\)</span>。从几何的角度说，每一个相关与不相关的标签对<span class="math inline">\(\left ( y_{j}, y_{k} \right ) \in Y_{i} \times \bar{Y_{i}}\)</span>，它们的判别边界都对应于一个超平面<span class="math inline">\(\left \langle \omega_{j}-\omega_{k},x \right \rangle + b_{j} - b_{k} = 0\)</span>。因此，上式考虑<span class="math inline">\(x_{i}\)</span>到每个相关、不相关标签对超平面距离的<span class="math inline">\(L_{2}\)</span>，并且返回在实例<span class="math inline">\(x_{i}\)</span>的相关标签<span class="math inline">\(Y_{i}\)</span>上的最小边界。因此，在整个训练集<span class="math inline">\(\mathcal{D}\)</span>上的学习系统边界就为：</p>
<p><span class="math display">\[\begin{align}
    \underset{\left ( x_{i}, Y_{i} \right ) \in \mathcal{D}}{min} \ \underset{\left ( y_{j}, y_{k} \right ) \in Y_{i} \times \bar{Y_{i}}}{min}\frac{\left \langle \omega_{j}-\omega_{k},x_{i} \right \rangle + b_{j} - b_{k}}{\left \| \omega_{j}-\omega_{k} \right \|}
\end{align}\]</span></p>
<blockquote>
<p>超平面的说明：一种忽略空间维数的线性函数。</p>
</blockquote>
<p>当学习系统能够为训练集中实例的每个相关和不相关的标签进行正确排序时，上式就会返回正边界。在这种理想状态下，就可以对线性分类器进行重新缩放：</p>
<p>1、<span class="math inline">\(\forall 1 \leq i \leq m \ and \ \left ( y_{i},y_{k} \right ) \in Y_{i} \times \bar{Y_{i}}, \ \left \langle \omega_{j} - \omega_{k}, x_{i} \right \rangle + b_{j} - b_{k} &gt; 1\)</span></p>
<p>2、<span class="math inline">\(\exists i^{*} \in \left \{ 1, \cdots , m \right \} \ and \ \left ( y_{j^{*}},y_{k^{*}} \right ) \in Y_{i^{*}} \times \bar{Y_{i^{*}}}, \ \left \langle \omega_{j^{*}} - \omega_{k^{*}}, x_{i^{*}} \right \rangle + b_{j^{*}} - b_{k^{*}} &gt; 1\)</span></p>
<p>因此，在整个训练集<span class="math inline">\(\mathcal{D}\)</span>上的学习系统的最大边界就可以表示为：</p>
<p><span class="math display">\[\begin{align}
    \begin{matrix}
        \underset{\mathcal{W}}{max} \ \underset{\left ( x_{i}, Y_{i} \right ) \in \mathcal{D}}{min} \ \underset{\left ( y_{j}, y_{k} \right ) \in Y_{i} \times \bar{Y_{i}}}{min} \ \frac{1}{\left \| \omega_{j}-\omega_{k} \right \|^{2}}\\ 
        subject \ to : \left \langle \omega_{j} - \omega_{k}, x_{i} \right \rangle + b_{j} - b_{k} \geq 1 \ \left ( 1 \leq i \leq m, \left ( y_{j}, y_{k} \right ) \in Y_{i} \times \bar{Y_{i}} \right )
    \end{matrix}
\end{align}\]</span></p>
<p>假设有足够的训练样本，因此对于每个标签对<span class="math inline">\(\left ( y_{j}, y_{k} \right )\left ( j \neq k \right )\)</span>，都存在<span class="math inline">\(\left ( x, Y \right ) \in \mathcal{D}\)</span>满足<span class="math inline">\(\left ( y_{j}, y_{k} \right ) \in Y_{i} \times \bar{Y_{i}}\)</span>。因此，上式就等价于<span class="math inline">\(max_{\mathcal{W}} \ min_{1 \leq j \leq k \leq q}\frac{1}{\left \| \omega_{j}-\omega_{k} \right \|^{2}}\)</span>，并且可以把问题优化后写为：</p>
<p><span class="math display">\[\begin{align}
    \begin{matrix}
        \underset{\mathcal{W}}{min} \ \underset{1 \leq j \leq k \leq q}{max} \ \left \| \omega_{j} - \omega_{k} \right \|^{2}\\ 
        subject \ to : \left \langle \omega_{j} - \omega_{k}, x_{i} \right \rangle + b_{j} - b_{k} \geq 1 \ \left ( 1 \leq i \leq m, \left ( y_{j}, y_{k} \right ) \in Y_{i} \times \bar{Y_{i}} \right )
    \end{matrix}
\end{align}\]</span></p>
<p>为了克服<span class="math inline">\(max\)</span>操作带来的困难，Rank-SVM选择<span class="math inline">\(sum\)</span>操作才逼近<span class="math inline">\(max\)</span>来简化上式：</p>
<p><span class="math display">\[\begin{align}
    \begin{matrix}
        \underset{\mathcal{W}}{min} \sum_{i=1}^{q} \left \| \omega_{j} \right \|^{2}\\ 
        subject \ to : \left \langle \omega_{j} - \omega_{k}, x_{i} \right \rangle + b_{j} - b_{k} \geq 1 \ \left ( 1 \leq i \leq m, \left ( y_{j}, y_{k} \right ) \in Y_{i} \times \bar{Y_{i}} \right )
    \end{matrix}
\end{align}\]</span></p>
<p>为了解决上式中的约束在实际情况下不能得到满足，因此在上式中引入了松弛变量：</p>
<p><span class="math display">\[\begin{align}
    \begin{matrix}
        \underset{\left \{ \mathcal{W}, \Xi \right \}}{min}\sum_{j=1}^{q}\left \| \mathcal{W}_{j} \right \|^{2} + C\sum_{i=1}^{m}\frac{1}{\left | Y_{i} \right |\left | \bar{Y_{i}} \right |}\underset{\left ( y_{j},y_{k} \right ) \in Y_{i} \times \bar{Y_{i}}}{\sum }\xi_{ijk}\\ 
        subject \ to : \left \langle \omega_{j} - \omega_{k}, x_{i} \right \rangle + b_{j} - b_{k} \geq 1 - \xi_{ijk},\ \xi_{ijk} \geq 0 \left ( 1 \leq i \leq m, \left ( y_{j}, y_{k} \right ) \in Y_{i} \times \bar{Y_{i}} \right )
    \end{matrix}
\end{align}\]</span></p>
<p>其中，<span class="math inline">\(\Xi = \left \{ \xi_{ijk} \mid 1 \leq i \leq m, \left ( y_{j}, y_{k} \right ) \in Y_{i} \times \bar{Y_{i}} \right \}\)</span>是一个松弛变量集。上式中的目标由权衡参数<span class="math inline">\(C\)</span>平衡的两部分组成。具体而言，第一部分对应于学习系统的边界，而第二部分对应于以铰链形式实现的学习系统替代了以排序损失实现的学习系统。注意，替代排序损失也可以用其他方式来实现，例如神经网络的全局误差函数的指数形式。</p>
<blockquote>
<p>上面一段都没动的说明：</p>
</blockquote>
<p>需要注意的是，上式是一个具有凸目标和线性约束的标准二次规划（QP）问题，可以用现成的QP求解器进行求解。此外，为了使Rank-SVM具有非线性分类能力，一种常用的方法是通过核技巧求解上式的对偶形式。此外，Rank-SVM还可以使用stacking来设置阈值函数：</p>
<p><span class="math display">\[\begin{align}
    \begin{matrix}
        t\left ( x  \right ) = \left \langle \omega^{*}, f^{*}\left ( x \right ) \right \rangle + b^{*}\\ 
        where \ \ f^{*}\left ( x \right ) = \left ( f\left ( x, y_{1} \right ), \cdots , f\left ( x, y_{q} \right ) \right )^{T} \ and \ f\left ( x, y_{j} \right ) = \left \langle \omega_{j}, x \right \rangle + b_{j}
    \end{matrix}
\end{align}\]</span></p>
<p>然后对于未知实例<span class="math inline">\(x\)</span>，预测的标签集为：</p>
<p><span class="math display">\[\begin{align}
    Y = \left \{ y_{j} \mid \left \langle \omega_{j}, x \right \rangle + b_{j} &gt; \left \langle \omega^{*}, f^{*}\left ( x \right ) \right \rangle + b^{*}, 1 \leq j \leq q \right \}
\end{align}\]</span></p>
<p>评论：Rank-SVM是一个在相关和不相关标签对上定义了超平面边界的二阶方法。该算法得益于核技术在处理非线性分类上的优势，并且基于此可以进一步的实现相关变体。首先，上式中的经验排序损失可以用其他损失结构来代替，如汉明损失。它可以被归类为结构化输出分类的一般形式。其次，阈值策略可以用stacking以外的技术来实现。第三，为了避免核技术在选择的问题，因此在多标签数据中可以使用多核技术来进行学习。</p>
<img src="/review-multilabel-alg/rank-svm.png" class title="Rank-SVM">
<h3 id="collective-multi-label-classifiercml">Collective Multi-Label Classifier（CML）</h3>
<p>该算法的基本思想是适应最大熵原理处理多标签数据，其中标签之间的相关性被编码为所得到的结果，其分布必须满足的约束。</p>
<p>对于任意的对标签实例<span class="math inline">\(\left ( x, Y \right )\)</span>，让<span class="math inline">\(\left(x, y\right)\)</span>使用二元标签向量<span class="math inline">\(y = \left ( y_{1}, y_{2}, \cdots , y_{q} \right )^{T} \in \left \{ -1, +1 \right \}^{q}\)</span>来表示相应的随机变量，其中第<span class="math inline">\(j\)</span>个分量表示<span class="math inline">\(Y\)</span>是否包含第<span class="math inline">\(j\)</span>个标签，即<span class="math inline">\(\left(y_{j} = +1\right)\)</span>表示包含，<span class="math inline">\(\left(y_{j} = -1\right)\)</span>表示不包含。从统计上讲，多标签学习任务可以等价于学习联合概率分布<span class="math inline">\(p\left(x, y\right)\)</span>。</p>
<p>假设给定他们的分布<span class="math inline">\(p\left ( \cdot ,\cdot \right )\)</span>，那么<span class="math inline">\(\mathcal{H}_{p}\left(x, y\right)\)</span>就表示实例与标签<span class="math inline">\(\left(x, y\right)\)</span>的熵。最大熵的原理是假定当前知识状态的最佳分布模型是一个服从与给定事实集合<span class="math inline">\(\mathcal{K}\)</span>的最大<span class="math inline">\(\mathcal{H}_{p}\left(x, y\right)\)</span>。</p>
<p><span class="math display">\[\begin{align}
    \begin{matrix}
        \underset{p}{max} \ \mathcal{H}_{p}\left ( x, y \right )\\ 
        subject to: \ \mathbb{E}_{p}\left [ f_{k}\left ( x, y \right ) \right ] = F_{k} \ \left ( k \in \mathcal{K} \right )
    \end{matrix}
\end{align}\]</span></p>
<p>一般来说，事实被表达为在实例与标签<span class="math inline">\(\left(x, y\right)\)</span>某个函数期望的约束，即约束为<span class="math inline">\(\mathbb{E}_{p}\left [ f_{k}\left ( x, y \right ) \right ] = F_{k}\)</span>。这里的<span class="math inline">\(\mathbb{E}_{p}\left(\cdot\right)\)</span>表示<span class="math inline">\(p\left( \cdot, \cdot\right)\)</span>的期望算子，而<span class="math inline">\(F_{k}\)</span>则表示训练集估计的期望值，如<span class="math inline">\(\frac{1}{m} \sum_{\left ( x, y \right ) \in \mathcal{D}}f_{k}\left ( x, y \right )\)</span>。</p>
<p>利用标准拉格朗日乘数法，结合<span class="math inline">\(p\left( \cdot, \cdot\right)\)</span>上的归一化约束（即<span class="math inline">\(\mathbb{E}_{p}\left[1\right] = 1\)</span>），可以求解方程（51）的约束优化问题。因此，最优分布落在了Gibbs分布族上。</p>
<p><span class="math display">\[\begin{align}
    p\left ( y \mid x \right ) = \frac{1}{Z_{\Lambda}\left ( x \right )}exp\left ( \sum_{k \in \mathcal{K}} \lambda_{k} \cdot f_{k}\left ( x, y \right ) \right )
\end{align}\]</span></p>
<blockquote>
<p>Gibbs的说明：</p>
</blockquote>
<p>这里的<span class="math inline">\(\Lambda = \left \{ \lambda_{k} \mid k \in \mathcal{K} \right \}\)</span>是一组待确定的参数集合，并且<span class="math inline">\(Z_{\Lambda}\left ( x \right )\)</span>是归一化因子的分区函数，即：</p>
<p><span class="math display">\[\begin{align}
    Z_{\Lambda}\left ( x \right ) = \sum_{x}exp\left ( \sum_{k \in \mathcal{K}} \lambda_{k} \cdot f_{k}\left ( x, y \right ) \right )
\end{align}\]</span></p>
<p>通过假设高斯先验（即<span class="math inline">\(\lambda_{k} \sim \mathcal{N}\left ( 0, \varepsilon^{2} \right )\)</span>）。通过最大化对数后验概率函数可以找到<span class="math inline">\(\Lambda\)</span>中的参数。</p>
<p><span class="math display">\[\begin{align}
    \begin{matrix}
        l\left ( \Lambda \mid \mathcal{D} \right ) = log\left ( \prod_{\left ( x, y \right ) \in \mathcal{D}} p\left ( y \mid x \right ) \right ) - \sum_{k \in \mathcal{K}}\frac{\lambda_{2}^{l}}{2\varepsilon^{2}}\\ 
        =\sum_{\left ( x, y \right ) \in \mathcal{D}}\left ( \sum_{k \in \mathcal{K}} \lambda_{k} \cdot f_{k}\left ( x, y \right ) - logZ_{\Lambda}\left ( x \right ) \right ) - \sum_{k \in \mathcal{K}}\frac{\lambda_{2}^{k}}{2\varepsilon^{2}}
    \end{matrix}
\end{align}\]</span></p>
<p>需要注意的是，等式（54）是<span class="math inline">\(\Lambda\)</span>上的一个凸函数，其全局最大值（虽然不是封闭形式）可以由任何现成的无约束优化方法如BFGS找到。一般来说，大多数数值分析都需要<span class="math inline">\(l\left ( \Lambda \mid \mathcal{D} \right )\)</span>的梯度。</p>
<p><span class="math display">\[\begin{align}
    \frac{\partial l\left ( \Lambda \mid \mathcal{D} \right )}{\partial \lambda_{k}}=\sum_{\left ( x,y \right ) \in \mathcal{D}}\left ( f_{k}\left ( x, y \right ) - \sum_{u}f_{k}\left ( x, y \right )p\left ( y \mid x \right ) \right )-\frac{\lambda_{k}}{\varepsilon^{2}} \ \left ( k \in \mathcal{K} \right )
\end{align}\]</span></p>
<p>对于CML，约束集通常包含两部分<span class="math inline">\(\mathcal{K} = \mathcal{K}_{1} \cup \mathcal{K}_{2}\)</span>。具体来说，<span class="math inline">\(\mathcal{K}_{1} = \left \{ \left ( l, j \right ) \mid 1 \leq l \leq d, 1 \leq j \leq q \right \}\)</span>使用<span class="math inline">\(f_{k}\left ( x, y \right ) = x_{l} \cdot \left \| y_{j} = 1 \right \|\left ( k = \left ( l, j \right ) \in \mathcal{K}_{1} \right )\)</span>指定了全部<span class="math inline">\(d \cdot q\)</span>个约束。此外，<span class="math inline">\(\mathcal{K}_{2} = \left \{ \left ( j_{1}, j_{2}, b_{1}, b_{2} \right ) \mid 1 \leq j_{1} \leq j_{2} \leq q , b_{1}, b_{2} \in \left \{ -1, +1 \right \}\right \}\)</span>使用<span class="math inline">\(f_{k}\left ( x, y \right ) = \left \| y_{j_{1}} = b_{1} \right \| \cdot \left \| y_{j_{2}} = b_{2} \right \| \left ( k = \left ( j_{1}, j_{2}, b_{1}, b_{2} \right ) \in \mathcal{K}_{2} \right )\)</span>指定了全部<span class="math inline">\(4 \cdot \binom{q}{2}\)</span>个约束。实际上，<span class="math inline">\(CML\)</span>的<span class="math inline">\(\mathcal{K}\)</span>中的约束可以用其他变体方式指定。</p>
<p>对于位置的实例<span class="math inline">\(x\)</span>，预测的标签集为：</p>
<p><span class="math display">\[\begin{align}
    Y = arg \ \underset{y}{max} \left ( y \mid x \right )
\end{align}\]</span></p>
<p>需要注意的是，使用<span class="math inline">\(arg \ max\)</span>进行精确推论只适用于较小的标签集。否则需要使用剪枝技术来减少<span class="math inline">\(arg \ max\)</span>的搜索空间，例如只考虑训练集中出现的标签集。</p>
<p>评论：CML是一个在每个标签对的相关性上增加了<span class="math inline">\(\mathcal{K}_{2}\)</span>约束的二阶方法。CML所研究的二阶相关性比Rank-SVM更具一般性，因为Rank-SVM只考虑相关与不相关的标签对。作为条件随机场模型（CRF），CML倾向使用在等式（52）中使用条件概率分布<span class="math inline">\(p\left ( y \mid x \right )\)</span>来进行分类。有趣的是，<span class="math inline">\(p\left ( y \mid x \right )\)</span>可以使用多种方式进行分解，例如<span class="math inline">\(p\left ( y \mid x \right ) = \prod_{j=1}^{q}p\left ( y_{j} \mid x, y_{1}, \cdots , y_{j-1} \right )\)</span>，其中的每一项可以由分类器链中的分类器来进行建模，或者<span class="math inline">\(p\left ( y \mid x \right ) = \prod_{j=1}^{q}p\left ( y_{j} \mid x, pa_{j} \right )\)</span>，其中的每一项可以由有向图中的节点<span class="math inline">\(y_{j}\)</span>和它的父节点<span class="math inline">\(pa_{j}\)</span>进行建模，并且当有向图具有有限拓扑结构的多位贝叶斯网络时，该算法成立。有向图也可用于建模多故障诊断，其中<span class="math inline">\(y_{j}\)</span>表示某一个设备部件的良好或故障状态。另一方面，已经有了一些多标签生成模型，其目的是模拟联合概率分布<span class="math inline">\(p\left ( y \mid x \right )\)</span>。</p>
<img src="/review-multilabel-alg/cml.png" class title="CML">
<blockquote>
<p>CRF的说明：</p>
</blockquote>
<h1 id="总结">总结</h1>
<img src="/review-multilabel-alg/algorithms-reviewed.png" class title="多标签学习算法综述">
<p>图5总结了八种多标签学习算法的特性，包括基本思想、标签相关性、计算复杂性、已经测试域和优化（代理）指标。从图5可以知道，汉明损失和排序损失是最流行的度量指标之一，并且在前文也对其进行了理论分析。此外，值得注意的是，通过Random k-Labelsets优化的子集精度仅是对k-Labelsets而不是整个标签空间。</p>
<p>图5中的测试领域是该算法对应的原始文件中使用效果最好的数据类型。然而，所有这些有代表性的多标签学习算法都是通用的，并且可以应用于各种数据类型。然而，每种学习算法的计算复杂度对其适用于不同规模的数据起着关键的作用。这里，可以从三个主要方面来研究数据可伸缩性，包括训练实例的数量（即<span class="math inline">\(m\)</span>）、维度（即<span class="math inline">\(d\)</span>）和可能的标签类别数量（即<span class="math inline">\(q\)</span>）。此外，当实例的维度远远大于类标签的数量时（即<span class="math inline">\(d \gg q\)</span>），使用将标签类别作为实例空间的额外特征的策略将对算法不会有太多改善。</p>
<p>作为研究最多的监督学习框架，图5中的几种算法都采用二元分类器作为多标签数据学习的中间步骤。对二元分类的转换的最初来源于著名的AdaBoost.MH算法，在该算法中每个多标签训练实例<span class="math inline">\(\left ( x_{i}, Y_{i} \right )\)</span>被转换成<span class="math inline">\(q\)</span>个二元实例<span class="math inline">\(\left \{ \left ( \left [ x_{i}, y_{j} \right ],\phi \left ( Y_{i}, y_{j} \right ) \right ) \mid 1 \leq j \leq q\right \}\)</span>。该算法被认为是一种高阶方法，其中<span class="math inline">\(\mathcal{Y}\)</span>中的标签被认为是<span class="math inline">\(\mathcal{X}\)</span>的附加特征，并且通过共享实例<span class="math inline">\(x\)</span>把彼此之间进行关联，只要二元学习算法<span class="math inline">\(\mathcal{B}\)</span>能够捕获特征之间的依赖关系。二元分类转换的其他方法可以通过诸如堆叠聚合或纠错输出码（ECOC）等技术来实现。</p>
<blockquote>
<p>AdaboSt.MH的说明：基于AdaBoost.M1算法的一种标签转化方案的多标签算法。</p>
</blockquote>
<blockquote>
<p>堆叠聚合的说明：</p>
</blockquote>
<blockquote>
<p>ECOC的说明：能够将多类分类问题转化为多个二分类问题，而且利用纠错输出码本身具有纠错能力的特性，可以提高监督学习算法的预测精度。</p>
</blockquote>
<p>此外，适应一阶算法的方法不能简单地认为是二元相关性与特定二元学习者的结合。例如，ML-KNN不仅仅是二元相关性与KNN的结合，而是利用贝叶斯推理与相邻信息进行推理。同时，ML-DT也不仅仅是二元相关性与决策树结合组成为单一决策树，而构建了<span class="math inline">\(q\)</span>个决策树，以适应所有类别的标签（基于多标签熵）。</p>
<h1 id="相关学习的设置">相关学习的设置</h1>
<p>与多标签学习相关的学习设置有很多值得讨论的问题，如多实例学习、有序分类、多任务学习、数据流分类等。</p>
<p>在多实例学习研究的问题中每个示例是由一袋实例和一个（二元）标签来描述。只有当该袋实例中至少包含了一个正实例，那么该袋就被认为是正例。与输出（标签）空间具有对象歧义（复杂语义）的多标签学习模型相比，多实例学习可以被认为是在输入（实例）空间具有对象的歧义性的模型。目前在多标签数据中多实例学习已经有了一些初步的尝试。</p>
<p>有序分类研究的问题是对所有类别的标签进行自然排序。在多标签学习中，可以对每类标签的相关性进行排序，以便将各个标签（<span class="math inline">\(y_{j} \in \left \{ -1, +1 \right \}\)</span>）进行分级归类（<span class="math inline">\(y_{j} \in \left \{ m_{1}, m_{2}, \cdots , m_{k} \right \} \ where \ m_{1} &lt; m_{2} &lt; \cdots &lt; m_{k}\)</span>）。因此，分级多标签学习只能提供模糊的序号，而不是标签相关性的明确判断。现有的研究表明，分级多标签学习可以通过将其转化为一组有序分类问题（每类标签都是一个问题），或一组标准的多标签学习问题（每个成员级别是一个问题）来解决。</p>
<p>多任务学习主要研究的是多个任务并行训练的问题，利用相关任务的训练信息作为归纳偏差，以此来提高其他任务的泛化性能。然而，多任务学习与多标签学习之间存在着本质的区别。首先，在多标签学习中，所有的示例共享相同的特征空间，而在多任务学习中，任务可以在相同的特征空间，也可以在不同的特征空间。其次，多标签学习的目的是预测与对象相关联的标签子集，而多任务学习的目的是同时完成多个任务，并不关心哪个任务子集应该与哪个对象相关联（该学习算法中把标签认为是一个任务），因为它通常假定每个对象与所有任务相关。第三，在多标签学习中，处理标签空间较大的问题很常见，但在多任务学习中，考虑大量任务是不合理的。然而，多任务学习的技术可能可以用来帮助多标签学习。</p>
<p>数据流分类主要研究现实世界中的对象在线生成和实时处理的问题。目前，多标签的流媒体数据广泛存在于即时新闻、电子邮件、微博等现实场景中。作为流媒体数据分析的常见挑战，如何有效地对多标签数据流进行分类是处理概念漂移问题的关键因素。现有的数据流分类模型通过在新的一批示例到达时更新分类器来实现概念漂移。一般采用采用衰落假设，即随着时间的推移，过去数据的影响逐渐下降，或者每当检测到概念漂移时保持变化检测器报警。</p>
<h1 id="结论">结论</h1>
<p>在本文中，回顾了最先进的多标签学习的范式、算法和相关的学习设置。此外，本文并没有尝试回顾所有的学习技术，而是选择八个最具代表性的多标签学习算法进行描述。在下表中总结了一些用于多标签学习的在线资源，包括学术活动（教程、研讨会、专题）、公开可用的软件和数据集。</p>
<img src="/review-multilabel-alg/online-resources.png" class title="多标签学习的在线资源">
<p>目前，虽然标签之间的相关性已经被用于各种多标签学习技术，但是在标签相关性的使用上还没有任何关于底层概念或任何原理性机制。最近的研究表明，标签之间的相关性可能是不对称的，即一个标签对另一个标签的影响不一定在相反的方向上是相同的，或者是局部相同，即不同的实例共享不同的标签相关性，很少有全局适用的相关性。然而，充分理解标签相关性，特别是对于具有较大输出空间的场景，仍然是多标签学习的目标。</p>
<p>在第三节重点介绍多标签学习算法的特性。这篇综述的一个对不同多标签学习算法优缺点的深入研究的补充。在文献中，我们可以找到一个最近尝试进行的具有广泛性的实验比较，即12个多标签学习算法与16个评价指标进行了比较。有趣的是，分类和排名指标最佳的算法是基于集成学习技术，即预测决策树的随机森林。然而，在更广泛的范围内或集中在某个类型内模型比较是值得进一步探讨的课题。</p>
]]></content>
      <categories>
        <category>论文</category>
        <category>综述</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>多标签分类</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux系统装机工具和方法</title>
    <url>/osinitial/</url>
    <content><![CDATA[<h1 id="linux常用软件安装">Linux常用软件安装</h1><h2 id="vim">VIM</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:jonathonf/vim</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install vim</span><br></pre></td></tr></table></figure><h2 id="shadowsocks-qt5">Shadowsocks-Qt5</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:hzwhuang/ss-qt5</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install shadowsocks-qt5</span><br></pre></td></tr></table></figure><a id="more"></a>




<h2 id="libreoffice">LibreOffice</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:libreoffice/ppa</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install libreoffice</span><br></pre></td></tr></table></figure>
<h2 id="shutter">Shutter</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:shutter/ppa</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install shutter</span><br></pre></td></tr></table></figure>
<h2 id="vscode">VSCode</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install curl</span><br><span class="line">curl https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor &gt; microsoft.gpg</span><br><span class="line">sudo mv microsoft.gpg /etc/apt/trusted.gpg.d/microsoft.gpg</span><br><span class="line">sudo sh -c <span class="string">'echo "deb [arch=amd64] https://packages.microsoft.com/repos/vscode stable main" &gt; /etc/apt/sources.list.d/vscode.list'</span></span><br><span class="line">sudo apt-get install apt-transport-https</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install code</span><br></pre></td></tr></table></figure>
<h2 id="deluge">Deluge</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:deluge-team/ppa</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install deluge</span><br></pre></td></tr></table></figure>
<h2 id="emacs">Emacs</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:kelleyk/emacs</span><br><span class="line">sudo apt update</span><br><span class="line">sudo apt install emacs25</span><br></pre></td></tr></table></figure>
<h2 id="gimp">GIMP</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:otto-kesselgulasch/gimp</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt install gimp</span><br></pre></td></tr></table></figure>
<h2 id="simplescreenrecorder">SimpleScreenRecorder</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:maarten-baert/simplescreenrecorder</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install simplescreenrecorder</span><br></pre></td></tr></table></figure>
<h2 id="timeshift">Timeshift</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-add-repository -y ppa:teejee2008/ppa</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install timeshift</span><br></pre></td></tr></table></figure>
<h2 id="sysmonitor">Sysmonitor</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:fossfreedom/indicator-sysmonitor</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install indicator-sysmonitor</span><br></pre></td></tr></table></figure>
<h2 id="vlc">VLC</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:videolan/master-daily</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install vlc</span><br></pre></td></tr></table></figure>
<h2 id="有道字典">有道字典</h2>
<p>首先到有道词典官网下载deb安装包，注意有道词典Ubuntu版本只支持到Ubuntu 14.04,如果在Ubuntu 16.04上安装会失败，因为官方的Ubuntu版本的deb包依赖gstreamer0.10-plugins-ugly，但是该软件在16.04里面已经没有了。所以我们要下载64位的deepin版的安装包。经过测试，64位的deepin版的deb包在Ubuntu 16.04上安装成功。</p>
<h2 id="guake">Guake</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:webupd8team/unstable</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install guake</span><br></pre></td></tr></table></figure>
<p>让Guake随系统自动启动，定位到"系统-&gt;首选项-&gt;启动程序-&gt;打开'启动程序喜好'程序"，单击'添加'按钮，在弹出的窗口中填写程序名称、命令和注释，命令一定不要写错（可以是程序名字guake，也可以是命令的绝对路径如/usr/bin/guake，建议采用前者），其他可以随便填。</p>
<h2 id="wps">WPS</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install lib32ncurses5 lib32z1 libpng12-0</span><br></pre></td></tr></table></figure>
<p><a href="http://wps-community.org/downloads?vl=fonts#download" target="_blank" rel="noopener">下载最新的WPS地址</a></p>
<blockquote>
<ul>
<li>下载该字体，解压后将整个wps_symbol_fonts中的内容目录拷贝到 /usr/share/fonts/wps-office 目录下</li>
<li>进入字体目录 /usr/share/fonts/wps-office</li>
<li>sudo mkfontdir</li>
<li>sudo mkfontscale</li>
<li>fc-cache</li>
</ul>
</blockquote>
<h2 id="指纹登录">指纹登录</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:fingerprint/fprint</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get upgrade</span><br><span class="line">sudo apt-get install libfprint0 fprint-demo libpam-fprintd</span><br></pre></td></tr></table></figure>
<h2 id="uget">UGet</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:plushuang-tw/uget-stable</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install uget aria2</span><br></pre></td></tr></table></figure>
<h2 id="ugetchromewrapper">uGetChromeWrapper</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:slgobinath/uget-chrome-wrapper</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install uget aria2</span><br></pre></td></tr></table></figure>
<h2 id="远程桌面remminal2w">远程桌面Remmina（L2W）</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-add-repository ppa:remmina-ppa-team/remmina-next</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install remmina remmina-plugin-rdp libfreerdp-plugins-standard</span><br></pre></td></tr></table></figure>
<h2 id="远程桌面xfcew2l">远程桌面XFCE（W2L）</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install xrdp vnc4server xfce4</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"xfce4-session"</span> &gt;~/.xsession</span><br><span class="line">sudo service xrdp restart</span><br></pre></td></tr></table></figure>
<h2 id="主题">主题</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install unity-tweak-tool</span><br><span class="line"></span><br><span class="line">sudo add-apt-repository ppa:noobslab/themes</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install flatabulous-theme</span><br><span class="line"></span><br><span class="line">sudo add-apt-repository ppa:noobslab/icons</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install ultra-flat-icons</span><br></pre></td></tr></table></figure>
<h2 id="蓝牙耳机">蓝牙耳机</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install blueman bluez*</span><br></pre></td></tr></table></figure>
<h2 id="virtualbox">VirtualBox</h2>
<p><a href="https://www.virtualbox.org/wiki/Linux_Downloads" target="_blank" rel="noopener">下载地址</a></p>
<blockquote>
<ul>
<li>Qt 5.3.2 or later. Qt 5.6.2 or later is recommended.</li>
<li>SDL 1.2.7 or later. This graphics library is typically called libsdl or similar.</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo dpkg -i virtualbox-6.0_6.0.4-128413~Ubuntu~xenial_amd64.deb</span><br></pre></td></tr></table></figure>
<h2 id="putty">Putty</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install libgtk2.0-dev</span><br><span class="line"><span class="built_in">cd</span> unix</span><br><span class="line">./configure</span><br><span class="line">make -f Makefile.gtk</span><br><span class="line"><span class="built_in">cd</span> ..</span><br><span class="line">make install</span><br></pre></td></tr></table></figure>
<h2 id="ftp的安装与设置">FTP的安装与设置</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install vsftpd</span><br></pre></td></tr></table></figure>
<p>修改FTP配置文件/etc/vsftpd.conf</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 是否开放本地用户写权限，即是否允许上传</span></span><br><span class="line">write_enable=YES</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为YES时，禁止所有用户访问上级目录，只能访问各自的家目录</span></span><br><span class="line">chroot_local_user=NO</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置vsftpd使用utf8编码的文件系统</span></span><br><span class="line">utf8_filesystem=YES</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果不设置这个，会出现425 Security: Bad IP connecting.类似于这种的错误</span></span><br><span class="line">pasv_enable=YES</span><br><span class="line">pasv_min_port=40000</span><br><span class="line">pasv_max_port=40010</span><br><span class="line">pasv_promiscuous=YES</span><br></pre></td></tr></table></figure>
<h2 id="putty配色">Putty配色</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">* Default Foreground: 255/255/255  </span><br><span class="line">* Default Background: 51/51/51  </span><br><span class="line">* ANSI Black: 77/77/77  </span><br><span class="line">* ANSI Green: 152/251/152  </span><br><span class="line">* ANSI Yellow: 240/230/140  </span><br><span class="line">* ANSI Blue: 205/133/63  </span><br><span class="line">* ANSI Blue Bold 135/206/235  </span><br><span class="line">* ANSI Magenta: 255/222/173 or 205/92/92  </span><br><span class="line">* ANSI Cyan: 255/160/160  </span><br><span class="line">* ANSI Cyan Bold: 255/215/0  </span><br><span class="line">* ANSI White: 245/222/179</span><br></pre></td></tr></table></figure>
<h2 id="telnet安装">Telnet安装</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install openbsd-inetd</span><br><span class="line">sudo apt-get install telnetd</span><br><span class="line">sudo service openbsd-inetd restart</span><br></pre></td></tr></table></figure>
<h2 id="firefox安装">FireFox安装</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:mozillateam/firefox-next </span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install firefox</span><br></pre></td></tr></table></figure>
<h2 id="ubuntu下罗技优联管理关键">Ubuntu下罗技优联管理关键</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install solaar-gnome3</span><br></pre></td></tr></table></figure>
<h2 id="安装ubuntu_live镜像制作软件cubic">安装Ubuntu_Live镜像制作软件cubic</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 081525E2B4F1283B</span><br><span class="line">sudo apt-add-repository ppa:cubic-wizard/release</span><br><span class="line">sudo apt update</span><br><span class="line">sudo apt install cubic</span><br></pre></td></tr></table></figure>
<h2 id="安装tmux">安装Tmux</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install tmux</span><br></pre></td></tr></table></figure>
<h2 id="安装ssh服务">安装SSH服务</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install openssh-server</span><br></pre></td></tr></table></figure>
<h2 id="peek">Peek</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:peek-developers/stable</span><br><span class="line">sudo apt update</span><br><span class="line">sudo apt install peek</span><br></pre></td></tr></table></figure>
<h2 id="kcptun">KCPTUN</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#! /bin/sh</span></span><br><span class="line"><span class="comment">### BEGIN INIT INFO</span></span><br><span class="line"><span class="comment"># Provides:          kcptun</span></span><br><span class="line"><span class="comment"># Required-Start:    </span></span><br><span class="line"><span class="comment"># Required-Stop:</span></span><br><span class="line"><span class="comment"># Should-Start:      </span></span><br><span class="line"><span class="comment"># Default-Start:     1 2 3 4 5</span></span><br><span class="line"><span class="comment"># Default-Stop:</span></span><br><span class="line"><span class="comment"># Short-Description: kcptun</span></span><br><span class="line"><span class="comment"># Description:       Network file systems are mounted by</span></span><br><span class="line"><span class="comment">#                    /etc/network/if-up.d/mountnfs in the background</span></span><br><span class="line"><span class="comment">#                    when interfaces are brought up; this script waits</span></span><br><span class="line"><span class="comment">#                    for them to be mounted before carrying on.</span></span><br><span class="line"><span class="comment">### END INIT INFO</span></span><br><span class="line"></span><br><span class="line">/opt/kcptun-linux-amd64/client_linux_amd64 -l :3000 -r 45.77.188.138:29900 -key <span class="string">"123456"</span> -crypt none -nocomp -datashard 70 -parityshard 30 -mtu 1400 -sndwnd 2048 -rcvwnd 2048 -dscp 46 -quiet -mode fast2</span><br></pre></td></tr></table></figure>
<h2 id="apt-fast">APT-FAST</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:apt-fast/stable</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install apt-fast</span><br></pre></td></tr></table></figure>
<h2 id="kvm">KVM</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install qemu-kvm qemu virt-manager virt-viewer libvirt-bin bridge-utils</span><br></pre></td></tr></table></figure>
<h2 id="sqlitebrowser">SQLiteBrowser</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository -y ppa:linuxgndu/sqlitebrowser</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install sqlitebrowser</span><br></pre></td></tr></table></figure>
<h2 id="masterpdf">MasterPDF</h2>
<p><a href="https://code-industry.net/downloads/" target="_blank" rel="noopener">官网</a></p>
<h2 id="texlive">Texlive</h2>
<p><a href="https://mirrors.tuna.tsinghua.edu.cn/CTAN/systems/texlive/Images/" target="_blank" rel="noopener">清华大学镜像下载地址</a></p>
<blockquote>
<ul>
<li>1、图形化安装</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install perl-tk</span><br><span class="line">sudo mount -o loop texlive2019.iso /mnt</span><br><span class="line"><span class="built_in">cd</span> /mnt/</span><br><span class="line">sudo ./install-tl -gui</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>2、设置字体</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo cp /usr/<span class="built_in">local</span>/texlive/2019/texmf-var/fonts/conf/texlive-fontconfig.conf /etc/fonts/conf.d/09-texlive.conf</span><br><span class="line">sudo <span class="built_in">fc</span>-cache -fsv</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>3、设置环境变量</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> MANPATH=<span class="variable">$&#123;MANPATH&#125;</span>:/usr/<span class="built_in">local</span>/texlive/2019/texmf-dist/doc/man</span><br><span class="line"><span class="built_in">export</span> INFOPATH=<span class="variable">$&#123;INFOPATH&#125;</span>:/usr/<span class="built_in">local</span>/texlive/2019/texmf-dist/doc/info</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$&#123;PATH&#125;</span>:/usr/<span class="built_in">local</span>/texlive/2019/bin/x86_64-linux</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>4、设置系统MAN</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">MANPATH_MAP /usr/<span class="built_in">local</span>/texlive/2019/bin/x86_64-linux /usr/<span class="built_in">local</span>/texlive/2019/texmf-dist/doc/man</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>5、启动环境变量</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">source</span> ~/.bashrc</span><br><span class="line"><span class="built_in">source</span> ~/.profile</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>6、测试</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tex --version</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>7、弹出镜像</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo umount /mnt</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>8、VSCode配置</li>
</ul>
</blockquote>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">"latex-workshop.latex.recipes": [</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="attr">"name"</span>: <span class="string">"xelatex"</span>,</span><br><span class="line">        <span class="attr">"tools"</span>: [</span><br><span class="line">            <span class="string">"xelatex"</span></span><br><span class="line">        ]</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="attr">"name"</span>: <span class="string">"xelatex ➞ bibtex ➞ exlatex × 2"</span>,</span><br><span class="line">        <span class="attr">"tools"</span>: [</span><br><span class="line">            <span class="string">"xelatex"</span>,</span><br><span class="line">            <span class="string">"bibtex"</span>,</span><br><span class="line">            <span class="string">"xelatex"</span>,</span><br><span class="line">            <span class="string">"xelatex"</span></span><br><span class="line">        ]</span><br><span class="line">    &#125;</span><br><span class="line">],</span><br><span class="line">"latex-workshop.latex.tools": [</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="attr">"name"</span>: <span class="string">"xelatex"</span>,</span><br><span class="line">        <span class="attr">"command"</span>: <span class="string">"xelatex"</span>,</span><br><span class="line">        <span class="attr">"args"</span>: [</span><br><span class="line">            <span class="string">"-synctex=1"</span>,</span><br><span class="line">            <span class="string">"-interaction=nonstopmode"</span>,</span><br><span class="line">            <span class="string">"-file-line-error"</span>,</span><br><span class="line">            <span class="string">"%DOC%"</span></span><br><span class="line">        ]</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="attr">"name"</span>: <span class="string">"bibtex"</span>,</span><br><span class="line">        <span class="attr">"command"</span>: <span class="string">"bibtex"</span>,</span><br><span class="line">        <span class="attr">"args"</span>: [</span><br><span class="line">            <span class="string">"%DOCFILE%"</span></span><br><span class="line">        ]</span><br><span class="line">    &#125;</span><br><span class="line">],</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>9、配置宏包的源</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo tlmgr option repository https://mirrors.tuna.tsinghua.edu.cn/CTAN/systems/texlive/tlnet</span><br><span class="line">sudo tlmgr update --self --all</span><br></pre></td></tr></table></figure>
<h2 id="miktex">MiKTeX</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys D6BC243565B2087BC3F897C9277A7293F59E4889</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"deb http://miktex.org/download/ubuntu xenial universe"</span> | sudo tee /etc/apt/sources.list.d/miktex.list</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install miktex</span><br></pre></td></tr></table></figure>
<h2 id="pinta">Pinta</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-fast install pinta</span><br></pre></td></tr></table></figure>
<h1 id="资源下载">资源下载</h1>
<h2 id="android生成key">Android生成Key</h2>
<p><a href="https://pan.baidu.com/s/11KC0kjlk0rHle7SSY7l5RA" target="_blank" rel="noopener">下载地址</a></p>
<p>提取码: pwkg</p>
<h2 id="常用字体">常用字体</h2>
<p><a href="https://pan.baidu.com/s/17htdYo6VgNKE6sjbywqKxQ" target="_blank" rel="noopener">下载地址</a></p>
<p>提取码: jqs7</p>
<h2 id="git-key">Git Key</h2>
<p><a href="https://pan.baidu.com/s/1un69NhkHRY0Xxx5_qFisAw" target="_blank" rel="noopener">下载地址</a></p>
<p>提取码: net4</p>
<h2 id="intellijidea认证服务">IntelliJIDEA认证服务</h2>
<p><a href="https://pan.baidu.com/s/1Mzqf2pCrSbI1bS5Manfy3Q" target="_blank" rel="noopener">下载地址</a></p>
<p>提取码: e2ck</p>
<h2 id="openocd">OpenOCD</h2>
<p><a href="https://pan.baidu.com/s/1SZiGeIYuXT00cWc8I3rO1w" target="_blank" rel="noopener">下载地址</a></p>
<p>提取码: gvsy</p>
<h2 id="switchyomega">SwitchyOmega</h2>
<p><a href="https://pan.baidu.com/s/1GmBNDaNxCNGBnhRzrINfUQ" target="_blank" rel="noopener">下载地址</a></p>
<p>提取码: myjc</p>
<h2 id="tomcat">Tomcat</h2>
<p><a href="https://pan.baidu.com/s/1VF4MOIaXNyNvVXtkIcEArg" target="_blank" rel="noopener">下载地址</a></p>
<p>提取码: dtbt</p>
<h2 id="virtualserialport">VirtualSerialPort</h2>
<p><a href="https://pan.baidu.com/s/1Uf7hZjXDGw9rVPGMNevVwQ" target="_blank" rel="noopener">下载地址</a></p>
<p>提取码: db6k</p>
<h2 id="wps缺失字体">WPS缺失字体</h2>
<p><a href="https://pan.baidu.com/s/1ghDNODeuG7oO9Xb3tG8XSQ" target="_blank" rel="noopener">下载地址</a></p>
<p>提取码: bjip</p>
<h2 id="kcptun配置">KCPTUN配置</h2>
<p><a href="https://pan.baidu.com/s/1gV_sWFxGODWA2NioEReCVw" target="_blank" rel="noopener">下载地址</a></p>
<p>提取码: xftw</p>
<h1 id="系统配置">系统配置</h1>
<h2 id="快捷方式">快捷方式</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[Desktop Entry]</span><br><span class="line">Name=Eclipse Java</span><br><span class="line">Comment=Code Editing. Redefined.</span><br><span class="line">GenericName=Text Editor</span><br><span class="line">Exec=/opt/eclipse/java-oxygen/eclipse/eclipse</span><br><span class="line">Icon=/opt/eclipse/java-oxygen/eclipse/icon.xpm</span><br><span class="line">Type=Application</span><br><span class="line">Categories=Utility;TextEditor;Development;IDE;</span><br><span class="line">Keywords=java;</span><br></pre></td></tr></table></figure>
<h2 id="解决windows与ubuntu双系统下的时间问题">解决Windows与Ubuntu双系统下的时间问题</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install ntpdate</span><br><span class="line">sudo ntpdate time.windows.com</span><br><span class="line">sudo hwclock --localtime --systohc</span><br></pre></td></tr></table></figure>
<p>重新进入Windows系统调整下时间就可以</p>
<h2 id="ubuntu系统下安装最新的nvidia驱动">Ubuntu系统下安装最新的Nvidia驱动</h2>
<blockquote>
<ul>
<li>1、到Nvidia<a href="https://www.nvidia.cn/Download/index.aspx?lang=cn" target="_blank" rel="noopener">官网下载最新.run驱动文件</a></li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>2、添加Nvidia最新PPA</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:graphics-drivers/ppa </span><br><span class="line">sudo apt update</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>3、在/etc/modprobe.d/中创建一个blacklist-nouveau.conf文件，并添加nouveau的禁用配置项</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">blacklist nouveau</span><br><span class="line">blacklist lbm-nouveau</span><br><span class="line">options nouveau modeset=0</span><br><span class="line"><span class="built_in">alias</span> nouveau off</span><br><span class="line"><span class="built_in">alias</span> lbm-nouveau off</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>4、重启计算机在登录页面使用<code>Ctrl+Alt+F1</code>进入字符界面后，键入下面指令，如果没有任何信息，表示Nouveau驱动禁用成功</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">lsmod | grep nouveau</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>5、使用.run文件删除当前所有的Nvidia驱动（不要手动删除，用自动方式更安全）</li>
</ul>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo chmod +x NVIDIA-Linux-x86_64-xxx.run</span><br><span class="line">sudo ./NVIDIA-Linux-x86_64-xxx.run –uninstall</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>6、使用PPA指令进行安装，并重启计算机，就可以在<strong>设置-&gt;详细信息</strong>中看到</li>
</ul>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install nvidia-xxx</span><br><span class="line">sudo reboot</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>7、在<strong>设置-&gt;详细信息</strong>中检查是否安装成功，也可以使用下面指令进行查看</li>
</ul>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nvidia-smi</span><br></pre></td></tr></table></figure>
<h2 id="设置应用程序开机自动启动">设置应用程序开机自动启动</h2>
<blockquote>
<ul>
<li>1、复制/etc/init.d目录下的一个.sh文件，并进行修改</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>2、执行下面的指令，把该启动文件添加到启动列表中</li>
</ul>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo update-rc.d huadian.sh defaults</span><br></pre></td></tr></table></figure>
<h2 id="解决ubuntu下zip文件乱码">解决Ubuntu下ZIP文件乱码</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install unar</span><br><span class="line">unar XXX.zip</span><br></pre></td></tr></table></figure>
<h2 id="ubuntu下增加ttyusb权限">Ubuntu下增加ttyUSB权限</h2>
<blockquote>
<ul>
<li>1、创建USB规则文件</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo gedit /etc/udev/rules.d/70-ttyusb.rules</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>2、添加文件内容</li>
</ul>
</blockquote>
<p>KERNEL=="ttyUSB[0-9]*", MODE="0666"</p>
<blockquote>
<ul>
<li>3、修改USB驱动权限</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo chmod 666 /dev/ttyUSB0</span><br></pre></td></tr></table></figure>
<h2 id="双系统grub配置">双系统GRUB配置</h2>
<blockquote>
<ul>
<li>在终端中输入：sudo gedit /etc/default/grub</li>
<li>将文本”GRUB_DEFAULT=0“中的0改成win7系统的序号4</li>
<li>在终端输入：sudo update-grub2</li>
</ul>
</blockquote>
<h2 id="ubuntu下增加keyboard权限">Ubuntu下增加Keyboard权限</h2>
<blockquote>
<ul>
<li>1、创建Keyboard规则文件</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo gedit /etc/udev/rules.d/70-usbkbd.rules</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>2、添加文件内容</li>
</ul>
</blockquote>
<p>KERNEL=="event*", NAME="input/%k", MODE="0666"</p>
<blockquote>
<ul>
<li>3、修改Keyboard驱动权限</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo chmod 666 /dev/input/by-id/usb-Logitech_USB_Receiver-if02-event-kbd</span><br></pre></td></tr></table></figure>
<h2 id="ubuntu下libpython安装错误解决">Ubuntu下libpython安装错误解决</h2>
<p>Ubuntu更新“libpython3.6-stdlib_3.6.5-5~16.04.york1_amd64.deb”时错误导致无法更新</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo dpkg --install --force all /var/cache/apt/archives/libpython3.6-stdlib_3.6.5-5~16.04.york1_amd64.deb</span><br><span class="line"></span><br><span class="line">sudo apt install -f</span><br></pre></td></tr></table></figure>
<h2 id="解决内存占满导致死机的问题">解决内存占满导致死机的问题</h2>
<p>编辑内存清空脚本</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line">sync &amp;&amp; <span class="built_in">echo</span> 1 | sudo tee /proc/sys/vm/drop_caches</span><br><span class="line">sync &amp;&amp; <span class="built_in">echo</span> 2 | sudo tee /proc/sys/vm/drop_caches</span><br><span class="line">sync &amp;&amp; <span class="built_in">echo</span> 3 | sudo tee /proc/sys/vm/drop_caches</span><br></pre></td></tr></table></figure>
<p>利用cron创建定时服务</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo -s</span><br><span class="line"></span><br><span class="line">crontab -e</span><br></pre></td></tr></table></figure>
<p>添加如下内容</p>
<blockquote>
<p>0 * * * * /opt/clearBuffer.sh</p>
</blockquote>
<h2 id="设置树莓派屏幕分辨率">设置树莓派屏幕分辨率</h2>
<p>config.txt设置</p>
<blockquote>
<p>hdmi_group=2</p>
</blockquote>
<blockquote>
<p>hdmi_mode=87</p>
</blockquote>
<blockquote>
<p>hdmi_cvt 800 480 60 6 0 0 0</p>
</blockquote>
<h1 id="开发工具">开发工具</h1>
<h2 id="java">Java</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:webupd8team/java</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install oracle-java8-installer</span><br><span class="line">sudo apt-get install oracle-java8-set-default</span><br></pre></td></tr></table></figure>
<h2 id="apache2">Apache2</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:ondrej/apache2</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt install apache2</span><br></pre></td></tr></table></figure>
<h2 id="php">PHP</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:ondrej/php</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install php7.1 php7.1-dev libapache2-mod-php7.1 libevent-dev php7.1-bz2 php7.1-cgi php7.1-cli php7.1-common php7.1-curl php7.1-gd php7.1-fpm php7.1-imap php7.1-json php7.1-mbstring php7.1-mcrypt php7.1-mysql php7.1-odbc php7.1-snmp php7.1-soap php7.1-sybase php7.1-tidy php7.1-xml php7.1-xmlrpc php7.1-xsl php7.1-zip php-pear</span><br><span class="line">sudo pecl install event</span><br><span class="line">sudo pecl install xdebug</span><br></pre></td></tr></table></figure>
<blockquote>
<p>修改WebServer位置</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/etc/apache2/sites-enabled/000-default.conf</span><br><span class="line"></span><br><span class="line">DocumentRoot /home/taowenyin/MyCode/WebServer</span><br><span class="line"></span><br><span class="line">&lt;Directory /home/taowenyin/MyCode/WebServer&gt;</span><br><span class="line">    Options FollowSymLinks Indexes</span><br><span class="line">    Require all granted</span><br><span class="line">&lt;/Directory&gt;</span><br></pre></td></tr></table></figure>
<h2 id="git">Git</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:git-core/ppa</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt install git</span><br></pre></td></tr></table></figure>
<h2 id="gitkraken">Gitkraken</h2>
<h2 id="node.js">Node.js</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">curl -o- https://raw.githubusercontent.com/creationix/nvm/v0.33.2/install.sh | bash</span><br><span class="line">nvm install node</span><br><span class="line"><span class="comment"># 切换到国内镜像</span></span><br><span class="line">npm config <span class="built_in">set</span> registry https://registry.npm.taobao.org</span><br></pre></td></tr></table></figure>
<h2 id="python的安装与配置">Python的安装与配置</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># sudo add-apt-repository ppa:jonathonf/python-2.7</span></span><br><span class="line"><span class="comment"># sudo add-apt-repository ppa:jonathonf/python-3.7</span></span><br><span class="line"></span><br><span class="line">sudo add-apt-repository ppa:deadsnakes/ppa</span><br><span class="line"></span><br><span class="line">sudo apt-get update</span><br><span class="line"></span><br><span class="line">sudo apt-get install python3.7</span><br><span class="line">sudo apt-get install python2.7</span><br><span class="line"></span><br><span class="line"><span class="comment"># 把Python2、3加入update-alternatives选择列表，最后1表示优先级，数字越大优先级越高</span></span><br><span class="line">sudo update-alternatives --install /usr/bin/python python /usr/bin/python2.7 1</span><br><span class="line">sudo update-alternatives --install /usr/bin/python python /usr/bin/python3.7 2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 选择需要的默认Python版本</span></span><br><span class="line">sudo update-alternatives --config python</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改Python3.5和3.7加入update-alternatives选择列表，最后1表示优先级，数字越大优先级越高</span></span><br><span class="line">sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.7 1</span><br><span class="line">sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.5 2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 选择需要的默认Python3版本</span></span><br><span class="line">sudo update-alternatives --config python3</span><br></pre></td></tr></table></figure>
<h2 id="android环境变量">Android环境变量</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> ANDROID_HOME=/home/taowenyin/Android/Sdk</span><br><span class="line"><span class="built_in">export</span> ANDROID_SDK_HOME=/home/taowenyin/Android</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$ANDROID_HOME</span>/platform-tools:<span class="variable">$ANDROID_HOME</span>/tools</span><br></pre></td></tr></table></figure>
<h2 id="c和32bit库">C++和32bit库</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install lib32ncurses5 lib32z1 libgl1-mesa-dev ttf-wqy-*</span><br></pre></td></tr></table></figure>
<h2 id="intellijidealicenseserver">IntelliJIDEALicenseServer</h2>
<p><a href="http://blog.lanyus.com/" target="_blank" rel="noopener">下载地址和使用说明</a></p>
<blockquote>
<ul>
<li>在终端中输入并添加：sudo gedit /etc/rc.loacl</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/opt/IntelliJIDEALicenseServer/IntelliJIDEALicenseServer_linux_amd64 -p 1017 -u taowenyin</span><br></pre></td></tr></table></figure>
<h2 id="tomcat9安装过程">Tomcat9安装过程</h2>
<blockquote>
<ul>
<li>1、下载Tomcat，并移动到opt目录下</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget http://mirrors.shu.edu.cn/apache/tomcat/tomcat-9/v9.0.13/bin/apache-tomcat-9.0.13.tar.gz</span><br><span class="line">tar -xzf apache-tomcat-9.0.13.tar.gz</span><br><span class="line">sudo chmod 777 /opt/ -R</span><br><span class="line">sudo mv apache-tomcat-9.0.13 /opt/tomcat</span><br><span class="line">sudo nano /opt/tomcat/conf/tomcat-users.xml</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>2、编辑Tomcat管理用户，及管理权限</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo nano /opt/tomcat9/conf/tomcat-users.xml</span><br></pre></td></tr></table></figure>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">tomcat-users</span> <span class="attr">xmlns</span>=<span class="string">"http://tomcat.apache.org/xml"</span></span></span><br><span class="line"><span class="tag">              <span class="attr">xmlns:xsi</span>=<span class="string">"http://www.w3.org/2001/XMLSchema-instance"</span></span></span><br><span class="line"><span class="tag">              <span class="attr">xsi:schemaLocation</span>=<span class="string">"http://tomcat.apache.org/xml tomcat-users.xsd"</span></span></span><br><span class="line"><span class="tag">              <span class="attr">version</span>=<span class="string">"1.0"</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 设置可以访问管理 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">role</span> <span class="attr">rolename</span>=<span class="string">"manager"</span>/&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 设置可以访问管理员 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">role</span> <span class="attr">rolename</span>=<span class="string">"admin"</span>/&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 设置可以访问管理页面 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">role</span> <span class="attr">rolename</span>=<span class="string">"manager-gui"</span>/&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 设置可以访问管理员页面 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">role</span> <span class="attr">rolename</span>=<span class="string">"admin-gui"</span>/&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 设置可以访问管理页面和管理员页面的账户 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">user</span> <span class="attr">username</span>=<span class="string">"admin"</span> <span class="attr">password</span>=<span class="string">"admin"</span> <span class="attr">roles</span>=<span class="string">"manager-gui,admin-gui,manager-script"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">tomcat-users</span>&gt;</span></span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>3、创建系统级的服务访问</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo nano /etc/systemd/system/tomcat.service</span><br></pre></td></tr></table></figure>
<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line">[Unit]</span><br><span class="line"><span class="attribute">Description</span>=Apache Tomcat Web Application Container</span><br><span class="line"><span class="attribute">After</span>=network.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line"><span class="attribute">Type</span>=forking</span><br><span class="line"></span><br><span class="line"><span class="attribute">Environment</span>=JAVA_HOME=/usr/lib/jvm/java-8-oracle</span><br><span class="line"><span class="attribute">Environment</span>=CATALINA_PID=/opt/tomcat/temp/tomcat.pid</span><br><span class="line"><span class="attribute">Environment</span>=CATALINA_HOME=/opt/tomcat</span><br><span class="line"><span class="attribute">Environment</span>=CATALINA_BASE=/opt/tomcat</span><br><span class="line"><span class="attribute">Environment</span>='CATALINA_OPTS=-Xms512M -Xmx1024M -server -XX:+UseParallelGC'</span><br><span class="line"><span class="attribute">Environment</span>='JAVA_OPTS=-Djava.awt.headless=true -Djava.security.egd=file:/dev/./urandom'</span><br><span class="line"></span><br><span class="line"><span class="attribute">ExecStart</span>=/opt/tomcat/bin/startup.sh</span><br><span class="line"><span class="attribute">ExecStop</span>=/opt/tomcat/bin/shutdown.sh</span><br><span class="line"></span><br><span class="line"><span class="attribute">User</span>=[登录账户]</span><br><span class="line"><span class="attribute">Group</span>=[登录账户组]</span><br><span class="line"><span class="attribute">UMask</span>=0007</span><br><span class="line"><span class="attribute">RestartSec</span>=10</span><br><span class="line"><span class="attribute">Restart</span>=always</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line"><span class="attribute">WantedBy</span>=multi-user.target</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo systemctl daemon-reload</span><br><span class="line">sudo systemctl start tomcat.service</span><br><span class="line">sudo systemctl restart tomcat.service</span><br><span class="line">sudo systemctl <span class="built_in">enable</span> tomcat.service</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>4、修改webapps/manager和webapps/host-manager下的context.xml为Tomcat下的各应用添加远程访问</li>
</ul>
</blockquote>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">Context</span> <span class="attr">antiResourceLocking</span>=<span class="string">"false"</span> <span class="attr">privileged</span>=<span class="string">"true"</span> &gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">Valve</span> <span class="attr">className</span>=<span class="string">"org.apache.catalina.valves.RemoteAddrValve"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">allow</span>=<span class="string">"\d+\.\d+\.\d+\.\d+"</span> /&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">Manager</span> <span class="attr">sessionAttributeValueClassNameFilter</span>=<span class="string">"java\.lang\.(?:Boolean|Integer|Long|Number|String)|org\.apache\.catalina\.filters\.CsrfPreventionFilter\$LruCache(?:\$1)?|java\.util\.(?:Linked)?HashMap"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">Context</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h2 id="mysql5.7远程登录设置">MySQL5.7远程登录设置</h2>
<p>安装MySQL</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget https://dev.mysql.com/get/mysql-apt-config_0.8.11-1_all.deb</span><br><span class="line">sudo dpkg -i mysql-apt-config_0.8.11-1_all.deb</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get -y install mysql-server</span><br></pre></td></tr></table></figure>
<p>编辑MySQL配置文件</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo nano /etc/mysql/mysql.conf.d/mysqld.cnf</span><br></pre></td></tr></table></figure>
<p>在下面行的开头加上#，注释掉该行，然后保存退出：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">bind</span>-address = 127.0.0.1</span><br></pre></td></tr></table></figure>
<p>登录MySQL，执行修改权限指令，并重启服务器：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql -u root -p</span><br><span class="line">grant all privileges on *.* to root@<span class="string">"%"</span> identified by <span class="string">"[root账户密码]"</span> with grant option;</span><br><span class="line">flush privileges;</span><br><span class="line"><span class="built_in">exit</span>;</span><br><span class="line">service mysql restart</span><br></pre></td></tr></table></figure>
<h2 id="为angular开启apache的rewrite功能">为Angular开启Apache的Rewrite功能</h2>
<blockquote>
<ul>
<li>1、在网站根目录下创建文件.htaccess，并填入如下内容</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">RewriteEngine On</span><br><span class="line">    <span class="comment"># If an existing asset or directory is requested go to it as it is</span></span><br><span class="line">    RewriteCond %&#123;DOCUMENT_ROOT&#125;%&#123;REQUEST_URI&#125; -f [OR]</span><br><span class="line">    RewriteCond %&#123;DOCUMENT_ROOT&#125;%&#123;REQUEST_URI&#125; -d</span><br><span class="line">    RewriteRule ^ - [L]</span><br><span class="line">    <span class="comment"># If the requested resource doesn't exist, use index.html</span></span><br><span class="line">RewriteRule ^ /index.html</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>2、开启Apache的Rewrite功能</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo a2enmod rewrite</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>3、修改apache2.conf下所有Directory的AllowOverride设置为All</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;Directory /&gt;</span><br><span class="line">     Options Indexes FollowSymLinks</span><br><span class="line">     AllowOverride All</span><br><span class="line">     Require all denied</span><br><span class="line">&lt;/Directory&gt;</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>4、修改/etc/apache2/sites-available下的000-default.conf中的DocumentRoot为/var/www/html/dist/ElectricIOTWeb</li>
</ul>
</blockquote>
<h2 id="解决labview的ni_application_web_server端口与tomcat冲突的问题8080">解决LabView的Ni_Application_Web_Server端口与Tomcat冲突的问题（8080）</h2>
<p>进入下面的网站，点击Web服务器配置，在HTTP端口中修改相应端口号</p>
<figure class="highlight http"><table><tr><td class="code"><pre><span class="line"><span class="attribute">http://localhost:3582</span></span><br></pre></td></tr></table></figure>
<h2 id="解决labview的mdns_responder_service与intellij_idea_java_web调试端口冲突的问题1099">解决LabView的mDNS_Responder_Service与IntelliJ_IDEA_Java_Web调试端口冲突的问题（1099）</h2>
<p>在Debug设置里面修改JMX端口号，避开1099</p>
<h2 id="查看端口被占用的情况">查看端口被占用的情况</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 获取PID号</span><br><span class="line">netstat -aon|findstr &quot;端口号&quot;</span><br><span class="line"></span><br><span class="line"># 根据PID号查询相应的进程</span><br><span class="line">tasklist|findstr &quot;PID号&quot;</span><br></pre></td></tr></table></figure>
<h2 id="gcc-arm-none-eabi和openocd安装">gcc-arm-none-eabi和OpenOCD安装</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:team-gcc-arm-embedded/ppa</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install gcc-arm-none-eabi</span><br><span class="line">sudo apt-get install openocd</span><br></pre></td></tr></table></figure>
<h2 id="clion安装stm32开发环境">CLion安装STM32开发环境</h2>
<p><a href="https://me.csdn.net/PoJiaA123" target="_blank" rel="noopener">参考链接</a></p>
<blockquote>
<ul>
<li>1、下载STM32CubeMx（注：Ubuntu 16.04只能使用V4.27.0）和CLion软件。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>2、安装gcc-arm-none-eabi和OpenOCD。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>3、在CLion中安装“OpenOCD + STM32CubeMX support for ARM embedded development”插件。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>4、在CLion的Build标签下的OpenOCD选项中设置OpenOCD目录（注：Ubuntu中使用命令行安装的OpenOCD不需要设置）。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>5、使用STM32CubeMx生成SW4STM32项目，但不要打开项目。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>6、使用CLion打开并选择项目（注：一切默认），并在工具栏Tool中现在“Update CMake project with STM32CubeMx project”。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>7、期间会跳出“Board config file”，选择一个和自己MCU相同的板子。</li>
</ul>
</blockquote>
<p><strong>改为C++编程</strong></p>
<blockquote>
<ul>
<li>8、修改main.c为main.cpp。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>9、修改CMakeLists.txt中的PROJECT(HelloSTM32ROS C CXX ASM)为PROJECT(HelloSTM32ROS CXX C ASM)（注：默认实际已经支持C++）。</li>
</ul>
</blockquote>
<h2 id="clion使用openocd进行调试stm32f103">CLion使用OpenOCD进行调试STM32F103</h2>
<blockquote>
<ul>
<li>1、在CLion中选择Board Config File为stm3210b_eval.cfg。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>2、在终端中执行下面的指令，使得OpenOCD与STM32链接。</li>
</ul>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">openocd -f /usr/share/openocd/scripts/interface/stlink-v2.cfg -f /usr/share/openocd/scripts/target/stm32f1x.cfg</span><br></pre></td></tr></table></figure>
<h2 id="ubuntu下安装stlink">Ubuntu下安装STLink</h2>
<blockquote>
<ul>
<li>1、准备工作。</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install libusb-1.0-0</span><br><span class="line">sudo apt-get install libgtk-3-dev</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>2、下载并安装STLink</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/texane/stlink.git</span><br><span class="line"><span class="built_in">cd</span> stlink/</span><br><span class="line">make release </span><br><span class="line">make debug </span><br><span class="line"><span class="built_in">cd</span> build/</span><br><span class="line">cmake -DCMAKE_BUILD_TYPE=Debug ..</span><br><span class="line">make</span><br><span class="line"><span class="built_in">cd</span> Release/</span><br><span class="line">sudo make install</span><br><span class="line">sudo ldconfig</span><br></pre></td></tr></table></figure>
<h2 id="解决stm32cubemx生成的代码不能用openocd和st-link调试">解决STM32CubeMX生成的代码不能用OpenOCD和ST-Link调试</h2>
<p><strong>原因：</strong> 由于STM32CubeMX默认生成的代码没有开启调试，使得OpenOCD、Keil等软件都不能进行仿真</p>
<blockquote>
<ul>
<li>1、在SYS的Debug选项中选择“Trace Asynchronous Sw“</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>2、用烧录软件重新烧录（可能的方法，在烧录之前把Boot0和Boot1拉高，烧录完成后再拉低）</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>3、就可以进行调试</li>
</ul>
</blockquote>
<h2 id="opencv3.4.1">OpenCV(&gt;=3.4.1)</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mkdir build</span><br><span class="line"><span class="built_in">cd</span> build</span><br><span class="line">cmake -DCMAKE_BUILD_TYPE=RELEASE -DCMAKE_INSTALL_PREFIX=/usr/<span class="built_in">local</span> -DINSTALL_PYTHON_EXAMPLES=ON -DINSTALL_C_EXAMPLES=ON -DOPENCV_EXTRA_MODULES_PATH=../../opencv_contrib-3.4.2/modules -DPYTHON3_EXECUTABLE=/usr/bin/python3.5 -DPYTHON3_INCLUDE_DIR=/usr/include/python3.5 -DPYTHON3_INCLUDE_DIR2=/usr/include/x86_64-linux-gnu/python3.5m -DPYTHON3_LIBRARY=/usr/lib/x86_64-linux-gnu/libpython3.5m.so -DPYTHON3_NUMPY_INCLUDE_DIRS=/usr/<span class="built_in">local</span>/lib/python3.5/dist-packages/numpy/core/include/ -DPYTHON2_EXECUTABLE=/usr/bin/python3.5 -DPYTHON2_INCLUDE_DIR=/usr/include/python3.5 -DPYTHON2_INCLUDE_DIR2=/usr/include/x86_64-linux-gnu/python3.5m -DPYTHON2_LIBRARY=/usr/lib/x86_64-linux-gnu/libpython3.5m.so -DPYTHON2_NUMPY_INCLUDE_DIRS=/usr/<span class="built_in">local</span>/lib/python3.5/dist-packages/numpy/core/include/ -DBUILD_EXAMPLES=ON ..</span><br><span class="line">make;sudo make install</span><br><span class="line">sudo gedit /etc/ld.so.conf.d/opencv.conf</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>填入/usr/local/lib</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo ldconfig</span><br><span class="line">sudo gedit /etc/bash.bashrc</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>填入PKG_CONFIG_PATH=$PKG_CONFIG_PATH:/usr/local/lib/pkgconfig和export PKG_CONFIG_PATH</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">source</span> /etc/bash.bashrc</span><br><span class="line">sudo updatedb</span><br></pre></td></tr></table></figure>
<h2 id="qt环境配置">QT环境配置</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export QTDIR=/opt/Qt/Qt5.12.1/5.12.1/gcc_64</span><br><span class="line">export PATH=$QTDIR/bin:$PATH</span><br><span class="line">export LD_LIBRARY_PATH=$QTDIR/lib:$LD_LIBRARY_PATH</span><br></pre></td></tr></table></figure>
<h2 id="添加最新的boost库">添加最新的Boost库</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:mhier/libboost-latest</span><br><span class="line">sudo apt-get update</span><br></pre></td></tr></table></figure>
<h2 id="虚拟串口的使用">虚拟串口的使用</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">python3 VirtualSerialPort.py</span><br></pre></td></tr></table></figure>
<h2 id="设置vscode自动格式化">设置VSCode自动格式化</h2>
<blockquote>
<ul>
<li>1、“Format On Save”设为true</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>2、“Detect Indentation”设为false</li>
</ul>
</blockquote>
<h2 id="sw4stm32项目转化为c">SW4STM32项目转化为C++</h2>
<blockquote>
<ul>
<li>1、右键项目，选择“Convert to C++”</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>2、修改main.c为main.cpp</li>
</ul>
</blockquote>
<h2 id="修改github的ssh端口解决22端口超时问题">修改GitHub的SSH端口解决22端口超时问题</h2>
<blockquote>
<ul>
<li>1、在id_rsa和id_rsa.pub创建config文件</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>2、写入如下内容</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Host github.com</span><br><span class="line">User wenyin.tao@163.com</span><br><span class="line">Hostname ssh.github.com</span><br><span class="line">PreferredAuthentications publickey</span><br><span class="line">IdentityFile ~/.ssh/id_rsa</span><br><span class="line">Port 443</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>3、修改GitHub端口</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ssh -T git@github.com</span><br></pre></td></tr></table></figure>
<h1 id="ros开发记录">ROS开发记录</h1>
<h2 id="rosserial_stm32的编译与安装">rosserial_stm32的编译与安装</h2>
<p><a href="https://blog.csdn.net/m0_38089090/article/details/79870815" target="_blank" rel="noopener">参考链接1</a></p>
<p><a href="https://blog.csdn.net/qq_37416258/article/details/84844051" target="_blank" rel="noopener">参考链接2</a></p>
<blockquote>
<ul>
<li>1、安装ROS串口库，指令如下。</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install ros-kinetic-rosserial</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>2、进入官网下载rosserial_stm32（<a href="http://wiki.ros.org/rosserial" target="_blank" rel="noopener">官网</a>）。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>3、把下载后的文件解压并重命名为rosserial_stm32，进入ROS的工作空间。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>4、复制rosserial_stm32文件夹到ROS工作空间的src目录中。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>5、在ROS工作空间中执行catkin_make指令进行编译。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>【可能需要的步骤】执行安装指令，并载入环境安装目录中的环境变量，指令如下。</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">catkin_make install</span><br><span class="line"><span class="built_in">source</span> ROS工作空间/install/setup.bash</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>6、进入STM32CubeMx创建的项目目录，并执行如下指令。</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rosrun rosserial_stm32 make_libraries.py .</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>7、修改Inc目录下的STM32Hardware.h文件，并把头文件中的“stm32f3xx_halxxx.h”修改为“stm32f1xx_halxxx.h”。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>8、修改main.c-&gt;main.cpp。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>9、修改CMakeLists.txt中的PROJECT(HelloSTM32ROS C CXX ASM)为PROJECT(HelloSTM32ROS CXX C ASM)。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>10、修改Inc目录下的STM32Hardware.h文件中的定时器和串口对象。</li>
</ul>
</blockquote>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">extern</span> TIM_HandleTypeDef htim2;</span><br><span class="line"><span class="keyword">extern</span> UART_HandleTypeDef huart1;</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>11、添加roscore文件夹和mainpp.cpp、mainpp.h，并完成相应内容。（注：文件名和文件夹名可以自定义）</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>12、使用该库需要启动rosserial作为数据传递的中介。</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rosrun rosserial_python serial_node.py</span><br></pre></td></tr></table></figure>
<h2 id="ros串口标准库serial的编译与使用">ROS串口标准库serial的编译与使用</h2>
<p><a href="http://wjwwood.io/serial/" target="_blank" rel="noopener">参考链接</a></p>
<blockquote>
<ul>
<li>1、下载serial库</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/wjwwood/serial.git</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>2、编译serial库</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install doxygen</span><br><span class="line"><span class="built_in">cd</span> serial</span><br><span class="line">make</span><br><span class="line">make <span class="built_in">test</span></span><br><span class="line">make docs</span><br><span class="line">make install</span><br><span class="line">make uninstall（卸载）</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>3、复制到ros库</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cp -R /tmp/usr/<span class="built_in">local</span>/include/ /opt/ros/kinetic/</span><br><span class="line">cp -R /tmp/usr/<span class="built_in">local</span>/lib/ /opt/ros/kinetic/</span><br><span class="line">cp -R /tmp/usr/<span class="built_in">local</span>/share/ /opt/ros/kinetic/</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>4、创建serial依赖的功能包</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">catkin_create_pkg &lt;package_name&gt; serial std_msgs roscpp rospy</span><br></pre></td></tr></table></figure>
<h2 id="ros机械臂控制问题集锦">ROS机械臂控制问题集锦</h2>
<p><a href="https://blog.csdn.net/weixin_39579805" target="_blank" rel="noopener">参考链接</a></p>
<h2 id="ros节点调试">ROS节点调试</h2>
<blockquote>
<ul>
<li>1、将下面两行加入到CMakeLists.txt中</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">set</span> (CMAKE_CXX_FLAGS <span class="string">"<span class="variable">$&#123;CMAKE_CXX_FLAGS&#125;</span> -g"</span>)</span><br><span class="line"><span class="built_in">set</span> (CMAKE_VERBOSE_MAKEFILE ON)</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>2、修改launch.json中的program</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="variable">$&#123;workspaceFolder&#125;</span>/devel/lib/&lt;package_name&gt;/&lt;node_name&gt;</span><br></pre></td></tr></table></figure>
<h2 id="ros键盘检测">ROS键盘检测</h2>
<blockquote>
<ul>
<li>1、进入官网下载keyboard_reader（<a href="https://github.com/UTNuclearRoboticsPublic/keyboard_reader" target="_blank" rel="noopener">官网</a>）。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>2、复制keyboard_reader文件夹到ROS工作空间的src目录中。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>3、修改keyboard_reader.cpp文件中的第118行内容。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>4、修改keyboard_reader.cpp文件中的第51行内容，以选择正确的键盘。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>5、在ROS工作空间中执行catkin_make指令进行编译。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>6、keyboard_reader项目的做法主要是创建一个“keyboard”话题，并发送自定义的Key.msg消息</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>7、创建订阅。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>8、修改package.xml，添加keyboard_reader依赖。</li>
</ul>
</blockquote>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">build_depend</span>&gt;</span>keyboard_reader<span class="tag">&lt;/<span class="name">build_depend</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">build_export_depend</span>&gt;</span>keyboard_reader<span class="tag">&lt;/<span class="name">build_export_depend</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">exec_depend</span>&gt;</span>keyboard_reader<span class="tag">&lt;/<span class="name">exec_depend</span>&gt;</span></span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>9、修改CMakeLists.txt，添加keyboard_reader依赖。</li>
</ul>
</blockquote>
<figure class="highlight makefile"><table><tr><td class="code"><pre><span class="line">find_package(catkin REQUIRED COMPONENTS</span><br><span class="line">  roscpp</span><br><span class="line">  rospy</span><br><span class="line">  std_msgs</span><br><span class="line">  keyboard_reader <span class="comment"># 添加的内容</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加MSG依赖</span></span><br><span class="line">add_dependencies(&lt;package_name&gt; &lt;depend_package_name&gt;_generate_messages_cpp)</span><br><span class="line">add_dependencies(ros_keyboard keyboard_reader_generate_messages_cpp)</span><br></pre></td></tr></table></figure>
<h2 id="gazebo模型">Gazebo模型</h2>
<figure class="highlight http"><table><tr><td class="code"><pre><span class="line"><span class="attribute">https://bitbucket.org/osrf/gazebo_models/downloads/</span></span><br></pre></td></tr></table></figure>
<p>下载并解压缩后放到/home/taowenyin/.gazebo/models目录下</p>
]]></content>
      <categories>
        <category>资源</category>
      </categories>
  </entry>
  <entry>
    <title>DeepGBM：A Deep Learning Framework Distilled by GBDT for Online Prediction Tasks</title>
    <url>/deepgbm/</url>
    <content><![CDATA[<h1 id="摘要">0.摘要</h1><p>在现实应用中，在线预测是最基本的任务。典型的在线预测任务具备两个特征，分别是表格式的输入空间和在线数据生成。具体说，就是表格式输入空间表示既有稀疏分类特征也有稠密的数值特征，而在线数据生成则是具有连续生成具有动态分布数据的任务。因此，利用表格输入空间进行有效学习和快速适应在线生成的数据是获取在线预测模型的两个重要挑战。虽然采用GBDT和NN已经在实践中被广泛使用，当时它们都有其自身的缺点。特别地，GBDT很难适应动态生成的在线数据，并且在面对稀疏的分类特征时往往是无效的；另一方面，在面对稠密的数值特征时，NN很难获取令人满意的性能。在这篇文章中，作者提出了一个新的学习框架——DeepGBM，该框架集成了NN和GBDT的优点并提出了两个新的NN组件：</p><a id="more"></a>

<p>1、CatNN：专注于处理稀疏的分类特征。</p>
<p>2、GBDT2NN：通过GBDT对稠密数值特征提取。</p>
<p>在这两个组件的支持下，DeepGBM可以同时利用分类和数值特性，同时保持高效在线更新的能力。</p>
<img src="/deepgbm/deepgbm.png" class title="DeepGBM框架">
<h1 id="介绍">1.介绍</h1>
<p>在线预测在许多实际工业应用中是非常重要的一类任务，如广告搜索中的点击预测、Web搜索中的内容排序、推荐系统中的内容优化、交通规划中的行程时间估计等。</p>
<p>一个典型的在线预测任务中通常具有两个基本特征，即表格式的输入空间和在线数据生成。其中，表格式的输入空间意味着在线预测任务的输入特征可以包括分类和表格数值特征。例如，在广告点击预测任务的特征空间中通常包含广告类别等分类特征空间，以及查询与广告文本相似性等数值特征空间，在线数据生成意味着这些任务的实际数据是以在线方式生成，并且数据分布是实时动态的。例如，在新闻推荐系统中可以实时生成大量的数据，并且在不断涌现的新闻中可以在不同的时间内产生动态的特征分布。</p>
<p>因此，要为在线预测任务寻求有效的基学习器模型，就必须解决两个主要挑战：</p>
<p>1、在表格式输入空间中如何学习有效的模型？</p>
<p>2、如何适配在线数据生成的模型？</p>
<p>目前，两类机器学习模型被广泛应用于在线预测任务，分别时GBDT和NN。不幸的是，这两个方法都不能同时很好的应对上面的两个挑战。换言之，在解决在线预测任务时，GBDT或NN都会产生各自的优缺点。</p>
<img src="/deepgbm/different-models.png" class title="不同模型的比较">
<h2 id="gbdt优缺点">1.1GBDT优缺点</h2>
<p>1、GBDT的主要优势在于它能够有效地处理密集的数值特征。因为，GBDT可以在每次迭代中选取信息增益最大的特征来构建树，因此它可以自动地选择和组合有用的数值特征，以更好地适应训练目标。这就是为什么GBDT在点击预测、Web搜索排名和其他公认的预测任务中展示了它的有效性。当时，GBDT在在线预测任务中有两个主要的弱点。首先，由于GBDT中<strong>学习到的树是不可微的，所以在联机模式下更新GBDT模型是很困难的。</strong>从0开始的不断训练使得GBDT在学习在线预测任务时效率很低，这一弱点阻碍了GBDT对超大规模数据的学习，<strong>因为将大量数据加载到内存中进行学习通常是不切实际的。</strong></p>
<p>2、GBDT无法在稀疏分类特征上进行学习。特别是将分类特征转换成稀疏、高维的one-hot编码后，稀疏特征的信息增益将变得非常小，<code>因为在稀疏特征情况下的具有不平衡增益的分割与不分割几乎相同。</code>因此，GBDT就不能利用稀疏特征来有效的构建决策树。尽管有一些分类编码方式可以直接将分类值转换为稠密的数值，但在对不同分类进行编码转换时，由于很多编码值可能非常相似，当时系统并不知道，所以就会造成与原始数据之间的误差。通过列举可能的二元分类，分类特征也可以直接用于决策树的学习，然而这种方法在分类特征稀疏的情况下，由于每类数据太少，因此统计信息会有误差，往往就会造成训练数据的过拟合。简而言之，虽然GBDT对稠密的数值特征具有较好的学习性，但还是有两个明显缺点，即难以适应在线数据生成和对于稀疏分类特征学习的无效性，这导致GBDT在许多在线预测任务中失败，特别是那些需要在线调整模型和包含许多稀疏分类特征的模型。</p>
<h2 id="nn优缺点">1.2NN优缺点</h2>
<p>神经网络的优势在于通过采用<strong>批处理模式的反向传播算法和公认的对稀疏分类特征具有较好学习性的嵌套结构</strong>对在线任务中的大规模数据学习的有效性。最近的一些研究表明，在如点击预测和推荐系统等在线预测任务中使用神经网络得到了成功的应用。然而，神经网络的主要挑战在于它在学习稠密的数值表格特征方面较弱。全连接神经网络（FCNN）虽然可以直接用于稠密的数值特征，但由于其全连接的模型结构导致了非常复杂的优化超平面方面很容易陷入局部最优，因此常常性能不理想。因此，在许多具有稠密数值表特征的任务中，神经网络性能不比GBDT好。综上所述，尽管神经网络能够有效地处理稀疏的分类特征，并且适应在线数据生成，但是对于稠密的数值表格特征学习仍然很难得到有效的模型。</p>
<p>本文提出的DeepGBM框架主要包含了两个组件：</p>
<p>1、CatNN：接收分类特征的NN结构。</p>
<p>2、GBDT2NN：接收数值特征的NN结构。</p>
<p>为了充分利用GBDT在学习数值特征方面的优势，GBDT2NN把GBDT学习到的内容转化为神经网络的建模过程。具体来说，为了提高知识提取的有效性，GBDT2NN不仅传递了预先训练的GBDT知识，而且还融合了所得到的树结构中隐含的特征重要性和数据划分知识。这样，在达到与GBDT相当的性能的同时，采用神经网络结构的GBDT2NN在面对在线数据生成时，可以很容易地通过不断涌现的数据进行模型更新。</p>
<p>DeepGBM由两个基于神经网络的组件CatNN和GBDT2NN组成，在保持高效在线学习的重要能力的同时，可以在类别特征和数值特征上具有较强的学习能力。本文的三个主要贡献：</p>
<p>1、结合GBDT和NN的优点提出了DeepGBM方法，在保留有效在线更新模型能力的同时，利用分类和数值两种特征，对各种具有表格数据的预测任务进行处理。</p>
<p>2、通过考虑GBDT模型学习树中输入选择、结构和输出，提出了一种将GBDT模型中学到的知识提取为神经网络模型的方法。</p>
<p>3、实验表明DeepGBM是一个成熟的模型，已经能够被用于各类预测任务中，并且具有较好的性能。通过枚举可能的二分类，分类特征也可以直接用于树学习，但是这种方法在分类特征稀疏的情况下会造成训练数据过度拟合。</p>
<h1 id="相关工作">2.相关工作</h1>
<h2 id="在在线预测任务中应用gbdt">2.1在在线预测任务中应用GBDT</h2>
<p>1、在线更新树模型：XGBoost、LightGBM等对在线更新树模型提供了简单的方法，它们保持树结构不变，并通过新数据更新叶输出，但是这种方法的性能离预期的太远。此外，还有人提出了当有新数据时，就重新查找树的分割点，但是由于忽略的历史数据，因此性能也不稳定。</p>
<p>2、树中的分类特征：为了方便决策树能更好的处理它们，通过一些编码方法将稀疏的分类值转换为稠密的数值。CatBoost使用类似数字编码来对分类特征编码，但是它会引起信息损失。</p>
<h2 id="在在线预测任务中应用nn">2.2在在线预测任务中应用NN</h2>
<p>由于FCNN中超平面的优化非常复杂，容易陷入局部最优解，即使采用了归一化和正则化技术，但是对于具有稠密数值特征的数据来说FCNN的性能并不优于GBDT。另一种是把稠密的数值特征进行离散化，形成分类的形式，从而可以更好地处理特征分类的相关工作。但是由于输出仍将需要连接到完全连接层，因此离散化实际上无法提高处理数值特征的效率，并且离散化会增加模型的复杂度，并且由于模型参数的增加而导致过拟合。</p>
<h2 id="组合nn和gbdt">2.3组合NN和GBDT</h2>
<p>1、Tree-link NN（像决策树的神经网络）：像GoogLeNet在一定程度上具有决策树的特点。但这些模型都主要面向机器视觉，而不是具有数据表输入的在线预测任务。还有如NNRF算法，该算法利用树状神经网络和随机特征选择来提高表格数据学习的效果，但是NNRF只使用随机特征组合，而不利用GBDT等训练数据的统计信息。</p>
<p>2、Convert Trees to NN（把决策树转化为神经网络）：这些工作通常效率很低，因为它们使用冗余且非常稀疏的神经网络来表示简单的决策树。当有许多树时，这种转换方案必须构造一个非常宽的神经网络来表示它们，这很难得到实际应用。</p>
<p>3、Combining NN and GBDT（组合NN和GBDT）：Facebook使用叶指数预测作为Logistic回归的输入分类特征。微软用GBDT拟合神经网络的残差。然而，由于GBDT中的在线更新模型问题没有得到解决，因此这些工作无法有效地使用。Facebook在其中也指出了这个问题，因为他们框架中的GBDT模型需要每天重新训练，才能获得良好的在线性能。</p>
<h1 id="deepgbm">3.DeepGBM</h1>
<p>CatNN是处理分类特征的NN结构，GBDT2NN是处理稠密数值特征的NN结构。</p>
<h2 id="处理稀疏分类特征的catnn">3.1处理稀疏分类特征的CatNN</h2>
<p>为了解决在线预测问题，NN已经被广泛的应用于学习分类特征的预测模型，如Wide&amp;Deep、PNN、DeepFM、xDeepFM。由于CATNN的目标是与这些模型的目标相同，因此可以直接把现有的神经网络结构应用在CatNN中。CatNN于前期的研究相似，主要使用嵌套技术（Embedding Technology）把高维的稀疏向量转化为稠密向量。此外，作者还利用前面研究得到的FM组件和Deep组件来学习特征之间的相互作用，并且CatNN并不限制于这两个组件，它可以使用任何的其他类似NN组件。</p>
<p>嵌套技术（Embedding Technology）是通过低维稠密数据来表示高维稀疏向量的一种方法：</p>
<p><span class="math display">\[\begin{equation}
    E_{V_{i}}\left(x_{i}\right)=\text { embedding_lookup}\left(V_{i}, x_{i}\right)
\end{equation}\]</span></p>
<p>其中<span class="math inline">\(x_{i}\)</span>是第<span class="math inline">\(i\)</span>维特征的值，<span class="math inline">\(V_{i}\)</span>存储了第<span class="math inline">\(i\)</span>个特征所有嵌套（embeddings）并且能够被用于反向传播，而<span class="math inline">\(E_{V_{i}}\left(x_{i}\right)\)</span>将返回<span class="math inline">\(x_{i}\)</span>对应的嵌套向量。基于此，作者使用FM组件学习1阶线性函数和2阶特征之间的相互作用：</p>
<p><span class="math display">\[\begin{equation}
    y_{FM}(x)=w_{0}+\langle w, x\rangle+\sum_{i=1}^{d} \sum_{j=i+1}^{d}\left\langle E_{V_{i}}\left(x_{i}\right), E_{V_{j}}\left(x_{j}\right)\right\rangle x_{i} x_{j}
\end{equation}\]</span></p>
<p>其中<span class="math inline">\(d\)</span>表示特征的数量，<span class="math inline">\(w_{0}\)</span>和<span class="math inline">\(w\)</span>是线性部分的参数<span class="math inline">\(\left \langle \cdot , \cdot \right \rangle\)</span>是内积操作。</p>
<p>然后，使用Deep组件来学习高阶特征之间的相互作用。</p>
<p><span class="math display">\[\begin{equation}
    y_{Deep}(x)=\mathcal{N}\left(\left[E_{V_{1}}\left(x_{1}\right)^{T}, E_{V_{2}}\left(x_{2}\right)^{T}, \ldots, E_{V_{d}}\left(x_{d}\right)^{T}\right]^{T} ; \theta\right)
\end{equation}\]</span></p>
<p>其中<span class="math inline">\(\mathcal{N}\left ( x, \theta \right )\)</span>表示输入为<span class="math inline">\(x\)</span>，参数为<span class="math inline">\(\theta\)</span>的多层NN网络模型。通过组合这FM和Deep组件，CatNN网络最终的输出为：</p>
<p><span class="math display">\[\begin{equation}
    y_{Cat}\left ( x \right )=y_{FM}\left ( x \right ) + y_{Deep}\left ( x \right )
\end{equation}\]</span></p>
<h2 id="处理稠密数值空间的gbdt2nn">3.2处理稠密数值空间的GBDT2NN</h2>
<p>本节首先介绍如何把一棵树提取为NN，然后推广这个想法至在GBDT中提取多棵树。</p>
<h3 id="提取一棵树">3.2.1提取一棵树</h3>
<p>之前的树提取工作大多依赖于已学习到的函数来进行模型知识的转换，从而使得新生成的模型与原模型相似。由于树与神经网络的本质不同，因此除了传统的模型提取方法外，树模型中的更多知识也可以被提取并转化为神经网络。例如树中的特征选择和权重，以及树结构中所隐含的数据划分，都是树中的重要知识。</p>
<p><strong>1、树特征选择：</strong>与神经网络相比，树模型的一个特点是不会使用所有的输入特征，因为它的学习会根据统计信息进行选择适合训练目标的特征。因此，我们可以根据树选择的特征来传递这些信息，以提高神经网络模型的学习效率，而不是使用所有的输入特征。形式上可以定义<span class="math inline">\(\mathbb{I}^{t}\)</span>作为树<span class="math inline">\(t\)</span>中已选择特征的指示器，然后只需要使用<span class="math inline">\(x\left [ \mathbb{I}^{t} \right ]\)</span>作为NN的输入。</p>
<p><strong>2、树结构：</strong>决策树的结构其实质上是将数据划分为多个不重叠的区域（叶子），即将数据聚类成不同的类，同一叶中的数据属于同一类。由于神经网络和决策树的本质不同，因此把直接把决策树转化为神金网络并不容易。幸运的是，由于神经网络已被证明能够逼近任何函数，所以我们可以使用神经网络模型来逼近树结构的函数，并实现结构信息的提取。如图2所示，可以使用神经网络来拟合树生成的聚类结果，从而使神经网络逼近决策树的结构函数。形式上将树结构函数<span class="math inline">\(t\)</span>表示为<span class="math inline">\(C^{t}\left ( x \right )\)</span>，该函数返回样本<span class="math inline">\(x\)</span>输出的叶索引，即树生成的聚类结果。然后使用神经网络模型来逼近结构函数<span class="math inline">\(C^{t}\left ( \cdot \right )\)</span>，学习过程可以表示为：</p>
<p><span class="math display">\[\begin{equation}
    \underset{\theta}{min}\frac{1}{n}\sum_{i=1}^{n}{\mathcal{L}}&#39;\left ( \mathcal{N}\left ( x^{i}\left [ \mathbb{I}^t \right ]; \theta \right ),L^{t,i} \right )
\end{equation}\]</span></p>
<p>其中<span class="math inline">\(n\)</span>表示训练样本的个数，<span class="math inline">\(x^{i}\)</span>表示是第<span class="math inline">\(i\)</span>个训练样本，<span class="math inline">\(L^{t,i}\)</span>是第<span class="math inline">\(x^{i}\)</span>样本以one-hot形式表示的叶索引<span class="math inline">\(C^{t}\left ( x^{i} \right )\)</span>，<span class="math inline">\(\mathbb{I}^{t}\)</span>是当前树<span class="math inline">\(t\)</span>中所用到特征的索引，<span class="math inline">\(\theta\)</span>是神经网络模型<span class="math inline">\(\mathcal{N}\)</span>的参数，并且可以通过方向传播进行更新，<span class="math inline">\({\mathcal{L}}&#39;\)</span>是多分类问题的损失函数，如交叉熵。因此，通过学习就可以得到一个网络模型<span class="math inline">\(\mathcal{N}\left ( \cdot ; \theta \right )\)</span>。由于神经网络具有很强的表达能力，学习的神经网络模型会完全逼近决策树的结构函数。</p>
<img src="/deepgbm/nn-approximate-tree.png" class title="神经网络逼近决策树">
<p><strong>3、树输出：</strong>在前面的步骤中学习了从输入到树结构的映射，所以要获取树的输出，只需要知道从树结构到树输出的映射。然而，由于每个叶索引都有对应的叶值，所以该映射并不需要学习。因此，可以将树<span class="math inline">\(t\)</span>的叶值表示为<span class="math inline">\(q^{t}\)</span>，<span class="math inline">\(q_{i}^{t}\)</span>表示第i个叶的叶值。因此就可以通过<span class="math inline">\(p^{t}=L^{t} \times q^{t}\)</span>把<span class="math inline">\(L^{t}\)</span>映射到树输出。</p>
<p>结合上面的方法，从<span class="math inline">\(t\)</span>树获取得到的神经网络的输出可以表示为：</p>
<p><span class="math display">\[\begin{equation}
    y^{t} \left ( x \right ) = \mathcal{N}\left ( x\left [ \mathbb{I}^{t}; \theta \right ] \right ) \times q^{t}
\end{equation}\]</span></p>
<h3 id="多树的提取">3.2.2多树的提取</h3>
<p>要把单树提取的思想推广到GBDT中，一个直接的方法就是把有多少树就有多少神经网络，并且一一对应（<span class="math inline">\(\#NN=\#tree\)</span>）。但是结构提取的目标（<span class="math inline">\(O\left ( \left | L \right | \times \#NN \right )\)</span>）具有高维性，因此该方法的效率很低。为了提高效率，提出了叶嵌套提取法（Leaf Embedding Distillation）和树分组法（Tree Grouping）来分别对<span class="math inline">\(\left | L \right |\)</span>和<span class="math inline">\(\#NN\)</span>进行降维。</p>
<p><strong>1、叶嵌套提取法（Leaf Embedding Distillation）：</strong>如图3所示，作者采用嵌套技术来降低结构提取目标<span class="math inline">\(L\)</span>的维数，同时对信息进行重新训练。更具体地说，由于叶指数和叶值之间存在双射关系，因此使用叶值来学习嵌套，从形式上说，这个学习的过程可以表示为：</p>
<p><span class="math display">\[\begin{equation}
    \underset{w,w_{0},\omega^{t}}{min}\frac{1}{n}\sum_{i=1}^{n}{\mathcal{L}}&#39;&#39;\left ( w^{T}\mathcal{H}\left ( L^{t,i};\omega^{t} \right ) + w_{0}, p^{t,i} \right )
\end{equation}\]</span></p>
<p>其中<span class="math inline">\(H^{t,i} = \mathcal{H}\left ( L^{t,i};\omega^{t} \right )\)</span>是一个具有参数<span class="math inline">\(\omega^{t}\)</span>的单层全连通网络，它将一个one-hot叶索引<span class="math inline">\(L^{t,i}\)</span>转换为稠密嵌套的<span class="math inline">\(H^{t,i}\)</span>，<span class="math inline">\(p^{t,i}\)</span>是样本<span class="math inline">\(x^{i}\)</span>的叶预测值，<span class="math inline">\({\mathcal{L}}&#39;&#39;\)</span>是与树中相同的损失函数，<span class="math inline">\(w\)</span>和<span class="math inline">\(w_{0}\)</span>是将嵌套映射为叶值的参数。这种方法，作者使用了嵌套式的稠密数据作为目标来逼近树结构函数，从而不是使用稀疏高维on-hot数据<span class="math inline">\(L\)</span>，其学习过程可以表示为：</p>
<p><span class="math display">\[\begin{equation}
    \min _{\boldsymbol{\theta}} \frac{1}{n} \sum_{i=1}^{n} \mathcal{L}\left(\mathcal{N}\left(\boldsymbol{x}^{i}\left[\mathbb{I}^{t}\right] ; \boldsymbol{\theta}\right), \boldsymbol{H}^{t, i}\right)
\end{equation}\]</span></p>
<p>其中<span class="math inline">\(\mathcal{L}\)</span>表示拟合<code>嵌套式稠密（Dense Embedding）</code>的类似于L2回归损失函数。由于<span class="math inline">\(H^{t, i}\)</span>的维数要比<span class="math inline">\(L\)</span>小得多，因此嵌套式叶提取在多树提取中更为有效，并且该方法使用更少的神经网络参数。</p>
<img src="/deepgbm/leaf-embedding-distillation.png" class title="叶嵌套提取法">
<p><strong>2、树分组法（Tree Grouping）：</strong>为了降低神经网络的纬度，作者把树进行了分组，并且使用一个神经网络从一组树中提取神经网络模型，但是分组存在两个问题，即如何对树进行分组和如何从一组树中提取神经网络模型，其步骤如下：</p>
<p>（1）对于分组策略已经有了许多方法，例如平均随机分组、按顺序分组、基于重要性或相似性分组等，在本文中，作者使用平均随机分组。假设有<span class="math inline">\(m\)</span>个树，并且把这些数分为<span class="math inline">\(k\)</span>组，那么每组中的树就有<span class="math inline">\(s=\left \lceil \frac{m}{k} \right \rceil\)</span>，并且第<span class="math inline">\(j\)</span>树分组为<span class="math inline">\(\mathbb{T}_{j}\)</span>，它包含了来自GBDT中的随机<span class="math inline">\(s\)</span>个树。</p>
<p>（2）为了从多棵树中提取模型，作者扩展多棵树的叶嵌套提取法。其形式为，假如某个树分组为<span class="math inline">\(\mathbb{T}\)</span>，那么就扩展式子7从多棵树中学习的叶嵌套。</p>
<p><span class="math display">\[\begin{equation}
    \underset{w,w_{0},\omega^{T}}{min}\frac{1}{n}\sum_{i=1}^{n}{\mathcal{L}}&#39;&#39;\left ( w^{T}\mathcal{H}\left ( \parallel_{t \in \mathbb{T}}\left ( L^{t,i} \right );\omega^{\mathbb{T}} \right ) + w_{0}, \underset{t \in \mathbb{T}}{\sum}p^{t,i} \right )
\end{equation}\]</span></p>
<p>其中<span class="math inline">\(\parallel_{t \in \mathbb{T}}\)</span>是一个链接操作，<span class="math inline">\(G^{\mathbb{T},i} = \mathcal{H}\left ( \parallel_{t \in \mathbb{T}}\left ( L^{t,i} \right );\omega^{\mathbb{T}} \right )\)</span>是一个单层的全连接神经网络用于转换多个ont-hot向量，即为了在<span class="math inline">\(\mathbb{T}\)</span>中使用嵌套式稠密数据<span class="math inline">\(G^{\mathbb{T},i}\)</span>，因此连接多个one-hot叶索引向量。然后将新的嵌套作为神经网络模型的提取目标，其学习过程可以表示为：</p>
<p><span class="math display">\[\begin{equation}
    \mathcal{L}^{\mathbb{T}}=\min _{\boldsymbol{\theta}^{\mathbb{T}}} \frac{1}{n} \sum_{i=1}^{n} \mathcal{L}\left(\mathcal{N}\left(x^{i}\left[\mathbb{I}^{\mathbb{T}}\right] ; \boldsymbol{\theta}^{\mathbb{T}}\right), G^{\mathbb{T}, i}\right)
\end{equation}\]</span></p>
<p>其中<span class="math inline">\(\mathbb{I}^{\mathbb{T}}\)</span>是在树分组<span class="math inline">\(\mathbb{T}\)</span>中所使用的特征集。当树分组<span class="math inline">\(\mathbb{T}\)</span>中树的数量很大时，<span class="math inline">\(\mathbb{I}^{\mathbb{T}}\)</span>可能会包含很多的特征，从而影响特征选择，因此，可以只能根据特征重要性在中使用最重要的特征。综上所述，从树分组<span class="math inline">\(\mathbb{T}\)</span>中提取的神经网络最终输出为：</p>
<p><span class="math display">\[\begin{equation}
    y_{\mathbb{T}}(\boldsymbol{x})=\boldsymbol{w}^{T} \times \mathcal{N}\left(\boldsymbol{x}\left[\mathbb{I}^{\mathbb{T}}\right] ; \theta^{\mathbb{T}}\right)+w_{0}
\end{equation}\]</span></p>
<p>并且包含<span class="math inline">\(k\)</span>个树分组的GBDT模型输出为：</p>
<p><span class="math display">\[\begin{equation}
    y_{GBDT2NN}(x)=\sum_{j=1}^{k} y_{\mathbb{T}_{j}}(x)
\end{equation}\]</span></p>
<p>综上所述，由于使用了叶嵌套提取法和树分组法，GBDT2NN可以有效地将GBDT中的许多树提取为一个紧凑的神经网络模型。此外，除了树的输出，树的特征选择和结构信息也被有效地提取到神经网络模型中。</p>
<h2 id="训练deepgbm">3.3训练DeepGBM</h2>
<h3 id="离线状态下端到端训练">3.3.1离线状态下端到端训练</h3>
<p>为了训练DeepGBM，首先需要使用离线数据训练GBDT模型，然后使用式子9得到GBDT中树的嵌套叶。然后就进行端到端的DeepGBM训练。DeepGBM的输出可以表示为：</p>
<p><span class="math display">\[\begin{equation}
    \hat{y}(\boldsymbol{x})=\sigma^{\prime}\left(w_{1} \times y_{GBDT2NN}(\boldsymbol{x})+w_{2} \times y_{Cat}(\boldsymbol{x})\right)
\end{equation}\]</span></p>
<p>其中<span class="math inline">\(w_{1}\)</span>和<span class="math inline">\(w_{2}\)</span>是GBDT2NN和CatNN结合后需要训练的参数，<span class="math inline">\(\sigma^{\prime}\)</span>是转换的输出，类似于二分类的<span class="math inline">\(sigmoid\)</span>函数。最后就可以使用下面这个端到端的损失函数训练：</p>
<p><span class="math display">\[\begin{equation}
    \mathcal{L}_{offline}=\alpha \mathcal{L}^{\prime \prime}(\hat{y}(x), y)+\beta \sum_{j=1}^{k} \mathcal{L}^{\mathbb{T}_{j}}
\end{equation}\]</span></p>
<p>其中<span class="math inline">\(y\)</span>是样本<span class="math inline">\(x\)</span>的训练目标，<span class="math inline">\(\mathcal{L}^{\prime \prime}\)</span>是一个类似于分类任务中的交叉熵损失函数，<span class="math inline">\(\mathcal{L}^{\mathbb{T}}\)</span>是在式子10中的树组<span class="math inline">\(\mathbb{T}\)</span>的嵌套损失函数。<span class="math inline">\(k\)</span>是树组的个数，<span class="math inline">\(\alpha\)</span>和<span class="math inline">\(\beta\)</span>是预先给定的超参数，分别用于控制端到端损失和嵌套损失的强度。</p>
<h3 id="在线模型更新">3.3.2在线模型更新</h3>
<p>由于GBDT模型是离线训练的，因此在在线更新中的嵌套学习中使用GBDT模型会影响在线实时性。因此，在在线模型更新中并不包括更新<span class="math inline">\(\mathcal{L}^{\mathbb{T}}\)</span>，同时在线更新模型的损失函数为：</p>
<p><span class="math display">\[\begin{equation}
    \mathcal{L}_{online}=\mathcal{L}^{\prime \prime}(\hat{y}(\mathbf{x}), y)
\end{equation}\]</span></p>
<p>这个只能用于计算端到端的损失。因此，当DeepGBM在线时，只需要通过<span class="math inline">\(\mathcal{L}_{online}\)</span>来更新模型的新数据，而不需要把GBDT和训练过程进行再次训练。综上所述，DeepGBM能非常有效地执行在线任务，并且它还可以很好地处理稠密数值特征和稀疏分类特征。</p>
]]></content>
      <categories>
        <category>论文</category>
      </categories>
      <tags>
        <tag>集成学习</tag>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>LightMC：A Dynamic And Efficient Multiclass Decomposition Algorithm</title>
    <url>/lightmc/</url>
    <content><![CDATA[<h1 id="摘要">0.摘要</h1><p>解决多分类问题目前的做法有三种，分别是OVA（一对多）、OVO（一对一）和ECOC（纠错输出码），其中OVA和OVO非常简单，但是忽略了类与类之间的关系，而ECOC虽然考虑了类与类之间的关系，但是其性能取决与编码矩阵和解码策略，而要通过找到一个合适的解码策略来发现一个有效的编码矩阵是非常耗时和不确定的。因此，作者提出了一个高效的动态多类分解算法LightMC。该算法不同于以往的固定编码矩阵和解码策略，LightMC采用可微的解码策略，在每次迭代中通过反向传播来训练基础学习器，使其能够动态地优化编码矩阵和解码策略，从而提高多分类的总体精度。</p><a id="more"></a>

<h1 id="介绍">1.介绍</h1>
<p>多分类问题是把一个数据集分为三个或者更多的分类。在一个标准的多分类学习过程中，一共有<span class="math inline">\(K &gt; 2\)</span>个数据类（例如：<span class="math inline">\(Y=\left \{ C_{1}, C_{2}, \cdots , C_{K} \right \}\)</span>），有<span class="math inline">\(n\)</span>个训练集（例如：<span class="math inline">\(S=\left \{ \left \{ x_{1}, y_{1} \right \}, \left \{ x_{2}, y_{2} \right \}, \cdots , \left \{ x_{n}, y_{n} \right \} \right \}\)</span>），每个训练实例都除以<span class="math inline">\(K\)</span>个分类中的一个，并且还有分类器函数<span class="math inline">\(f\left ( x \right )\)</span>，当有一个新实例<span class="math inline">\(x\)</span>时，可以预测该实例属于哪个分类。</p>
<p>目前多分类最热的方法就是多类分解方法，即是将多分类问题变为一系列互不相关的二分类问题，然后把这些二分类问题进行重新组合，从而解决原始的多分类问题。</p>
<p><strong>本篇论文的主要贡献：</strong></p>
<p>1、作者提出的动态分解算法LightMC在输出精度和效率上都优于传统的ECOC。</p>
<p>2、作者定义了一个可微的解码策略，并通过已知的反向传播算法，得到了一种动态优化编码矩阵的有效算法。</p>
<p>3、通过对多个公共大数据集的大量实验分析，证明了新分解算法的有效性和高效性。</p>
<h2 id="ova和ovo">1.1OVA和OVO</h2>
<p>1、OVA：训练K个不同的基学习器，当第<span class="math inline">\(i\)</span>个学习在所有实例中学得的正例就属于<span class="math inline">\(C_{i}\)</span>，否则就不属于<span class="math inline">\(C_{i}\)</span>。</p>
<p>2、OVO：训练<span class="math inline">\(\frac{K \times \left ( K - 1 \right )}{2}\)</span>个不同的基学习器，每一个学习器用于区分两个分类。</p>
<p>OVA和OVO虽然简单，并且也被广泛使用，但是缺陷也很明显，即忽略了类和类之间的关联，例如“Kitty”分类与“猫”的类分的关联性就远大于与“狗”分类的关联性。因此采用OVA和OVA的训练时就不能利用分类之间的关联性降低分类的成本，并提高分类的精度，因此存在低效，且计算复杂度高的问题。同时，当K很大时，且处理大型分类数据时，会导致极高的训练成本。</p>
<h2 id="ecoc">1.2ECOC</h2>
<p>ECOC优于OVA和OVO，一定程度上解决了OVA和OVO中类之间完全独立的问题。ECOC依赖于一个编码矩阵，该编码矩阵为实例定义了一个转换标签，从而把多分类问题变为二分类问题，然后通过去相关和纠错的方式进行重新组合。在ECOC中，通过为不同的分类对生成不同的距离，使得分类之间的相关性能够应用到整个学习过程中。假如“猫”、“Kitty”、“狗”对应的编码矩阵为（1,1,1）、（1,1,-1）和（-1,-1,-1），那么学习模型就可以保证”猫“和”Kitty“之间的距离比”猫“和”Kitty“之间的距离更近。由于编码矩阵的长度和基学习器的数量可以比<span class="math inline">\(K\)</span>小得多，因此基于ECOC的方法可以显著降低OVA和OVO的计算复杂度，特别是类别数量K非常大时。</p>
<p>基于ECOC方法的模型性能高度依赖编码矩阵和解码策略的设计，而随机生成的编码矩阵对结果具有高度的不确定性。为了解决这一问题，在优化编码矩阵方面做了很多工作。由于其多分类问题的复杂性，因此要找到一个优化矩阵是不可能的，甚至次优化矩阵也很难找到，因此无法得到优化矩阵阻碍了ECOC的应用。</p>
<h2 id="lightmc">1.3LightMC</h2>
<p>不同于以往ECOC使用固定的编码矩阵和解码策略，LightMC能够动态的优化编码矩阵和解码策略，并且在每次迭代时对基学习器进行训练，从而达到更为精确的多类分类。为了达到这个目的，LightMC利用了一种可微的解码策略，使得可以通过梯度下降来进行优化，从而保证进一步降低训练损失。LightMC除了提高最终分类精度和获得更利于分类性能的编码矩阵和解码策略外，LightMC还可以显著提高效率，因为它节省了搜索次优编码矩阵的时间。LightMC在模型的训练过程中优化了编码矩阵，不需要再花费太多的时间来初始化编码矩阵，即使是随机编码矩阵也可以得到令人满意的结果。</p>
<h1 id="相关工作">2.相关工作</h1>
<h2 id="ecoc-1">2.1ECOC</h2>
<p>在ECOC中，每一个分类<span class="math inline">\(k\)</span>都有一个编码字<span class="math inline">\(M_{k}\)</span>，该编码字<span class="math inline">\(M_{k}\)</span>表示在第<span class="math inline">\(j\)</span>个基分类器中类<span class="math inline">\(k\)</span>中数据的标签。所有编码字组合一个编码矩阵<span class="math inline">\(M \in \left \{ 1,-1 \right \}^{K \times L}\)</span>，其中L是一个编码字的长度和所有基学习器的数量。所有基学习器的输出为<span class="math inline">\(o=\left \{ o_{1}, o_{2}, \cdots , o_{L} \right \}\)</span>，最后通过解码策略就可以得到多分类的结果。</p>
<p><span class="math display">\[\begin{align}
    \hat{y} = argmin_{k}\left ( t \right ),where\ t_{k}=\frac{1}{2}\sum_{j=1}^{L}\left | M_{kj} - sgn\left ( o_{j} \right ) \right |
\end{align}\]</span></p>
<p>其中<span class="math inline">\(\hat{y}\)</span>表示预测的分类，<span class="math inline">\(sgn\)</span>是一个符号函数，当<span class="math inline">\(o \geq 0\)</span>时<span class="math inline">\(sgn\left ( 0 \right ) = 1\)</span>，否则就为<span class="math inline">\(-1\)</span>。解码策略采用Hamming解码，通过选择Hamming距离最小的类来进行预测，此外，通过Hamming解码还能够纠正基学习期中一定数量的错误。</p>
<p>与传统的分解方法相比，ECOC方法具有许多优点：</p>
<p>1、通过编码矩阵来表示不同类对之间距离，使得能够将类之间的相关性集成到分类模型中，从而进一步提高分类精度。</p>
<p>2、由于代码长度<span class="math inline">\(L\)</span>（即基学习器的数量）可能比类<span class="math inline">\(K\)</span>的数量小得多，因此ECOC方法比OVA和OVO更有效，特别是当<span class="math inline">\(K\)</span>非常大时。</p>
<p>但是ECOC方法的性能主要取决于编码矩阵的设计，并且寻找一个最优的编码矩阵的复杂度是NP-难问题。因此，要找到一个最优的编码矩阵是不可能的，甚至找到一个次优的编码矩阵也是很难，这个问题也制约了ECOC的使用。</p>
<h2 id="其他工作">2.2其他工作</h2>
<p>近年来，许多人试图改进找到合适的编码举证来改进ECOC分解方法。例如，对分类空间进行分层划分，从而以生成相应的编码；通过遗传算法来产生具有较好性能的编码矩阵；通过使用频谱分解找到良好的编码矩阵或通过采用连续值编码矩阵，以及在松弛编码矩阵上的整数约束，可以对ECOC方法有显著改善。虽然这些研究对ECOC的改进具有一定的帮助，但是还是有两个主要的挑战：</p>
<p>1、效率：为了提高多分类的精度，前面的许多工作是设计一个具有<span class="math inline">\(L\)</span>长度的编码矩阵（长度从<span class="math inline">\(K-1\)</span>到<span class="math inline">\(K^{2}\)</span>），这使得基学习器几乎与OVA和OVO所需的模型一样多。这使得ECOC方法在大规模分类问题中非常低效。</p>
<p>2、可扩展性：事实上，以前的ECOC方法研究主要在小规模分类数据下进行，通常只有几十个类和数千个样本组成。</p>
<h1 id="lightmc-1">3.LightMC</h1>
<p>为了解决ECOC方法的两个主要问题，LightMC没有在训练前确定编码矩阵和解码策略，而是试图通过直接优化目标函数，并结合基学习器的训练，动态改善ECOC分解。更具体地说，LightMC引入了一种新的可微解码策略，使得LightMC能够在基学习器训练过程中直接通过梯度下降来优化编码矩阵和解码策略。LightMC有两方面的优势：</p>
<p>1、有效性：LightMC没有将编码矩阵和解码策略的设计与基学习器的训练分离，而是通过联合优化编码矩阵、解码策略和基学习器，从而进一步提高了ECOC分类的精度。</p>
<p>2、高效性：由于在后续的训练中会自动优化编码矩阵，因此LightMC可以显著降低在训练前找到一个好的编码矩阵所需的时间成本。</p>
<img src="/lightmc/lightmc.png" class title="LightMC的算法">
<p>图1是LightMC算法的流程：</p>
<p>1、首先通过现有的ECOC方法初始化编码矩阵。</p>
<p>2、为了充分利用基学习器的训练信息，LightMC采用交替优化算法，将基学习器的学习与编解码优化相结合，即训练基学习器时，编解码策略是固定的，反之则是动态的。</p>
<p>3、这样的联合学习将反复运行，直到整个训练结果收敛。</p>
<p>需要注意的是，不同于在训练前确定编码矩阵，LightMC开发了一个端到端的解决方案，以迭代的方式联合训练基学习器和分解模型。算法1就是LightMC算法的描述，在该算法中有两个步骤：</p>
<p>1、解码训练：用于优化解码策略。</p>
<p>2、编码矩阵训练：用于优化编码举证。</p>
<h2 id="新的可微解码策略softmax解码">3.1新的可微解码策略：Softmax解码</h2>
<p>为了寻找最优的编码与解码策略，就需要对全局目标函数进行直接优化。由于大多数现有的解码策略是不可微的，因此广泛使用的反向传播方法无法用来直接优化全局目标函数。为了解决这个问题，设计一种在保持纠错特性的同时具有可微性的解码策略是至关重要的。</p>
<p>通过研究式1中的解码策略，可以知道有两个不可微的函数，分别是：<span class="math inline">\(sgn\)</span>和<span class="math inline">\(argmin\)</span>。根据文献可以知道，当生成的距离结果为<code>Manhattan（L1）距离</code>时，可以直接删除<span class="math inline">\(sgn\)</span>函数，并且仍然保持纠错特性。此外，<span class="math inline">\(argmin\)</span>函数可以由被广泛使用的<span class="math inline">\(softmax\)</span>函数代替，该函数可以近似于<span class="math inline">\(argmin\)</span>函数，并且具有连续的输出概率和可微的特性，步骤如下：</p>
<p>1、把<span class="math inline">\(argmin\)</span>替换成<span class="math inline">\(argmax\)</span>，同时把<span class="math inline">\(M_{kj}\)</span>的符号颠倒。通过这个方法，当第<span class="math inline">\(j\)</span>个分类器<span class="math inline">\(o_{j}\)</span>输出为<span class="math inline">\(M_{kj}\)</span>时，距离将是最大值而不是最小值。</p>
<p>2、把<span class="math inline">\(argmax\)</span>替换为<span class="math inline">\(softmax\)</span>，则整个解码策略变为：</p>
<p><span class="math display">\[\begin{align}
    \hat{y} = softmax\left ( t \right ),where\ t_{k}=\frac{1}{2}\sum_{j=1}^{L}\left | -M_{kj} - o_{j} \right |
\end{align}\]</span></p>
<p>其中<span class="math inline">\(t_{k}\)</span>表示分类器输出和分类<span class="math inline">\(k\)</span>的编码之间的相似性。尽管在本算法中使用了L1损失，但在文献中提到的L2损失或其他距离函数也同样适用，并且会产生类似的结果。通过上述转换，解码策略将把最高得分分配给最接近输出向量的类，即输出向量具有纠错属性。基于这个原因就可以采用被广泛使用的梯度下降算法来直接优化解码策略。此外，新的解码函数可以重写为单层softmax回归的形式，因为等式2中的距离函数满足：</p>
<p><span class="math display">\[\begin{align}
    \left | -M_{kj} - o_{j} \right | = \left\{
    \begin{matrix}
        1-o_{j}, &amp; M_{kj}=-1 \\ 
        1+o_{j}, &amp; M_{kj}=1
    \end{matrix}\right.
    =1+M_{kj}o_{j}
\end{align}\]</span></p>
<p>因此允许将解码策略重写为：</p>
<p><span class="math display">\[\begin{align}
    \begin{matrix}
        t_{k}=\frac{1}{2}\sum_{j=1}^{L}\left ( 1+M_{kj}o_{j} \right )=\frac{1}{2}\left ( M_{k}^{T}o+L \right )\\
        let\ \theta_{k}=M_{k},b_{k}=L \\ 
        \hat{y}=softmax(t),\ t_{k} = \frac{1}{2}\left ( \theta_{k}^{T}o + b_{k} \right )
    \end{matrix}
\end{align}\]</span></p>
<p>其形式与具有softmax激活的单层线性模型完全相同。因此，就可以使用梯度下降来训练由<span class="math inline">\(M\)</span>初始化的softmax参数<span class="math inline">\(\Theta\)</span>，以减低整体误差。考虑到导数计算的方便性，我们选择了与softmax函数一起使用的多类交叉熵作为损失函数。单个数据点上的总损失可以表示为：</p>
<p><span class="math display">\[\begin{align}
    \begin{matrix}
        J = -\sum_{k=1}^{K}\left ( 1-y_{k} \right )\log\left ( 1-\hat{y_{k}} \right )+y_{k}\log\left (\hat{y_{k}} \right )\\ 
        where\ \Theta\ is\ updated\ by\ \theta_{k}^{t}=\theta_{k}^{t-1}-\gamma_{1}\frac{\partial J}{\partial\theta_{k}^{t-1}}
    \end{matrix}
\end{align}\]</span></p>
<p>其中<span class="math inline">\(\gamma_{1}\)</span>为学习率，<span class="math inline">\(y\)</span>是从原始标签转化而来的one-hot向量。这个优化的过程被称为解码训练（TrainDecoding），如算法2。像普通的梯度下降一样，数据被划分成小批量，用于计算当前梯度，以便进行一次梯度更新。这里也可以使用L1/L2正则化来提高泛化能力。由于梯度下降能够有效性保证了迭代过程中整体损失的减少，从而保证了解码训练（TrainDecoding）是一种有效的改进译码策略的方法。</p>
<h2 id="编码矩阵优化">3.2编码矩阵优化</h2>
<p>如果softmax解码的输入<span class="math inline">\(o\)</span>可以通过反向传播来更新，那么就能够进一步降低总体训练损失。那么对应的更新过程就可以定义为<span class="math inline">\(o^{t}=o^{t-1}-\gamma_{2}\frac{\partial J}{\partial o^{t-1}}\)</span>，其中<span class="math inline">\(\gamma_{2}\)</span>表示学习率。但是，<span class="math inline">\(o\)</span>不能够直接被更新，因为<span class="math inline">\(o\)</span>是基学习器的输出。幸运的是，优化编码矩阵<span class="math inline">\(M\)</span>可以间接地更新<span class="math inline">\(o\)</span>，从而进一步降低整体训练损失。</p>
<p>当<span class="math inline">\(M_{kj}\)</span>被用于训练基学习器<span class="math inline">\(j\)</span>时，它已经决定了数据属于哪一个分类<span class="math inline">\(k\)</span>。假设基学习器能够完美地匹配给定的学习目标，那么对于分类器<span class="math inline">\(j\)</span>来说，任何属于<span class="math inline">\(k\)</span>类的数据该分类类器的输出总是满足<span class="math inline">\(o_{j}^{i}=M_{kj}\)</span>。因此，<span class="math inline">\(M\)</span>的变化会影响基学习器的学习目标，从而进一步影响基学习器的输出<span class="math inline">\(o\)</span>。由于在当基学习器能够完美地匹配给定的学习目标时，梯度<span class="math inline">\(\frac{\partial J}{\partial M_{kj}}\)</span>与<span class="math inline">\(G_{ij} = \frac{\partial J}{\partial o_{j}^{i}}\)</span>相同，此时就可以使用梯度下降的方法来优化<span class="math inline">\(M\)</span>：<span class="math inline">\(M_{kj}^{t}=M_{kj}^{t-1}-\gamma_{2}\frac{\partial J}{\partial M_{kj}^{t-1}}=M_{kj}^{t-1}-\gamma_{2}\frac{\partial J}{\partial G_{kj}^{t-1}}\)</span>。</p>
<p>但是，实践中往往没有完美的基学习器，所以也就不能用上面的方法来优化直接<span class="math inline">\(M\)</span>，因为<span class="math inline">\(\frac{\partial J}{\partial M_{kj}} \neq G_{ij}\)</span>。然而，有许多数据样本可以用于单个类<span class="math inline">\(k\)</span>，即一个<span class="math inline">\(M_{kj}\)</span>对应多个<span class="math inline">\(G_{ij}\)</span>，其中<span class="math inline">\(y_{i}=k\)</span>。因此，可以不必使用不稳定梯度点<span class="math inline">\(G_{ij}\)</span>，而是使用每个类的平均梯度来对<span class="math inline">\(\frac{\partial J}{\partial o_{j}^{i}}\)</span>进行更稳定的估计：</p>
<p><span class="math display">\[\begin{align}
    \frac{\partial J}{\partial M_{kj}} :=\frac{1}{\left | \Omega_{k} \right |}\underset{i \in \Omega_{k}}{\sum G_{ij}},\ where\ \Omega_{k}=\left \{ i \mid y_{i} = k \right \}
\end{align}\]</span></p>
<p>然后利用该估计更新编码矩阵。该优化算法在算法3中已经描述，这一算法与一般的反向传播算法几乎相同，不同的是在执行更新之前使用整个批处理数据来计算平均梯度。实验结果也证明了该方法的有效性，即通过优化全局目标函数，可以对编码矩阵进行一定的细化，以减少损失，提高泛化能力。</p>
<h2 id="讨论">3.3讨论</h2>
<p>1、效率：与现有的ECOC方法相比，LightMC更高效，因为它可以在训练之前使用更少的时间来找到编码矩阵。同时，由于编码矩阵在后续的训练中会被动态地细化，因此它甚至可以产生可比较的性能。此外，LightMC只需要很少的额外优化计算成本，这与单层线性模型的成本相同，并且比神经网络和GBDT等强大的基学习器相比成本要小得多。</p>
<p>2、小批量编码优化方法：由于使用整个批处理进行更新，因此算法3的在内存的使用效率上较低。实际上，切换到较小的批处理（mini-batch）进行更新是很自然的，因为平均梯度也可以在mini-batch中进行计算。</p>
<p>3、分布式编码：在现有的ECOC方法中使用二进制编码。另一方面，LightMC采用分布式编码进行连续优化。显然，分布式编码（也称为<code>嵌入</code>）比二进制编码包含了更多的信息，这使得LightMC能够利用更多的信息来处理类之间的相关性。</p>
<p>4、基学习器的交替训练：在算法1中，如果基学习器不是Boosting学习器，而是如神经网络，那么每次迭代时都可以调用LightMC。而对于Boosting学习器，LightMC从第<span class="math inline">\(i_{s}\)</span>轮开始，每<span class="math inline">\(\frac{1}{\alpha}\)</span>轮调用一次。这是因为在Boosting学习器中有一个学习率<span class="math inline">\(\alpha\)</span>，它会在每次迭代时减少模型的输出。因此，Boosting学习器需要更多的迭代来适应新的训练目标。因此，使用循环次数<span class="math inline">\(i_{s}\)</span>和每<span class="math inline">\(\frac{1}{\alpha}\)</span>次调用一次LightMC可以提高效率，而不需要在每个迭代中调用LightMC。</p>
<p>5、与神经网络中的Softmax层比较：softmax解码的形式确实与神经网络中的softmax层相类似，但是它们确实不同的。</p>
<p>（1）神经网络中的softmax层实际上与OVA分解相同，它不使用编码矩阵来编码类之间的相关性。</p>
<p>（2）它们使用不同的优化方案，优化神经网络中的softmax层主要是为了降低了每个样本的损失，而softmax解码则是优化了每个类的损失（公式6）。softmax在神经网络的实际应用中，很难说哪一种方法更好，甚至最近的一些研究发现，在使用固定的softmax层时，精度几乎相同。</p>
]]></content>
      <categories>
        <category>论文</category>
      </categories>
      <tags>
        <tag>集成学习</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>GBDT理解和推导</title>
    <url>/gbdt/</url>
    <content><![CDATA[<h1 id="gbdt介绍">1.GBDT介绍</h1><p>GBDT（Gradient Boosting Descision Tree，梯度上升决策树），与AdaBoost不同的是，GBDT限定了基模型只能是CART回归树（不是分类树），其次是采用梯度提升的思想来不断更新模型。</p><blockquote>
<p>例：假设一个人的年龄为30岁，现在要使用模型来预测。第一次拟合目标为30，假设预测值为20，两者相差10；第二次拟合目标不再是30，而是对应的前面的残差10，假设这次预测值为8，这次残差为2；此时如果继续预测，那下一次的拟合目标就是2。以此类推，每次拟合的目标都是上一次的残差值。</p>
</blockquote><a id="more"></a>


<p>GBDT其实就是参考这一思想。假设前一个迭代的集成模型为<span class="math inline">\(H_{t-1}\left ( x \right )\)</span>，对应的损失函数为<span class="math inline">\(L\left ( y, H_{t-1}\left ( x \right ) \right )\)</span>，那么在本轮的迭代中就需要找到一个拟合目标，并训练一个模型<span class="math inline">\(h_{t}\left ( x \right )\)</span>，使得本轮的集成模型为<span class="math inline">\(H_{t}\left ( x \right ) = H_{t-1}\left ( x \right ) + h_{t}\left ( x \right )\)</span>，并且对应的损失函数<span class="math inline">\(L\left ( y, H_{t}\left ( x \right ) \right )\)</span>最小。在实际应用中，如果拟合的目标只是残差的话，是有一定的局限性，因为只有损失函数为平方损失函数时，那么拟合目标为残差才能降低损失函数，但是是其他损失函数，那么拟合目标就不是仅仅是残差。</p>
<p>如果损失函数为均方差损失函数<span class="math inline">\(L\left ( y, H_{t}\left ( x \right ) \right ) = \left ( \frac{1}{2} \right ) \ast \left ( y_{i} - H_{t}\left ( x \right ) \right )^{2}\)</span>，那么对<span class="math inline">\(H_{t}\left ( x \right )\)</span>进行求导，就可以得到负梯度：</p>
<p><span class="math display">\[\begin{align}
    -\left [ \frac{\partial L\left ( y, H_{t}\left ( x \right ) \right )}{\partial H_{t}\left ( x \right )} \right ] = \left ( y - H_{t}\left ( x \right ) \right )
\end{align}\]</span></p>
<p>此时，拟合的目标恰好为残差，所以只有平方损失函数的拟合目标为残差。为了把GBDT扩展到更复杂的损失函数，<strong>因此提出了使用上一轮的损失函数的负梯度来作为本轮的拟合目标。</strong></p>
<h1 id="为什么使用负梯度">2.为什么使用负梯度</h1>
<p>为了证明拟合目标的损失函数的负梯度就能够降低损失函数，需要在证明的过程中使用泰勒一阶展开公式，因此泰勒一阶展开的形式为：</p>
<p><span class="math display">\[\begin{align}
    f\left ( x \right ) \approx f\left ( x_{0} \right ) + {f}&#39;\left ( x_{0} \right )\left ( x - x_{0} \right )
\end{align}\]</span></p>
<p>而在<span class="math inline">\(t\)</span>轮的集成模型可以表示为：</p>
<p><span class="math display">\[\begin{align}
    H_{t}\left ( x \right ) = H_{t-1}\left ( x \right ) + h_{t}\left ( x \right )
\end{align}\]</span></p>
<p>因此，第<span class="math inline">\(t\)</span>轮的损失函数<span class="math inline">\(L\left ( y, H_{t}\left ( x \right ) \right )\)</span>在<span class="math inline">\(H_{t-1}\left ( x \right )\)</span>处进行一阶泰勒展开为：</p>
<p><span class="math display">\[\begin{align}
    \begin{matrix}
        L\left ( y, H_{t}\left ( x \right ) \right ) \approx L\left ( y, H_{t-1}\left ( x \right ) \right ) + \frac{\partial L\left ( y, H_{t-1}\left ( x \right ) \right )}{\partial H_{t-1}\left ( x \right )}\left ( H_{t}\left ( x \right ) - H_{t-1}\left ( x \right ) \right )\\ 
        = L\left ( y, H_{t-1}\left ( x \right ) \right ) + \frac{\partial L\left ( y, H_{t-1}\left ( x \right ) \right )}{\partial H_{t-1}\left ( x \right )}h_{t}\left ( x \right )
    \end{matrix}
\end{align}\]</span></p>
<p>如果想要<span class="math inline">\(L\left ( y, H_{t}\left ( x \right ) \right ) \leq L\left ( y, H_{t-1}\left ( x \right ) \right )\)</span>，那么上式中的<span class="math inline">\(\frac{\partial L\left ( y, H_{t-1}\left ( x \right ) \right )}{\partial H_{t-1}\left ( x \right )}h_{t}\left ( x \right ) \leq 0\)</span>，那么就可以使：</p>
<p><span class="math display">\[\begin{align}
    h_{t}\left ( x \right ) = -\frac{\partial L\left ( y, H_{t-1}\left ( x \right ) \right )}{\partial H_{t-1}\left ( x \right )}
\end{align}\]</span></p>
<p>此时</p>
<p><span class="math display">\[\begin{align}
    L\left ( y, H_{t}\left ( x \right ) \right ) = L\left ( y, H_{t-1}\left ( x \right ) \right ) - \left ( \frac{\partial L\left ( y, H_{t-1}\left ( x \right ) \right )}{\partial H_{t-1}\left ( x \right )} \right )^2
\end{align}\]</span></p>
<p>从上面的过程可以知道<span class="math inline">\(h_{t}\left ( x \right ) = -\frac{\partial L\left ( y, H_{t-1}\left ( x \right ) \right )}{\partial H_{t-1}\left ( x \right )}\)</span>，因此如果第<span class="math inline">\(t\)</span>轮的基模型<span class="math inline">\(h_{t}\left ( x \right )\)</span>拟合的目标是<span class="math inline">\(t-1\)</span>轮损失函数的负梯度，那么第<span class="math inline">\(t\)</span>轮的损失函数最小。如果损失函数为平方损失函数是，负梯度也就变为了残差。<strong>GBDT的求解过程可以认为是在函数空间的梯度下降，也就是将带求解的模型函数作为梯度下降中要求解的参数。</strong></p>
<h1 id="gbdt算法流程">3.GBDT算法流程</h1>
<p>假设训练集<span class="math inline">\(D=\left \{ \left ( x^{\left ( 1 \right )}, y^{\left ( 1 \right )} \right ),\left ( x^{\left ( 2 \right )}, y^{\left ( 2 \right )} \right ), \cdots ,\left ( x^{\left ( m \right )}, y^{\left ( m \right )} \right ) \right \}\)</span>，其中样本的个数为<span class="math inline">\(m\)</span>，<span class="math inline">\(y^{\left ( i \right )} \in \left \{ -1, +1 \right \}, i \in \left \{ 1, 2, \cdots , m \right \}\)</span>，并且基模型的个数为<span class="math inline">\(T\)</span>。</p>
<p>1、初始化模型<span class="math inline">\(H_{0}\left ( x \right ) = 0\)</span>。</p>
<p>2、令<span class="math inline">\(t = 1, 2, \cdots , T\)</span>，循环执行下面的语句：</p>
<p>（1）对于每个样本<span class="math inline">\(i = 1, 2, \cdots , m\)</span>，计算<span class="math inline">\(t-1\)</span>轮损失函数的负梯度<span class="math inline">\(\frac{\partial L\left ( y^{\left ( i \right )}, H_{t-1}\left ( x^{\left ( i \right )} \right ) \right )}{\partial H_{t-1}\left ( x^{\left ( i \right )} \right )}\)</span>，并更新<span class="math inline">\(t\)</span>轮样本的标签<span class="math inline">\(y^{\left ( i \right )}\)</span>。</p>
<p>（2）根据更新后的<span class="math inline">\(\left ( x^{\left ( i \right )}, y^{\left ( i \right )} \right ), i = 1, 2, \cdots , m\)</span>训练得到第<span class="math inline">\(t\)</span>轮的基模型（CART回归树）<span class="math inline">\(h_{t}\left ( x \right )\)</span>。</p>
<p>（3）生成第<span class="math inline">\(t\)</span>轮集成模型<span class="math inline">\(H_{t}\left ( x \right ) = H_{t-1}\left ( x \right ) + h_{t}\left ( x \right )\)</span>。</p>
<h1 id="gbdt正则化">4.GBDT正则化</h1>
<p>GBDT正则化常见的有三种方式：</p>
<p>1、类似于AdaBoost中的学习率<span class="math inline">\(v\)</span>，则集成模型变为</p>
<p><span class="math display">\[\begin{align}
    H_{t}\left ( x \right ) = H_{t-1}\left ( x \right ) + vh_{t}\left ( x \right )
\end{align}\]</span></p>
<p><span class="math inline">\(v\)</span>的取值范围是<span class="math inline">\(0 &lt; v \leq 1\)</span>，对于同样的训练集学习效果，<span class="math inline">\(v\)</span>越小就表示需要更多的迭代次数，通常用学习率和迭代最大次数一起来决定算法的拟合效果。</p>
<p>2、采用控制子采样（SubSample）的比例，也就是说只是用一部分样本训练，但是如果采样比例过小的话，在降低方差的同时也会提高偏差，因此不能过低。</p>
<p>3、控制每个决策树（基模型）的复杂度，也就是对决策树进行一些剪枝操作。</p>
<h1 id="参考资料">参考资料</h1>
<p><a href="https://mp.weixin.qq.com/s?__biz=MzU1MjYzNjQwOQ==&amp;mid=2247485694&amp;idx=1&amp;sn=9a07363ba6e3993d6bc8d03cff491bb6&amp;chksm=fbfe5268cc89db7e994c698fad2cc51ff4820d175d3e93def7ce21a820327391d5c1863077e3&amp;mpshare=1&amp;scene=1&amp;srcid=1111PiSfwyQV742U1d1y0N3i&amp;sharer_sharetime=1573563502132&amp;sharer_shareid=98178f12e3ad48ff6ea4030d6be95e39&amp;pass_ticket=RJ0j6JdTxwW3VpGnjiKrEDd90fErtIsefimQSsFiOiGzIaL7WonIFJuRM%2BUUNQ1P#rd" target="_blank" rel="noopener">XGBoost都已经烂大街了，你还不知道GBDT是咋回事？</a></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>集成学习</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>LightGBM：A Highly Efficient Gradient Boosting Decision Tree</title>
    <url>/lightgbm/</url>
    <content><![CDATA[<h1 id="介绍">1.介绍</h1><p>以往的GBDT中，当数据特征纬度较高、数据量较大时，其算法的效率和可扩展性都不是很理想，主要原因是需要针对每一个特征的信息增益来确认其分割特征点。为了解决这个问题，作者提出两个新方法，分别是GOSS（Gradient-based One-Side Sampling，基于梯度的单边采样）和EFB（Exclusive Feature Bundling，互斥特征捆绑）。</p><a id="more"></a>

<p>1、GOSS：<strong>核心思想是把很大一部分小梯度的排除在计算信息增益之外，而主要使用具有大梯度的数据来计算信息增益。</strong>原因是具有大梯度的特征在计算信息增益时扮演了更为主要的角色，因此GOSS可以在小数据量的情况下获取较为精确的信息增益估计。此外，在采样率的情况下，且信息增益的取值范围较大时，这种方法相较于均匀随机采样可以得到得到更精确的增益估计。</p>
<p>2、EFB：<strong>核心思想是把相互排斥的特征（即很少同时取相同值）捆绑在一起，从而减少特征的数量。</strong>要找到精确的互斥特征绑定是一个NP难问题，但是通过贪心算法进行实现很好的近似，从而实现在特征数量最少的同时，也不影响分割点确定的准确性，该问题可以转化为<code>图着色问题</code>。</p>
<h1 id="准备工作">2.准备工作</h1>
<h2 id="gbdt和复杂度分析">2.1GBDT和复杂度分析</h2>
<p>GBDT的主要性能瓶颈在于学习决策树，而学习决策树最耗时的是寻找最佳分割点。目前最受欢迎的寻找最优分割点的算法有两个，分别是pre-sorted算法（预排序算法）和histogram-based算法（基于直方图的算法）。</p>
<p>1、pre-sorted算法（预排序算法）：该算法在预排序的特征值上枚举了所有可能的分割点。该算法能够找到最优分割点，但是在训练速度和内存损耗上效率不高。</p>
<p>2、histogram-based算法（基于直方图的算法）：不同于pre-sorted算法，histogram-based算法不需要在分类后的特征值上寻找分割点，而是将连续的特征值存储到离散的存储箱（bins）中，并在训练过程中利用这些存储箱（bins）构造每个叶节点的特征直方图。<strong>由于histogram-based算法在内存损耗和训练速度上更为高效，因此作者采用这种方法来寻找最佳分割点。</strong>该算法的构建直方图的复杂度是<span class="math inline">\(O\left ( \#data \times \#feature \right )\)</span>，最佳分割点的复杂度是<span class="math inline">\(O\left ( \#bin \times \#feature \right )\)</span>，其中由于bin的数量通常比数据data要小很多，所以分割点寻找的复杂度主要在直方图的构建，那么如果能够降低数据或者特征，那么就可以大大提高GBDT的训练速度。</p>
<h2 id="相关工作">2.2相关工作</h2>
<p>GBDT的实现已经有许多，如XGBoost、pGBRT、Scikit-learn，以及GBM。其中使用R语言实现的Scikit-learn和GBM中，寻找最优分割点就采用的pre-sorted算法，而在pGBRT中则是histogram-based算法，XGBoost同时支持两种算法。由于XGBoost是其他算法中最为出色的，因此作者采用XGBoost作为验证的基准。</p>
<p>为了降低训练数据的大小，通常的做法是对数据进行下采样，把权重小于固定阈值的数据进行过滤。例如，在SGB中每次迭代中使用随机子集来训练弱学习器，或在训练过程中采用动态采样率。然而，除了SGB外，这些工作都是基于AdaBoost，不能够直接用于GBDT，因为GBDT在初始化时数据没有相应的权重。虽然SGB能够支持GBDT，但是它会是GBDT损失经度，所以也不是一个理想的选择。</p>
<p>类似的，为了降低训练集中特征的数量，通常的做法是过滤掉相对较弱的特征，如利用PCA（主成分分析）或者PP（投影追踪）来完成。但是，这些方法主要依赖于特征所包含的冗余信息，而在实践中这些做法并不总是正确，因为所谓特征就是具有独特的性质，而如果直接删除，那么都会在一定程度上影响训练的效果。</p>
<p>在实际应用中，大规模数据往往非常稀疏。采用pre-sorted算法的GBDT通过忽略数据值为0的特征来降低训练成本，但是采用histogram-based算法的GBDT就不能针对稀疏问题进行有效的解决，这是因为histogram-based算法不管数据的特征值是0还是非0都需要检索特征箱中的值，而基于histogram-based算法的GBDT正是利用这一稀疏特性。</p>
<p>所以针对以往研究的局限性，提出了两种新方法，分别是基于梯度的单边采样（GOSS）和互斥特征捆绑（EFB），算法如图1。</p>
<img src="/lightgbm/goss-efb.png" class title="GOSS和EFB的算法">
<center>
图1：GOSS和EFB的算法
</center>
<h1 id="gossgradient-based-one-side-sampling基于梯度的单边采样">3.GOSS（Gradient-based One-Side Sampling，基于梯度的单边采样）</h1>
<p>使用GOSS在进行采样时对小梯度的数据只进行部分采样，因此为了补偿减少数据造成的分布影响，在计算信息增益时，GOSS对小梯度数据引入了一个常数乘法器，具体方法如下：</p>
<p>1、GOSS首先把数据按照它们的梯度绝对值进行排序。</p>
<p>2、选择前面<span class="math inline">\(a \times 100\%\)</span>的数据。</p>
<p>3、在剩余的数据中随机选取<span class="math inline">\(b \times 100\%\)</span>的数据。</p>
<p>4、在计算信息增益时将具有小梯度的数据放大<span class="math inline">\(\frac{1-a}{b}\)</span>倍。</p>
<p>通过GOSS，就可以把模型的更多注意力放在梯度较大、训练不足的数据上，并且不会改变原始数据的分布。</p>
<h1 id="原理分析">3.1原理分析</h1>
<p>GBDT使用决策树来得到一个函数，该函数把输入空间<span class="math inline">\(\mathcal{X}^{\mathcal{S}}\)</span>转化为一个梯度空间<span class="math inline">\(\mathcal{G}\)</span>。假设有一个服从<code>独立同分布</code>的具有<span class="math inline">\(n\)</span>个实例的训练集<span class="math inline">\(\left \{ x_{1}, x_{2}, \cdots , x_{n} \right \}\)</span>，其中<span class="math inline">\(x_{i}\)</span>是一个在<span class="math inline">\(\mathcal{X}^{\mathcal{S}}\)</span>空间中具有<span class="math inline">\(s\)</span>维的向量。在每次梯度增强的迭代中，模型损失函数的负梯度就可以表示为<span class="math inline">\(\left \{ g_{1}, g_{2}, \cdots , g_{n} \right \}\)</span>。决策树通过最大信息增益来把模型分为不同的节点。在GBDT中，信息增益的计算常常通过分裂后的方差来获得。</p>
<p><strong>定义：</strong>假设<span class="math inline">\(O\)</span>是训练集在决策树中的固定节点。此节点上点<span class="math inline">\(d\)</span>处的分割特征<span class="math inline">\(j\)</span>的方差增益可以定义为：</p>
<p><span class="math display">\[\begin{align}
    \begin{matrix}
        V_{j \mid O}\left ( d \right )=\frac{1}{n_{O}}\left ( \frac{\left ( \sum_{\left \{ x_{i} \in O\ :\ x_{xj} \leq d \right \}}g_{i} \right )^{2}}{n_{l \mid O}^{j}\left ( d \right )} + \frac{\left ( \sum_{\left \{ x_{i} \in O\ :\ x_{xj} &gt; d \right \}}g_{i} \right )^{2}}{n_{r \mid O}^{j}\left ( d \right )} \right )\\ 
        where\ n_{O}=\sum I\left [ x_{i} \in O \right ]\\ 
        \ n_{l \mid O}^{j}\left ( d \right )=\sum I\left [ x_{i} \in O\ :\ x_{ij} \leq d \right ]\ and\ n_{r \mid O}^{j}\left ( d \right )=\sum I\left [ x_{i} \in O\ :\ x_{ij} &gt; d \right ]
    \end{matrix}
\end{align}\]</span></p>
<p>上式中的<span class="math inline">\(\frac{\left ( \sum_{\left \{ x_{i} \in O\ :\ x_{xj} \leq d \right \}}g_{i} \right )^{2}}{n_{l \mid O}^{j}\left ( d \right )}\)</span>表示左支的平均信息增益，<span class="math inline">\(\frac{\left ( \sum_{\left \{ x_{i} \in O\ :\ x_{xj} &gt; d \right \}}g_{i} \right )^{2}}{n_{r \mid O}^{j}\left ( d \right )}\)</span>表示右支的平均信息增益，最后求得以节点<span class="math inline">\(d\)</span>和特征点<span class="math inline">\(j\)</span>为分割点的平均方差增益。</p>
<p>对于特征<span class="math inline">\(j\)</span>来说，决策树选择<span class="math inline">\(d_{j}^{\ast}=argmax_{d}V_{j}\left ( d \right )\)</span>节点，并且计算最大增益<span class="math inline">\(V_{j}\left ( d_{j}^{\ast} \right )\)</span>。然后，数据以<span class="math inline">\(d_{j\ast}\)</span>节点处的<span class="math inline">\(j^{\ast}\)</span>个特征进行分割为左右两个子节点。</p>
<p>在GOSS方法中：</p>
<p>1、把训练集中的所有数据按照梯度的绝对值进行降序排列。</p>
<p>2、保留最大梯度的前<span class="math inline">\(a\%\)</span>，并得到一个子集<span class="math inline">\(A\)</span>。</p>
<p>3、对于剩余数据集<span class="math inline">\(A^{c},\ \left ( 1-a \right ) \times 100\%\)</span>中的实例都是具有较小梯度，作者随机从该数据集中采样一个子数据集<span class="math inline">\(B\)</span>，该子数据集的大小为<span class="math inline">\(b \times \left | A^{c} \right |\)</span>。</p>
<p>4、最后作者在<span class="math inline">\(A \cup B\)</span>的集合中求方差增益<span class="math inline">\(\tilde{V_{j}}\left ( d \right )\)</span>来对数据进行分割。</p>
<p><span class="math display">\[\begin{align}
    \begin{matrix}
        \tilde{V_{j}}\left ( d \right ) = \frac{1}{n}\left ( \frac{\left ( \sum_{x_{i} \in A_{l}}g_{i} + \frac{1-a}{b}\sum_{x_{i} \in B_{l}} g_{i} \right )^{2}}{n_{l}^{j}\left ( d \right )} + \frac{\left ( \sum_{x_{i} \in A_{r}}g_{i} + \frac{1-a}{b}\sum_{x_{i} \in B_{r}} g_{i} \right )^{2}}{n_{r}^{j}\left ( d \right )} \right )\\ 
        where \ A_{l} = \left \{ x_{i} \in A\ :\ x_{ij} \leq d \right \} \\ 
        A_{r} = \left \{ x_{i} \in A\ :\ x_{ij} &gt; d \right \} \\ 
        B_{l} = \left \{ x_{i} \in B\ :\ x_{ij} \leq d \right \} \\ 
        B_{r} = \left \{ x_{i} \in B\ :\ x_{ij} &gt; d \right \}
    \end{matrix}
\end{align}\]</span></p>
<p>由于对数据进行了部分采样，因此系数<span class="math inline">\(\frac{1-a}{b}\)</span>的作用是把<span class="math inline">\(B\)</span>的梯度规则化到<span class="math inline">\(A^{c}\)</span>大小。因此GOSS中，作者使用一个较小的数据集来计算得到<span class="math inline">\(\tilde{V_{j}}\left ( d \right )\)</span>，而不是使用所有的实例来精确的获取精确的分割点<span class="math inline">\(V_{j \mid O}\left ( d \right )\)</span>，从而大幅减少计算的成本。同时，结果证明了GOSS不会损失太多的训练精度，并且结果优于随机采样。</p>
<p><strong>定理：</strong>假设GOSS中的近似误差为<span class="math inline">\(\varepsilon \left ( d \right ) = \left | \tilde{V_{j}}\left ( d \right ) - V_{j \mid O}\left ( d \right ) \right |\)</span>，并且左梯度<span class="math inline">\(\bar{g}_{l}^{j}\left ( d \right )=\frac{\sum_{x_{i} \in \left ( A \cup A^{c} \right )_{l}}\left | g_{i} \right |}{n_{l}^{j}\left ( d \right )}\)</span>，右梯度<span class="math inline">\(\bar{g}_{r}^{j}\left ( d \right )=\frac{\sum_{x_{i} \in \left ( A \cup A^{c} \right )_{r}}\left | g_{i} \right |}{n_{r}^{j}\left ( d \right )}\)</span>。当概率至少为<span class="math inline">\(1-\delta\)</span>时，就可以得到</p>
<p><span class="math display">\[\begin{align}
    \begin{matrix}
        \varepsilon \left ( d \right ) \leq C_{a,b}^{2}\ln\frac{1}{\delta} \cdot max\left \{ \frac{1}{n_{l}^{j}\left ( d \right )}, \frac{1}{n_{r}^{j}\left ( d \right )} \right \}+2DC_{a,b}\sqrt{\frac{\ln\frac{1}{\delta}}{n}}\\ 
        where\ C_{a,b}=\frac{1-a}{\sqrt{b}}max_{x_{i} \in A^{c}}\left | g_{i} \right |\ and \ D=max\left ( \bar{g}_{l}^{j}\left ( d \right ),\bar{g}_{r}^{j}\left ( d \right ) \right )
    \end{matrix}
\end{align}\]</span></p>
<p>根据这个定理，可以得到以下推论：</p>
<p>1、GOSS的逼近比为<span class="math inline">\(\mathcal{O}\left ( \frac{1}{n_{l}^{j}\left ( d \right )} + \frac{1}{n_{r}^{j}\left ( d \right )} + \frac{1}{\sqrt{n}}\right )\)</span>。假如决策树分裂的不太平衡时（例如，<span class="math inline">\(n_{l}^{j}\left ( d \right ) \geq \mathcal{O}\left ( \sqrt{n} \right )\)</span>并且<span class="math inline">\(n_{r}^{j}\left ( d \right ) \geq \mathcal{O}\left ( \sqrt{n} \right )\)</span>），近似误差主要有上式中的第二项为主，并且当$n <span class="math inline">\(时\)</span>(  )$趋近于0。这就意味着当数据量很大时，近似值非常准确。</p>
<p>2、在GOSS中，随机采样的一个特殊情况就是<span class="math inline">\(a = 0\)</span>。在大多情况下，GOSS可以在<span class="math inline">\(C_{0, \beta} &gt; C_{a , \beta - a}\)</span>的性能优于随机采样，该条件等价于当<span class="math inline">\(\alpha_{a} = \frac{max_{x_{i} \in A \cup A^{c}}\left | g_{i} \right |}{max_{x_{i} \in A^{c}}\left | g_{i} \right |}\)</span>时，<span class="math inline">\(\frac{\alpha_{a}}{\sqrt{\beta}} &gt; \frac{1-a}{\sqrt{\beta - a}}\)</span>。</p>
<p>作者认为在GOSS中泛化误差为<span class="math inline">\(\varepsilon_{gen}^{GOSS}\left ( d \right )=\left | \tilde{V}_{j}\left ( d \right )-V_{\ast}\left ( d \right ) \right |\)</span>，即通过GOSS对训练集进行抽样后计算得到的方差增益与真实分布计算得到的真实方差增益之间的差距。作者推导得到<span class="math inline">\(\varepsilon_{gen}^{GOSS}\left ( d \right ) \leq \left | \tilde{V}_{j}\left ( d \right )-V_{j}\left ( d \right ) \right | + \left | V_{j}\left ( d \right )-V_{\ast}\left ( d \right ) \right | \triangleq \varepsilon_{GOSS}\left ( d \right ) + \varepsilon_{gen}\left ( d \right )\)</span>，因此如果GOSS近似是正确的，那么GOSS的泛化误差将接近于使用全部数据计算得到的泛化误差。此外，抽样会增加基础学习者的多样性，这可能有助于提高泛化性能</p>
<h1 id="efbexclusive-feature-bundling互斥特征捆绑">4.EFB（Exclusive Feature Bundling，互斥特征捆绑）</h1>
<img src="/lightgbm/gb-mef.png" class title="贪婪绑定和合并互斥特征的算法">
<center>
图2：贪婪绑定和合并互斥特征的算法
</center>
<p>高位数据通常非常稀疏，并且特征空间的稀疏性就给了作者设计一个接近无损降低特征数量的可能。具体来说，在稀疏特征空间中，许多特征是互斥的，即它们从不同时取非零值，因此就可以安全的把这些互斥特征绑定到一个特征上。通过一个精心设计的特征搜索算法，就可以从特征集中构建与单个特征相同的特征直方图。使用这种方法，直方图构建的复杂度就由<span class="math inline">\(O\left ( \#data \times \#feature \right )\)</span>变为<span class="math inline">\(O\left ( \#data \times \#bundle \right )\)</span>，并且<span class="math inline">\(\#bundle \ll \#feature\)</span>。从而在不影响精度的前提下，显著提升GBDT的训练速度。<strong>在该算法中主要有两个问题需要解决，分别是如何确定哪些特性需要被绑定在一起和如何构造绑定的特征包。</strong></p>
<p><strong>定理：</strong>将特征划分为最小数量的互斥包是NP难问题。</p>
<p><strong>证明：</strong>该问题可以归结为图着色问题。由于图着色问题是NP-难问题，因此可以推导出将特征划分为最小数量的互斥包也是NP难问题。</p>
<p>给定任意图着色问题的实例<span class="math inline">\(G=\left ( V,E \right )\)</span>，通过实例<span class="math inline">\(G\)</span>中的每一个特征得到一个关联矩阵<span class="math inline">\(\left | V \right |\)</span>，该矩阵中每一行都是一个特征。很容易看出，在该问题中，一个互斥特征集对应于一组具有相同颜色的顶点，反之亦然。</p>
<p>1、对于<strong>如何确定哪些特性需要被绑定在一起的问题（算法3）</strong>而言：在上面的定理中已经表明寻找最有特征集是一个NP-难问题，这也就表明要在有限时间内找到解是不可能的。为了找到一个较好的近似算法，作者首先将特征绑定定义为图着色问题，如果两个特征不是互斥，那么就以特征为顶点，并为两个特征之间添加一条边。然后使用贪心算法，从而实现图着色到特征邦定集的良好结果（具有恒定的近似比）。此外，还有有相当多的特性，并不是完全互斥，但也很少有同时为非零的情况。如果该算法允许少量的冲突，那么就可以得到更少的特征集，从而进一步提高计算效率。通过简单的计算，随机地改变一小部分特征值最多影响训练的精度为<span class="math inline">\(\mathcal{O}\left ( \left [ \left ( 1-\gamma \right )n \right ]^\frac{-2}{3} \right )\)</span>，其中<span class="math inline">\(\gamma\)</span>是每个个特征集中的最大冲突率（即不是完全互斥的比例）。因此，如果选择一个相对较小的<span class="math inline">\(\gamma\)</span>，就将能够在精度和效率之间达到一个很好的平衡。</p>
<p>基于上面的结论，作者为互斥特征集设计了算法3，具体流程如下：</p>
<p>（1）构建一个带加权边的图，其权值对应于特征之间的总冲突。</p>
<p>（2）按照特征在图中的度进行降序排列。</p>
<p>（3）检查有序列表中的每个特征，并小于一个较小的冲突值时就（由<span class="math inline">\(\gamma\)</span>控制）把其分配给现有的特征集，反之则创建一个新的特征集。</p>
<p>算法3的时间复杂度为<span class="math inline">\(O\left ( \#feature^{2} \right )\)</span>，并且在训练前只运行一次。当特征的数量不是很大时，这种复杂性是可以接受的，但如果有数百万个特征，仍然可能受到影响。为了进一步提高效率，作者提出在不构建图的情况下更为有效的排序策略：</p>
<p><strong>按非零值计数排序，这与按度排序相似，因为非零值越多，冲突的概率就越高。</strong></p>
<p>2、对于<strong>如何构造绑定的特征包的问题（算法4）</strong>而言：这个问题的关键是如何保证在新的特征集中可以识别出原始特征。由于在直方图算法中存储了离散的特征集，而不是连续的特征值，因此我们可以通过<strong>在不同的特征集中放置互斥特征来构造特征集，并且通过向原始特征值添加偏移来完成</strong>。</p>
<p>（1）假设在一个特征集中有两个特征，其中原始特征A的取值为[0, 10)，原始特征B的取值为[0, 20)。</p>
<p>（2）在特征B中添加一个偏移量10，使得特征B变为[10, 30)。</p>
<p>（3）这样就可以安全的把特征A和B进行合并，从而形成一个新的特征集来代替原始的特征A和B，取值范围为[0, 30]。</p>
<p>EFB算法可以将大量的互斥特征捆绑到较少的特征集上，从而有效地避免了对零特征值的不必要计算。此外，也可以为每个特征使用一个表来记录具有非零值的数据，从而忽略零特征值来直方图的算法。通过在表中寻找数据可以把一个特征的直方图构建的计算成本从<span class="math inline">\(O\left ( \#data \right )\)</span>变为<span class="math inline">\(O\left ( \#non-zero-data \right )\)</span>。但是，这种方法需要额外的内存和计算成本来维护整个树生长过程中的每个特征表，并且这个优化并不与EFB相冲突，当特征集稀疏时仍然可以使用EFB。</p>
<h1 id="参考资料">参考资料</h1>
<p><a href="https://zhuanlan.zhihu.com/p/38516467" target="_blank" rel="noopener">LightGBM原理分析</a></p>
<p><a href="https://www.msra.cn/zh-cn/news/features/lightgbm-20170105" target="_blank" rel="noopener">开源 | LightGBM：三天内收获GitHub 1000 星</a></p>
<p><a href="https://www.zhihu.com/question/51644470" target="_blank" rel="noopener">如何看待微软新开源的LightGBM?</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/34698733" target="_blank" rel="noopener">从结构到性能，一文概述XGBoost、Light GBM和CatBoost的同与不同</a></p>
<p><a href="https://blog.csdn.net/qq_24519677/article/details/82811215" target="_blank" rel="noopener">Lightgbm基本原理介绍</a></p>
<p><a href="https://baike.baidu.com/item/%E5%9B%BE%E7%9D%80%E8%89%B2%E9%97%AE%E9%A2%98" target="_blank" rel="noopener">图着色问题</a></p>
]]></content>
      <categories>
        <category>论文</category>
      </categories>
      <tags>
        <tag>集成学习</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>XGBoost理解和推导</title>
    <url>/xgboost/</url>
    <content><![CDATA[<h1 id="xgboost介绍">1.XGBoost介绍</h1><p>XGBoost（Extreme Gradient Boosting，极端梯度提升），它是一种基于决策树的集成机器学习算法，采用了梯度提升框架。<strong>对于非结构化数据（如图像、文本等），采用神经网络往往比其他算法和框架算法要好，但对于中小型结构/表格数据来说，决策树是目前最佳的方式。</strong></p><h1 id="参考资料">参考资料</h1><a id="more"></a>


<p><a href="https://zhuanlan.zhihu.com/p/65805298" target="_blank" rel="noopener">超级女王XGBoost到底“绝”在哪里？</a></p>
<p><a href="https://mp.weixin.qq.com/s?__biz=Mzg2MjI5Mzk0MA==&amp;mid=2247484256&amp;idx=1&amp;sn=9417deb81a2ebffb5e81603425badab7&amp;chksm=ce0b59bbf97cd0ad03e79a87e8f0b8e9479768203046357305a4a5483f2de316580ccff6b7cf&amp;mpshare=1&amp;scene=1&amp;srcid=1109RvBiOxncgoKzk6aNypZR&amp;sharer_sharetime=1573614979764&amp;sharer_shareid=98178f12e3ad48ff6ea4030d6be95e39&amp;pass_ticket=ya68GxkXVWyQO%2Fydmx5oO9LpFScrK%2BdaKW%2BrLyBrVg5f9LKKBiLd1F6CgRa%2BfPeg#rd" target="_blank" rel="noopener">XGBoost超详细推导，终于有人讲明白了！</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/41417638" target="_blank" rel="noopener">【干货合集】通俗理解kaggle比赛大杀器xgboost</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/87885678" target="_blank" rel="noopener">【机器学习】决策树（下）——XGBoost、LightGBM（非常详细）</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/36794802" target="_blank" rel="noopener">XGBoost论文阅读及其原理</a></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>集成学习</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>AdaBoost理解和推导</title>
    <url>/adaboost/</url>
    <content><![CDATA[<h1 id="adaboost介绍">1.AdaBoost介绍</h1><p>AdaBoost（Adaptive Boosting，自适应增强集成算法），即自动通过<strong>前一个基模型对样本预测的误差来调整当前模型中的权重</strong>，然后基于新的权重来训练得到新的模型，以此反复直到训练的基模型个数达到设定的数量，最后把<strong>所有基模型的训练结果</strong>通过组合策略进行集成，从而得到最终模型。基模型的种类很多，可以是决策树，也可以是神经网络等。这个过程对结果的预测主要有两部分的提高：</p><a id="more"></a>

<p>1、训练过程的提高：通过<strong>增加</strong>之前基模型中的分类或回归错误的样本权重，并<strong>降低</strong>了正确分类和回归的样本权重，从而使得<strong>错误分类的样本在下一个基模型中能够被更大的关注。</strong></p>
<p>2、模型结果的提高：在最后的组成策略中采用<strong>增加</strong>分类或回归误差较小的模型权重，<strong>降低</strong>分类或回归误差较大的模型权重，从而<strong>提高正确模型的贡献率。</strong></p>
<p>AdaBoost要解决的问题主要有四个，分别是：</p>
<p>1、如何计算模型的预测的误差率？</p>
<p>2、如何确定基模型的权重</p>
<p>3、如何更新样本的权重</p>
<p>4、如何将多个基模型组合在一起</p>
<p>假设训练集<span class="math inline">\(D=\left \{ \left ( x^{\left ( 1 \right )}, y^{\left ( 1 \right )} \right ),\left ( x^{\left ( 2 \right )}, y^{\left ( 2 \right )} \right ), \cdots ,\left ( x^{\left ( m \right )}, y^{\left ( m \right )} \right ) \right \}\)</span>，其中样本的个数为<span class="math inline">\(m\)</span>，<span class="math inline">\(y^{\left ( i \right )} \in \left \{ -1, +1 \right \}, i \in \left \{ 1, 2, \cdots , m \right \}\)</span>，并且基模型的个数为<span class="math inline">\(T\)</span>，第<span class="math inline">\(t\)</span>个基模型中每个训练样本的权重为<span class="math inline">\(D_{t}=\left ( \omega_{t,1}, \omega_{t,2}, \cdots , \omega_{t,m} \right )\)</span>。</p>
<h1 id="分类问题中adaboost的算法流程">2.分类问题中AdaBoost的算法流程</h1>
<p>1、初始化数据集在<strong>第一个</strong>基模型中的样本权重：<span class="math inline">\(D_{1}=\left ( \omega_{1,1}, \omega_{1,2}, \cdots , \omega_{1,m} \right )\)</span>，其中<span class="math inline">\(\omega_{1,i} = \frac{1}{m}, i = 1, 2, \cdots , m\)</span>。</p>
<p>2、令<span class="math inline">\(t = 1, 2, \cdots, T\)</span>，循环执行下面的语句：</p>
<p>（1）使用带有权重的训练集<span class="math inline">\(D_{t}\)</span>训练一个基模型<span class="math inline">\(h_{t}\)</span>。</p>
<p>（2）计算基模型<span class="math inline">\(h_{t}\)</span>的误差率<span class="math inline">\(\epsilon_{t} = \sum_{i=1}^{m}\omega_{t,i}I\left ( x^{\left ( i \right )} \neq y^{\left ( i \right )} \right )\)</span>，其中<span class="math inline">\(I \in \left \{ 0, 1 \right \}\)</span>为指示函数。</p>
<p>（3）计算基模型<span class="math inline">\(h_{t}\)</span>在最终集成模型中的权重<span class="math inline">\(\alpha_{t}=\frac{1}{2}\ln \frac{1-\epsilon_{t}}{\epsilon_{t}}\)</span>。</p>
<p>（4）更新下一个基模型中样本的权重<span class="math inline">\(D_{t+1}=\left ( \omega_{t+1,1}, \omega_{t+1,2}, \cdots , \omega_{t+1,m} \right )\)</span>，其中</p>
<p><span class="math display">\[\begin{align}
    \omega_{t+1,i} = \frac{\omega_{t,i}}{Z_{t}}exp\left ( -\alpha_{t}y^{\left ( i \right )}h_{t}\left ( x^{\left ( i \right )} \right ) \right ), i = 1, 2, \cdots, m
\end{align}\]</span></p>
<p>其中，<span class="math inline">\(Z_{t} = \sum_{i=1}^{m}\omega_{t,i}exp\left ( -\alpha_{t}y^{\left ( i \right )}h_{t}\left ( x^{\left ( i \right )} \right ) \right )\)</span>是一个归一化因子。</p>
<p>3、组合各基模型，从而生成最终模型<span class="math inline">\(H\left ( x \right ) = \sum_{t=1}^{T}\alpha_{t}h_{t}\left ( x \right )\)</span>。</p>
<p>结合上面的流程就可以回答AdaBoost要解决的四个主要问题：</p>
<p><strong>1、如何计算模型的预测的误差率：</strong>模型的预测误差<span class="math inline">\(\epsilon_{t}\)</span>通过计算带权重的样本错误分类比例来获得样本的预测误差。</p>
<p><strong>2、如何确定基模型的权重：</strong>基模型模型的权重<span class="math inline">\(\alpha_{t}\)</span>随着模型的预测误差<span class="math inline">\(\epsilon_{t}\)</span>的减小而增大，也就是说基模型的预测误差越小，最后对于最终集成模型的权重越大。由于一般模型的准确率都大于0.5，因此<span class="math inline">\(1 - \epsilon_{t} \geq 0.5\)</span>，所以<span class="math inline">\(\frac{1 - \epsilon_{t}}{\epsilon_{t}} \geq 1\)</span>，所以<span class="math inline">\(\alpha_{t}=\frac{1}{2}\ln \frac{1-\epsilon_{t}}{\epsilon_{t}} \geq 0\)</span>。</p>
<p><strong>3、如何更新样本的权重：</strong>由于<span class="math inline">\(y^{\left ( i \right )} \in \left \{ -1, +1 \right \}, h_{t}\left ( x^{\left ( i \right )} \right ) \in \left \{ -1, +1 \right \}\)</span>，因此<span class="math inline">\(\omega_{t+1,i} = \frac{\omega_{t,i}}{Z_{t}}exp\left ( -\alpha_{t}y^{\left ( i \right )}h_{t}\left ( x^{\left ( i \right )} \right ) \right )\)</span>可以等价为：</p>
<p><span class="math display">\[\begin{align}
    \omega_{t+1,i} = \left\{\begin{matrix}
    \frac{\omega_{t,i}exp\left ( \alpha_{t} \right )}{Z_{t}} &amp; h_{t}\left ( x^{\left ( i \right )} \right ) \neq y^{i}\\ 
    \frac{\omega_{t,i}exp\left ( -\alpha_{t} \right )}{Z_{t}} &amp; h_{t}\left ( x^{\left ( i \right )} \right ) = y^{i}
    \end{matrix}\right.
\end{align}\]</span></p>
<p>相比于正确分类的样本，错误分类的样本权重被放大了<span class="math inline">\(exp\left ( 2 \alpha t \right ) = \frac{1 - \epsilon_{t}}{\epsilon_{t}}\)</span>倍，因此也是<span class="math inline">\(\alpha_{t}=\frac{1}{2}\ln \frac{1-\epsilon_{t}}{\epsilon_{t}} \geq 0\)</span>中<span class="math inline">\(\frac{1}{2}\)</span>的由来。</p>
<p><strong>4、如何将多个基模型组合在一起：</strong>将各个模型的权重<span class="math inline">\(\alpha_{t}\)</span>与对应的基模型的预测结果<span class="math inline">\(h_{t}\left ( x \right )\)</span>相乘后求和。</p>
<h1 id="adaboost的损失函数">3.AdaBoost的损失函数</h1>
<p>在AdaBoost中，第<span class="math inline">\(t\)</span>轮计算中，通过带有权重的数据集<span class="math inline">\(D_{t}\)</span>得到一个基模型<span class="math inline">\(h_{t}\left ( x \right )\)</span>和对应的权重<span class="math inline">\(\alpha_{t}\)</span>，这时得到的集成模型为<span class="math inline">\(H_{t}\left ( x \right )\)</span>，其对应的损失函数为：</p>
<p><span class="math display">\[\begin{align}
    L\left ( y, H_{t}\left ( x \right ) \right ) = \sum_{i=1}^{m}exp\left ( -y^{\left ( i \right )}H_{t}\left ( x^{\left ( i \right )} \right ) \right )
\end{align}\]</span></p>
<p>其中<span class="math inline">\(m\)</span>为样本总数，<span class="math inline">\(t\)</span>为当前训练的基模型的个数和第<span class="math inline">\(t\)</span>个基模型。</p>
<p>通过AdaBoost最终模型的表达式<span class="math inline">\(H\left ( x \right ) = \sum_{t=1}^{T}\alpha_{t}h_{t}\left ( x \right )\)</span>可以得到：</p>
<p><span class="math display">\[\begin{align}
    H_{t}\left ( x \right ) = H_{t-1}\left ( x \right ) + \alpha_{t}h_{t}\left ( x \right )
\end{align}\]</span></p>
<p>可以得到损失函数为</p>
<p><span class="math display">\[\begin{align}
    L\left ( y, H_{t}\left ( x \right ) \right ) = \sum_{i=1}^{m}exp\left ( -y^{\left ( i \right )}\left ( H_{t-1}\left ( x^{\left ( i \right )} \right ) + \alpha_{t}h_{t}\left ( x^{\left ( i \right )} \right ) \right ) \right)
\end{align}\]</span></p>
<p>有了损失函数，最小化损失函数即可得到<span class="math inline">\(\alpha_{t}\)</span>和<span class="math inline">\(h_{t}{\left ( x \right )}\)</span>，即</p>
<p><span class="math display">\[\begin{align}
    \left ( \alpha_{t}, h_{t}\left ( x \right ) \right ) = arg \, \underset{\alpha ,h\left ( x \right )}{min} \sum_{i=1}^{m}exp\left ( -y^{\left ( i \right )}\left ( H_{t-1}\left ( x^{\left ( i \right )} \right ) + \alpha_{t}h_{t}\left ( x^{\left ( i \right )} \right ) \right ) \right)
\end{align}\]</span></p>
<h1 id="参考资料">参考资料</h1>
<p><a href="https://mp.weixin.qq.com/s?__biz=MzU1MjYzNjQwOQ==&amp;mid=2247485628&amp;idx=1&amp;sn=d11d3dbfb414614b980d72227549273c&amp;chksm=fbfe522acc89db3cd36f7e875b389e1ce344bf1effe31877c1b0e7dd3111b46ec8ebe4c0eb9a&amp;mpshare=1&amp;scene=1&amp;srcid=1111G2VhASsJxwHgzxgzAh4U&amp;sharer_sharetime=1573470134351&amp;sharer_shareid=98178f12e3ad48ff6ea4030d6be95e39&amp;pass_ticket=MkqdUCw3eQi%2F6alllcv%2FqGjCfFLhCgOjS2xNHsKL6Fi6sWeIqC%2BZz2YVHbV2%2Fo9q#rd" target="_blank" rel="noopener">AdaBoost：一个经典的自适应增强集成算法</a></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>集成学习</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
</search>
