<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>【2016】XGBoost：A Scalable Tree Boosting System</title>
    <url>/xgboost/</url>
    <content><![CDATA[<h1 id="摘要">摘要</h1><p>Tree Boosting是一个高效且被广泛使用的机器学习方法。在本文中，作者描述了一个名叫XGBoost的可扩展的端到端的Tree Boosting系统，该系统能够被数据科学家广泛的应用在许多机器学习的挑战中，从而获取最新的结果。作者为稀疏数据和加权分位点提出了一个全新的稀疏感知算法来近似树的学习。更重要的是，作者提供了有关缓存访问模式、数据压缩和分片的方法，以构建一个可伸缩的Tree Boosting系统。通过结合这些方法，XGBoost可以使用比现有系统少得多的资源来可扩展到有数十亿级实例的数据集。</p><a id="more"></a>

<h1 id="介绍">介绍</h1>
<p>在许多领域中，机器学习和数据驱动方法变得越来越重要。智能垃圾邮件分类器通过从大量垃圾邮件数据和用户反馈中学习来保护我们的电子邮件；广告系统学习将正确的广告与正确的上下文匹配；欺诈检测系统保护银行免受恶意攻击者的攻击；异常事件检测系统帮助实验物理学家找到导致新的物理事件。有两个重要的因素使得这些应用非常成功：使用了有效的统计模型来获取复杂数据之间的依赖关系，以及从大型数据集中根据感兴趣的部分生成模型，从而得到可伸缩的学习系统。</p>
<p>在机器学习方法的实际应用中，Gradient Tree Boosting是一种被广泛应用的技术。Tree Boosting方法已经证明在许多标签分类任务中能提供更好的结果。如使用Tree Boosting进行排序的算法LambdaMART在排序问题上获得了更好的结果。除了作为一个独立的预测工具外，Tree Boosting还被整合到现实世界中，如用于广告点击率预测。最后，Tree Boosting实际上是集成方法的一种，并被用于Netflix大奖等挑战赛中。</p>
<p>在本文中，作者描述了XGBoost方法，该方法是一个可伸缩的Tree Boosting学习系统。本系统是一个开源系统。该系统的影响已经被广泛认识到在一些机器学习和数据挖掘的挑战。以机器学习竞赛网站Kaggle举办的挑战赛为例。在2015年Kaggle博客上发布的29种获奖挑战解决方案中，有17种解决方案使用了XGBoost。在这些解决方案中，有8个单独使用XGBoost训练模型，而其他大多数解决方案则将XGBoost与神经网络集成在一起。为了进行比较，在11个解决方案中使用了第二种最受欢迎的方法，即深度神经网络。在2015年的KDDCup中也见证了该系统的成功，前十名中的每个获胜团队都使用XGBoost。此外，获胜的团队报告说，集成方法的性能仅比配置良好的XGBoost好一小部分。</p>
<p>这些结果表明，作者的系统在大多数问题上能获得更好的结果。这些成功解决方案中的问题包括：商店销售预测、高能物理事件分类、Web文本分类、客户行为预测、运动检测、广告点击率预测、恶意软件分类、产品分类、危险风险预测、大规模在线课程辍学率预测。虽然领域相关的数据分析和特征工程在这些解决方案中扮演着重要的角色，但是XGBoost是学习者的一致选择，这表明了我们的系统和Tree Boosting的影响性和重要性。</p>
<p>XGBoost成功背后最重要的因素是它在所有场景中的可伸缩性。该系统在单台机器上的运行速度是现有流行解决方案的十倍以上，并且在分布式或内存受限的设置中可扩展到数十亿个示例。XGBoost的可伸缩性归因于几个重要的系统和算法优化。这些创新包括：一个处理稀疏数据的全新的树学习算法；理论上合理的加权分位数算法程序能够处理近似树学习中的实例权重。并行和分布式计算使学习更快，从而使模型探索更快。更重要的是，XGBoost利用了核心计算，使数据科学家能够在桌面上处理数亿个示例。最后，将这些技术结合起来，使端到端系统能够以最少的集群资源扩展到更大的数据，这更令人兴奋。本文的主要贡献如下：</p>
<blockquote>
<ul>
<li>作者设计了一个具有高扩展性的端到端的Tree Boosting学习系统。</li>
<li>作者提出了一个理论上合理的加权分位数算法，用于处理近似树学习中的实例权重。</li>
<li>作者引入了一个全新的用于并行树学习的稀疏感知算法。</li>
<li>我们提出了一种有效的缓存感知块结构，用于树外学习。</li>
</ul>
</blockquote>
<p>尽管已有一些有关并行树增强的工作，但是尚未探索诸如核外计算，缓存感知和稀疏感知学习等方向。更重要的是，结合了所有这些方面的端到端系统为真实的案例提供了一种新颖的解决方案。这使数据科学家和研究人员能够构建功能强大的树木增强算法变体。除了这些主要贡献外，作者还在提出正则化学习目标方面做了额外的改进。</p>
<h1 id="tree-boosting简介">Tree Boosting简介</h1>
<p>本章作者将回顾Gradient Tree Boosting算法。推导结果与现有文献中Gradient Boosting推导方法相同。具体而言，二阶方法源自Friedman等人。作者在正则化目标上做了一些微改进，发现在实践中有帮助。</p>
<h2 id="正则化学习目标">正则化学习目标</h2>
<p>给定一个数据集，该数据集有<span class="math inline">\(n\)</span>个实例和<span class="math inline">\(m\)</span>个特征<span class="math inline">\(\mathcal{D}=\left\{\left(\mathbf{x}_{i}, y_{i}\right)\right\}\left(|\mathcal{D}|=n, \mathbf{x}_{i} \in \mathbb{R}^{m}, y_{i} \in \mathbb{R}\right)\)</span>，一个集成树模型（如图1所示）<span class="math inline">\(K\)</span>个加法函数来预测输出。</p>
<p><span class="math display">\[\begin{equation}
\hat{y}_{i}=\phi\left(\mathbf{x}_{i}\right)=\sum_{k=1}^{K} f_{k}\left(\mathbf{x}_{i}\right), \quad f_{k} \in \mathcal{F}
\end{equation}\]</span></p>
<p>其中，<span class="math inline">\(\mathcal{F}=\left\{f(\mathbf{x})=w_{q(\mathbf{x})}\right\}\left(q: \mathbb{R}^{m} \rightarrow T, w \in \mathbb{R}^{T}\right)\)</span>是一个回归树空间（CART）。这里<span class="math inline">\(q\)</span>表示实例映射到对应树的叶索引。<span class="math inline">\(T\)</span>是每个棵树的叶节点数量。每个<span class="math inline">\(f_{k}\)</span>对应一个独立的树结构<span class="math inline">\(q\)</span>和权重为<span class="math inline">\(w\)</span>的叶节点。与决策树不同，每个回归树在每个叶节点上都包含连续分数，作者使用<span class="math inline">\(w_{i}\)</span>来表示第<span class="math inline">\(i\)</span>个叶节点的分数。给定一个实例，作者使用树（通过<span class="math inline">\(q\)</span>给定）中决策规则把一个实例分配到对应的叶节点上，并且通过对应的叶节点（通过<span class="math inline">\(w\)</span>给定）求和来计算最终的预测值。要在学习模型中使用这些算法集，作者就最小化了以下正则化目标。</p>
<p><span class="math display">\[\begin{equation}
\begin{matrix}
\mathcal{L}(\phi)=\sum_{i} l\left(\hat{y}_{i}, y_{i}\right)+\sum_{k} \Omega\left(f_{k}\right)\\ 
\text { where } \Omega(f)=\gamma T+\frac{1}{2} \lambda\|w\|^{2}
\end{matrix}
\end{equation}\]</span></p>
<p>这里的<span class="math inline">\(l\)</span>是一个可微的突损失函数，用于度量预测的<span class="math inline">\(\hat{y}_{i}\)</span>和目标<span class="math inline">\(y_{i}\)</span>的区别。第二个表达式中的<span class="math inline">\(\Omega\)</span>是模型复杂度的惩罚项（例如回归树函数）。附加的正则项有助于平滑最终学习到的权重，以避免过度拟合。直观地说，正则化目标是为了选择一个简单预测函数的模型。在正则贪婪森林（RGF）模型中就使用了类似的正则化技术。我们的目标和相应的学习算法比RGF简单且易于并行化。当正则化参数为零时，目标回归到传统的梯度树增强。</p>
<img src="/xgboost/tree-ensemble-model.png" class title="集成树模型">
<p>图1：集成树模型。实例的最终预测是所有所有树预测结果的和</p>
<h2 id="gradient-tree-boosting">Gradient Tree Boosting</h2>
<p>公式2的集成树模型包含使用函数作为参数，并且在欧式空间中不能使用传统的优化方法进行优化。相反，该模型是以加法的方式训练的。从形式上来说，让<span class="math inline">\(\hat{y}_{i}^{(t)}\)</span>是第<span class="math inline">\(i\)</span>个实例在<span class="math inline">\(t\)</span>次迭代中预测，并且需要加上<span class="math inline">\(f_{t}\)</span>从而最小化下面的目标函数。</p>
<p><span class="math display">\[\mathcal{L}^{(t)}=\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}^{(t-1)}+f_{t}\left(\mathbf{x}_{i}\right)\right)+\Omega\left(f_{t}\right)\]</span></p>
<p>因此加上<span class="math inline">\(f_{t}\)</span>修改公式（2）模型。通常二阶近似可用于快速优化目标。</p>
<p><span class="math display">\[\mathcal{L}^{(t)} \simeq \sum_{i=1}^{n}\left[l\left(y_{i}, \hat{y}^{(t-1)}\right)+g_{i} f_{t}\left(\mathbf{x}_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(\mathbf{x}_{i}\right)\right]+\Omega\left(f_{t}\right)\]</span></p>
<p>其中<span class="math inline">\(g_{i}=\partial_{\hat{y}^{(t-1)}} l\left(y_{i}, \hat{y}^{(t-1)}\right)\)</span>和<span class="math inline">\(h_{i}=\partial_{\hat{y}}^{2}(t-1) l\left(y_{i}, \hat{y}^{(t-1)}\right)\)</span>是损失函数中的一阶和二阶统计。在<span class="math inline">\(t\)</span>步，作者移除了常数项从而简化了目标函数。</p>
<p><span class="math display">\[\begin{equation}
\tilde{\mathcal{L}}^{(t)}=\sum_{i=1}^{n}\left[g_{i} f_{t}\left(\mathbf{x}_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(\mathbf{x}_{i}\right)\right]+\Omega\left(f_{t}\right)
\end{equation}\]</span></p>
<p>定义<span class="math inline">\(I_{j}=\left\{i | q\left(\mathbf{x}_{i}\right)=j\right\}\)</span>为叶节点<span class="math inline">\(j\)</span>上的实例集。作者通过展开<span class="math inline">\(\Omega\)</span>重写公式三得到。</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\tilde{\mathcal{L}}^{(t)} &amp;=\sum_{i=1}^{n}\left[g_{i} f_{t}\left(\mathbf{x}_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(\mathbf{x}_{i}\right)\right]+\gamma T+\frac{1}{2} \lambda \sum_{j=1}^{T} w_{j}^{2} \\
&amp;=\sum_{j=1}^{T}\left[\left(\sum_{i \in I_{j}} g_{i}\right) w_{j}+\frac{1}{2}\left(\sum_{i \in I_{j}} h_{i}+\lambda\right) w_{j}^{2}\right]+\gamma T
\end{aligned}
\end{equation}\]</span></p>
<p>对于固定结构<span class="math inline">\(q(\mathbf{x})\)</span>，作者通过下式计算得到在叶节点<span class="math inline">\(j\)</span>上的最有权重<span class="math inline">\(w_{j}^{*}\)</span>：</p>
<p><span class="math display">\[\begin{equation}
w_{j}^{*}=-\frac{\sum_{i \in I_{j}} g_{i}}{\sum_{i \in I_{j}} h_{i}+\lambda}
\end{equation}\]</span></p>
<p>并且通过下式计算对应的最优解</p>
<p><span class="math display">\[\begin{equation}
\tilde{\mathcal{L}}^{(t)}(q)=-\frac{1}{2} \sum_{j=1}^{T} \frac{\left(\sum_{i \in I_{j}} g_{i}\right)^{2}}{\sum_{i \in I_{j}} h_{i}+\lambda}+\gamma T
\end{equation}\]</span></p>
<p>公式6是评价评分函数，可用来衡量树结构<span class="math inline">\(q\)</span>的质量。该分数类似于评估决策树的不纯度情况，不同的是它是通过更广泛的目标函数得出。图2展示了这个分数是如何计算的。</p>
<img src="/xgboost/structure-score-calculation.png" class title="树结构不纯度分数计算">
<p>图2：树结构分数。我们只需要对每一片叶子的梯度和二阶梯度进行统计，然后应用评分公式得到质量分数。</p>
<p>通常不可能枚举所有可能的树结构<span class="math inline">\(q\)</span>。取而代之的是一个贪心算法，它从一个叶子开始，迭代地向树中添加分支。假设<span class="math inline">\(I_{L}\)</span>和<span class="math inline">\(I_{R}\)</span>是节点左右两边的数据集。设<span class="math inline">\(I=I_{L} \cup I_{R}\)</span>，那么分割后的损失减少量由下是给出：</p>
<p><span class="math display">\[\begin{equation}
\mathcal{L}_{s p l i t}=\frac{1}{2}\left[\frac{\left(\sum_{i \in I_{L}} g_{i}\right)^{2}}{\sum_{i \in I_{L}} h_{i}+\lambda}+\frac{\left(\sum_{i \in I_{R}} g_{i}\right)^{2}}{\sum_{i \in I_{R}} h_{i}+\lambda}-\frac{\left(\sum_{i \in I} g_{i}\right)^{2}}{\sum_{i \in I} h_{i}+\lambda}\right]-\gamma
\end{equation}\]</span></p>
<p>该公式通常在实践中用于评估拆分后的候选对象。</p>
<h2 id="收缩率和列特征采样">收缩率和列（特征）采样</h2>
<p>除了2.1节的正则化目标函数外，还有两个额外的技术来进一步防止过拟合。第一个技术是Friedman的收缩率。在每一步Tree Boosting生长后，收缩率将新增加的权重按因子<span class="math inline">\(\eta\)</span>进行缩放。类似于随机优化中的学习率，收缩减少了每棵树的影响，并为以后的树留出了改进模型的空间。第二个技术是列（特征）采样。这个技术在随即森林中使用，它在用于梯度增强的商业软件TreeNet中实现，但在现有的开源软件包中未实现。根据用户反馈，与传统的行采样（也支持）相比，使用列采样可以防止过度拟合。列采样的使用也加快了稍后描述的并行算法的计算。</p>
<h1 id="分割查找算法">分割查找算法</h1>
<h2 id="basic-exact-greedy-algorithm基本精确的贪心算法">Basic Exact Greedy Algorithm（基本精确的贪心算法）</h2>
<p>树学习中的一个关键问题是如何找到公式7所表示的最佳分割点。为了找到最佳分割点，分割算法会在所有特征上枚举所有可能的分割点。作者称之为精确贪心算法。大多数现有的单机Tree Boosting实现，如scikit-learn、GBM，以及单机版的XGBoost都支持精确贪心算法。精确贪心算法如算法1所示。计算上要求枚举连续特征的所有可能分割。为了实现这一点，算法必须首先根据特征值对数据进行排序，然后访问排序后的数据，从而逐渐增加公式7中树结构得分的梯度统计。</p>
<img src="/xgboost/exact-greedy-algorithm.png" class title="分割点寻找的精准贪心算法">
<h2 id="近似算法">近似算法</h2>
<p>精准贪心算法非常强大，因为它枚举了所有可能的分裂点。然而，当数据不能完全装入内存时，就不可能有效地这样做。同样的问题也出现在分布式设置中。为了支持这两种设置中的有效梯度树提升，需要一种近似算法。</p>
<p>在算法2中，作者总结了一个近似的框架，类似于在过去的文献中提出的想法。综上所述，该算法首先根据特征分布的比例提出候选分割点（具体标准将在3.3节给出）。该算法将连续特征映射到由这些候选点分割的组成的内存中，对统计信息进行聚合，并根据聚合的统计信息在多个方案中找到最优解。</p>
<img src="/xgboost/approximate-algorithm-split-finding.png" class title="分割点寻找的近似算法">
<p>根据需求的不同，有两种算法改进，一种是全局变量在树构造的初始阶段提出所有候选拆分，并在所有级别使用相同的拆分查找建议，另一种是局部变量在每次拆分后重新提出。全局方法比局部方法需要更少的选择步骤。但是，通常全局方案需要更多的候选点，因为每次拆分后候选点都没有细化。而局部方案在分割后细化候选对象，并且可能更适合更深的树。在Higgs boson数据集上不同算法的比较如图3所示。作者发现，局部方案确实需要更少的候选人。如果有足够的候选人，全局方案可以和本地提案一样准确。</p>
<img src="/xgboost/higgs-dataset.png" class title="Higgs-10M数据集上测试AUC收敛性的比较">
<p>图3：Higgs-10M数据集上测试AUC收敛性的比较。EPS参数对应于近似数略图的精度。这大致相当于提案中的1/eps桶。作者发现，局部方案所需的存储空间更少，因为它细化了分离的候选人。</p>
<p>大多数现有的分布式树学习近似算法也遵循这一框架。值得注意的是，也可以直接构造近似直方图的梯度统计。也可以使用其他的二进制策略而不是分位数。分位数策略受益于可分配和可重新计算，作者将在下一小节详细介绍。从图3中，作者还发现，分位数策略可以是近似的精度与精准贪心算法相同的精度。</p>
<p>作者系统有效地支持精确贪心的单机设置，以及具有局部和全局方案的近似算法。用户可以根据自己的需要自由选择方法。</p>
<h2 id="加权分位数算法">加权分位数算法</h2>
<p>近似算法中的一个重要步骤是提出候选分割点。通常，特征的百分位数用于使候选数据在数据上均匀分布。形式上，如果多数据集<span class="math inline">\(\mathcal{D}_{k}=\left\{\left(x_{1 k}, h_{1}\right),\left(x_{2 k}, h_{2}\right) \cdots\left(x_{n k}, h_{n}\right)\right\}\)</span>表示第<span class="math inline">\(k\)</span>个特征值和每个训练实例上的二阶梯度统计。作者定义一个排序函数<span class="math inline">\(r_{k}: \mathbb{R} \rightarrow[0,+\infty)\)</span>如：</p>
<p><span class="math display">\[\begin{equation}
r_{k}(z)=\frac{1}{\sum_{(x, h) \in \mathcal{D}_{k}} h} \sum_{(x, h) \in \mathcal{D}_{k}, x&lt;z} h
\end{equation}\]</span></p>
<p>它表示特征值<span class="math inline">\(k\)</span>小于<span class="math inline">\(z\)</span>的实例的比例。算法的目标是找到候选的分割点<span class="math inline">\(\left\{s_{k 1}, s_{k 2}, \cdots s_{k l}\right\}\)</span>，如：</p>
<p><span class="math display">\[\begin{equation}
\left|r_{k}\left(s_{k, j}\right)-r_{k}\left(s_{k, j+1}\right)\right|&lt;\epsilon, \quad s_{k 1}=\min _{i} \mathbf{x}_{i k}, s_{k l}=\max _{i} \mathbf{x}_{i k}
\end{equation}\]</span></p>
<p>这里的<span class="math inline">\(\epsilon\)</span>是一个近似因子。表示大约有<span class="math inline">\(\frac{1}{\epsilon}\)</span>个候选点。每个数据点都是用<span class="math inline">\(h_{i}\)</span>作为权值。为了明白为什么<span class="math inline">\(h_{i}\)</span>表示权重，作者重写了公式3：</p>
<p><span class="math display">\[\sum_{i=1}^{n} \frac{1}{2} h_{i}\left(f_{t}\left(\mathbf{x}_{i}\right)-g_{i} / h_{i}\right)^{2}+\Omega\left(f_{t}\right)+\text { constant }\]</span></p>
<p>该式就是标签集<span class="math inline">\(\frac{g_{i}}{h_{i}}\)</span>和权重<span class="math inline">\(h_{i}\)</span>的加权平方损失。对于大型数据集，找到满足条件的候选拆分是非常重要的。当每个实例具有相等的权重时，使用现有的分位数算法来解决了这个问题。然而，对于加权的数据集来说就没有现成分位数。因此，大多数现有的近似算法要么对可能失败的数据子集进行排序，要么对算法没有理论上的保证。为了解决这个问题，我们引入了一种全新的分布式加权分位数算法，该算法可以在可证明的理论保证下处理加权数据。总体思路是提出一种支持合并和修剪操作的数据结构，并证明每个操作都可以保持一定的准确性。附录中给出了该算法的详细说明以及证明。</p>
<h2 id="稀疏感知的分割点查找">稀疏感知的分割点查找</h2>
<p>在许多实际问题中，输入数据<span class="math inline">\(x\)</span>为稀疏是很常见的。稀疏性有多种原因：1）数据中存在缺失值；2）统计数据中经常出现零条目；3）特征工程的结果，如one-hot编码。因此使算法知道数据中的稀疏模式非常重要。为此，我们建议在每个树节点中添加一个默认方向，如图4所示。当稀疏矩阵<span class="math inline">\(x\)</span>中缺少值时，实例将分类为默认方向。每个分支中都有两个默认方向选择。从数据中获取最佳默认方向，如算法3所示。算法的关键改进是仅访问没有丢失的条目<span class="math inline">\(I_{k}\)</span>。提出的算法将不存在视为缺失值，并学习处理缺失值的最佳方向。当不存在对应于用户指定的值时，也可以应用相同的算法，方法是将枚举限制为相同的解。</p>
<img src="/xgboost/tree-structure-with-default-directions.png" class title="有默认方向的树结构">
<p>图4：有默认方向的树结构。当特征缺少分割所需的功能时，会将示例分类为默认方向。</p>
<img src="/xgboost/sparsity-aware-split-finding.png" class title="稀疏感知分割点查找算法">
<p>据我们所知，大多数现有的树学习算法要么只对密集数据进行优化，要么需要特定的程序来处理有限的情况，如分类编码。而XGBoost以统一的方式处理所有稀疏数据。更重要的是，作者的方法利用稀疏性，使得计算复杂度与输入中的非缺失条目的数目成线性关系。如图5所示，展示了在Allstate-10K数据集上的稀疏感知和朴素实现的比较（数据集的介绍在第6节会说明）。作者发现稀疏感知算法的运行速度是原始算法的50倍。这证实了稀疏感知算法的重要性。</p>
<img src="/xgboost/impact-of-sparsity-aware-algorithm.png" class title="稀疏感知算法的影响">
<p>图5：在Allstate-10K数据集上稀疏感知算法的影响。数据集稀疏主要是由于一个热编码。稀疏感知算法比不考虑稀疏性的原始算法快50倍以上。</p>
<h1 id="系统设计">系统设计</h1>
<h2 id="按列分块并行化学习">按列分块并行化学习</h2>
<p>树学习最耗时的部分是将数据排序。为了降低排序的成本，作者将数据存储在内存单元中，称之为块。每个块中的数据以压缩列（compressed column，CSC）格式存储，每列按相应的特征值排序。这个输入数据布局只需要在训练之前计算一次，并且可以在以后的迭代中重复用。</p>
<p>在精确贪心算法中，我们将整个数据集存储在一个块中，并通过对其预排序条目进行线性扫描来运行分割搜索算法。我们对所有的叶节点进行集体的分割查找，因此在块上扫描一次，将收集所有叶子分支中的分割候选的统计信息。如图6所示，展示了如何将数据集转换为该格式，并使用块结构找到最佳分割。</p>
<img src="/xgboost/block-structure-parallel-learning.png" class title="块结构的并行学习">
<p>图6：块结构的并行学习。块中的每列会通过相应的特征值进行重新排列。对块中的每列进行线性扫描能够枚举所有的分割点。</p>
<p>当使用近似算法时块结构也可以有所帮助，即可以使用多个块，每个块对应于数据集中的行子集。不同的块可以分布在不同的机器上，也可以存储在磁盘上。通过使用排序结构，分位数查找步骤就变成对排序列的线性扫描。这对于在每个分支频繁生成候选节点的局部算法算法尤其有价值。直方图聚合中的二值搜索也成为一种线性时间合并算法。</p>
<p>并行的收集每一列的统计信息，可以为系统提供了一种并行的分割点查找算法。重要的是，列块结构还支持列采样，因为很容易在块中选择列的子集。</p>
<p><code>时间复杂度分析</code>设<span class="math inline">\(d\)</span>是树的最大深度，<span class="math inline">\(K\)</span>是所有树的数量。对于精确的贪心算法来说，原始空间感知算法的时间复杂度是<span class="math inline">\(O\left(K d\|\mathbf{x}\|_{0} \log n\right)\)</span>。这里作者使用<span class="math inline">\(\|\mathbf{x}\|_{0}\)</span>表示训练数据中没有丢失条目的数据数量。此外，在块结构上Tree Boosting只需要时间复杂度是<span class="math inline">\(O\left(K d\|\mathbf{x}\|_{0}+\|\mathbf{x}\|_{0} \log n\right)\)</span>。这里的<span class="math inline">\(O\left(\|\mathbf{x}\|_{0} \log n\right)\)</span>是可以被平摊的一次预处理花费的时间。这个分析说明了块结构能够额外的节约<span class="math inline">\(\log n\)</span>，这在<span class="math inline">\(n\)</span>很大时非常重要。对于近似算法，原始的二值搜索算法的时间复杂度是<span class="math inline">\(O\left(K d\|\mathbf{x}\|_{0} \log q\right)\)</span>。这里的<span class="math inline">\(q\)</span>是当前数据集中候选节点的数量。通常<span class="math inline">\(q\)</span>的值在32到100之间，但是<span class="math inline">\(log\)</span>因子仍然会带来开销。如果使用块结构，时间复杂度就会降低到<span class="math inline">\(O\left(K d\|\mathbf{x}\|_{0}+\|\mathbf{x}\|_{0} \log B\right)\)</span>，其中<span class="math inline">\(B\)</span>是每个块中最大的行数。这样就可以在计算中节省额外的<span class="math inline">\(\log q\)</span>因子。这是一个非连续内存访问。</p>
<h2 id="缓存感知访问">缓存感知访问</h2>
<p>虽然所提出的块结构有助于优化分裂点查找的计算复杂度，但是新算法需要通过行索引间接获取梯度统计，因为这些值按特征顺序被访问。枚举分裂点的原始实现在累积和非连续内存获取操作之间引入了直接读/写依赖关系(如图8所示)。当梯度统计数据不适合CPU缓存且发生缓存未命中时，这会减慢拆分查找速度。</p>
<img src="/xgboost/short-range-data-dependency-pattern.png" class title="短距离数据相关性模式">
<p>图8：短距离数据相关性模式，可能由于缓存丢失中而导致停顿。</p>
<p>对于精确贪心算法，我们可以通过一个缓存感知的预取算法来缓解这个问题。具体来说，作者在每个线程中分配一个内部缓冲区，将梯度统计信息提取到其中，然后以小批量的方式执行累积。此预取将直接读/写依赖项更改为更长的依赖项，并有助于在行数较大时减少运行时开销。图7给出了有缓存感知和没缓存感知算法下在Higgs和Allstate数据集的比较。作者发现，当数据集较大时，精确贪婪算法的缓存感知实现的运行速度是原始算法的两倍。</p>
<img src="/xgboost/impact-of-cache-aware.png" class title="精确贪婪算法中缓存感知预取的影响">
<p>图7：精确贪婪算法中缓存感知预取的影响。作者发现缓存未命中效应会影响大数据集（1000万个实例）的性能。当数据集很大时，使用缓存感知预取可将性能提高两倍。</p>
<p>对于近似算法，我们通过选择正确的块大小来解决问题。我们将块大小定义为块中包含的最大实例数，因为这反映了梯度统计的缓存存储成本。选择过小的块大小会导致每个线程的工作量较小，并导致效率低下的并行化。另一方面，过大的块会导致缓存不命中，因为梯度统计信息不适合CPU缓存。选择合适的块大小可以平衡了这两个因素。我们比较了两个数据集上块大小的各种选择，结果在图9中展示。这一结果验证了我们的讨论，并表明每个块选择<span class="math inline">\(2^{16}\)</span>个示例平衡了缓存属性和并行化。</p>
<img src="/xgboost/impact-of-block-size-in-the-approximate-algorithm.png" class title="在近似算法中块大小的影响">
<p>图9：在近似算法中块大小的影响。作者发现，过小的块会导致效率低下的并行化，而过大的块也会由于缓存未命中而减慢训练速度。</p>
<h2 id="核外计算块">核外计算块</h2>
<p>作者系统的目标之一是充分利用机器的资源来实现可扩展的学习。除了处理器和内存之外，利用磁盘空间来处理不适合主内存的数据也很重要。为了实现核外计算，我们将数据分为多个块，并将每个块存储在磁盘上。在计算过程中，使用独立的线程将块预取到主内存缓冲区中很重要，因此可以在读取磁盘的同时进行计算。但是，由于磁盘读取占用了大部分计算时间，因此不能完全解决问题。重要的是减少开销并增加磁盘IO的吞吐量。作者主要使用两种技术来改进核外计算。</p>
<p><code>1、块压缩</code>第一个技术作者使用块压缩技术。块通过列进行压缩，加载到主内存时由独立线程动态解压缩。这有助于将解压中的一些计算与磁盘读取成本进行权衡。作者使用通用的压缩算法来压缩特征值。对于行索引，作者用块的起始索引减去行索引，并使用16位整数存储每个偏移量。每个块有<span class="math inline">\(2^{16}\)</span>个实例是一个较好的设置。在大多数测试数据集中，获得了大约26%到29%的压缩比。</p>
<p><code>2、块分片</code>第二种技术是以另一种方式将数据分片到多个磁盘上。为每个磁盘分配一个预取线程，并将数据提取到内存缓冲区中。然后，训练线程交替地从每个缓冲区读取数据。当多个磁盘可用时，这有助于提高磁盘读取的吞吐量。</p>
<h1 id="相关工作">相关工作</h1>
<p>作者的系统实现了Gradient Boosting，在函数空间中执行加法优化。Gradient Tree Boosting已经成功应用与分类、学习排序、结构预测等领域。XGBoost采用了一个正则化模型来防止过度拟合。这类似于以前关于正则贪心森林的工作，但简化了并行化的目标和算法。列采样是从随机森林中借用的一种简单而有效的技术。虽然稀疏感知学习在线性模型等其他类型的模型中是必不可少的，但是很少有在树学习工作中以一种原则性的方式考虑这个问题。本文提出的算法是第一个处理各种稀疏模式的统一方法。</p>
<p>关于并行化树学习已有若干工作。这些算法大多属于本文所描述的近似框架。值得注意的是，还可以按列划分数据，并应用精确的贪婪算法。作者的框架也支持这一点，并且可以使用诸如缓存感知之类的技术来改进这类算法。虽然大多数现有的工作集中在算法方面的并行化，我们的工作改进了两个未开发的方向：核心外计算和缓存感知学习。这给我们提供了如何联合优化系统和算法的启发，并提供了一个端到端的系统，可以在非常有限的计算资源下处理大规模问题。作者还总结了系统和现有的开源实现在表1中的比较。</p>
<p>表1：主要的Tree Boosting系统的对比</p>
<img src="/xgboost/comparison-major-tree-boosting-systems.png" class title="主要的Tree Boosting系统的对比">
<p>分位数汇总（不带权重）是数据库社区中的一个经典问题。然而，近似Tree Boosting算法揭示了更一般的问题——找到加权数据上的分位数。据我们所知，本文提出的加权分位数草图是解决这一问题的第一个方法。加权分位数汇总不是树学习中独有的，而是对数据科学和机器学习中的其他应用中非常有用的内容。</p>
<h1 id="端到端的评估">端到端的评估</h1>
<h2 id="系统实现">系统实现</h2>
<p>作者以开源的方式实现了XGBoost。该软件包是可移植和可重用的。支持多种加权分类和排序目标函数，支持自定义目标函数。它以流行语言（如python、R、Julia）提供，并与语言原生数据科学管道（如scikitlearn）自然集成。分布式版本构建在顶级的Rabbit库之上。XGBoost的可移植性使其可在许多生态系统中使用，而不仅限于特定的平台。分布式XGBoost本机运行在Hadoop，MPI Sun Grid引擎上。最近，作者还在jvm大数据堆栈（例如Flink和Spark）上启用了分布式XGBoost。分布式版本也已集成到阿里巴巴的云平台Tianchi中。</p>
<h2 id="数据集和设置">数据集和设置</h2>
<h2 id="分类任务">分类任务</h2>
<h2 id="学习排序">学习排序</h2>
<h2 id="核外实验">核外实验</h2>
<h2 id="分布式实验">分布式实验</h2>
<h1 id="结论">结论</h1>
<p>本文介绍了作者在构建XGBoost时所吸取的经验教训，XGBoost是一个可扩展的Tree Boosting 系统，被数据科学家广泛使用，并在许多问题上提供了更好的结果。同时，作者还提出了一种新的稀疏感知算法处理稀疏数据和理论上合理的加权分位数近似学习。经验表明，缓存访问模式、数据压缩和分片是构建用于树提升的可伸缩端到端系统的基本要素。这些经验教训也可以应用于其他机器学习系统。通过结合这些见解，XGBoost能够使用最少的资源解决现实世界规模的问题。</p>
<h1 id="参考资料">参考资料</h1>
<p><a href="https://github.com/dmlc/xgboost" target="_blank" rel="noopener">XGBoost源代码</a></p>
<p><a href="https://github.com/dmlc/rabit" target="_blank" rel="noopener">分布式XGBoost源代码</a></p>
<p><a href="https://www.kaggle.com/c/ClaimPredictionChallenge" target="_blank" rel="noopener">Allstate insurance claim数据集</a></p>
<p><a href="https://archive.ics.uci.edu/ml/datasets/HIGGS" target="_blank" rel="noopener">Higgs boson数据集</a></p>
<p><a href="https://archive.ics.uci.edu/ml/datasets/HIGGS" target="_blank" rel="noopener">Higgs boson数据集</a></p>
<p><a href="http://labs.criteo.com/downloads/download-terabyteclick-logs/" target="_blank" rel="noopener">Criteo数据集</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/65805298" target="_blank" rel="noopener">超级女王XGBoost到底“绝”在哪里？</a></p>
<p><a href="https://mp.weixin.qq.com/s?__biz=Mzg2MjI5Mzk0MA==&amp;mid=2247484256&amp;idx=1&amp;sn=9417deb81a2ebffb5e81603425badab7&amp;chksm=ce0b59bbf97cd0ad03e79a87e8f0b8e9479768203046357305a4a5483f2de316580ccff6b7cf&amp;mpshare=1&amp;scene=1&amp;srcid=1109RvBiOxncgoKzk6aNypZR&amp;sharer_sharetime=1573614979764&amp;sharer_shareid=98178f12e3ad48ff6ea4030d6be95e39&amp;pass_ticket=ya68GxkXVWyQO%2Fydmx5oO9LpFScrK%2BdaKW%2BrLyBrVg5f9LKKBiLd1F6CgRa%2BfPeg#rd" target="_blank" rel="noopener">XGBoost超详细推导，终于有人讲明白了！</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/41417638" target="_blank" rel="noopener">【干货合集】通俗理解kaggle比赛大杀器xgboost</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/87885678" target="_blank" rel="noopener">【机器学习】决策树（下）——XGBoost、LightGBM（非常详细）</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/36794802" target="_blank" rel="noopener">XGBoost论文阅读及其原理</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/31654000" target="_blank" rel="noopener">机器学习竞赛大杀器XGBoost--原理篇</a></p>
]]></content>
      <categories>
        <category>论文</category>
      </categories>
      <tags>
        <tag>集成学习</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>【2018】Fast Multi-Instance Multi-Label Learning</title>
    <url>/fast-multi-instance-label/</url>
    <content><![CDATA[<h1 id="摘要">摘要</h1><p>在许多实际任务中，特别是那些涉及图像、文本等语义复杂的数据对象的任务中，一个对象可以用多个实例表示，同时与多个标签相关联。这些任务都可以表示为多实例多标签学习（MIML）问题，并且近几年得到了广泛的研究。现有的MIML方法已经在许多应用中得到证明是有用的；然而，这些方法中的大部分只能处理中等规模的数据。为了有效的处理大型数据集，本文作者提出了“MIML-Fast”方法，该方法首先构建了一个所有标签共享的低维子空间，然后通过随即梯度下降（SGD）来训练特定的标签线性模型来近似标签的排序损失。尽管MIML问题非常复杂，但是MIML-Fast能够利用共享空间中标签之间的关系发现复杂的标签子关系，从而获得优秀的性能。试验表明，MIML-Fast的性能与目前最先进的技术相当，但是却花费更少的时间。此外，MIML-Fast方法能够识别每个标签最具代表性的实例，从而提供了一个了解输入模式和输出标签语义之间关系的机会。</p><a id="more"></a>

<h1 id="介绍">介绍</h1>
<p>在传统的监督学习中，一个对象表示为一个实例，并且只与一个标签相关联。然而，在许多实际应用中，一个对象可以由多个实例来描述，并且关联多个标签分类。例如，在图像分类任务中，一个图像可以与多个语义标签相关联，并且可以分成多个段落，每个段落由一个实例来表示，如图1所示。在文本分类中，一篇文章可能属于多个分类，并且可以使用一个实例包来进行表示，每个段落代表一个实例；在基因功能预测人物中，一个基因通常与多个标签关联，因为这个基因与多个功能相关，并且可以通过一组不同视图的图像来表达。因此，MIML学习是近几年提出的面向复杂对象的学习框架。</p>
<img src="/fast-multi-instance-label/miml-example.png" class title="MIML实例">
<p>图1：MIML实例：图像由多个实例表示，并与多个标签关联</p>
<p>在过去几年，许多MIML算法被提出，并且其中的一些方法是把任务进行简化。例如，MIMLBoost把MIML任务简化为彝族多实例单标签的学习任务；MIMLSVM则通过把一个实例包转化为单实例来把任务简化为多标签学习任务。而DMIMLSVM方法则直接优化了MIML任务中的损失函数。还有其他相关的工作来试图使用现有的技术来解决MIML任务。</p>
<p>该方法获得了良好的性能，并且验证了MIML框架在各种应用中的优越性。然而，随着表达能力的增强，MIML的假设空间急剧膨胀，导致现有方法较高的复杂性和较低的效率。这些方法通常比较耗时，并且不能处理大规模数据，严重限制了应用MIML的学习。</p>
<p>为了克服这个问题，本文中，作者提出了一个全新的方法“MIML-Fast”来加速在MIML中数据的学习。尽管为提高效率采用了简单的线性模型，但MIML-Fast可以有效地近似原始的MIML问题。具体来说，为了有效利用多标签之间的关系，作者首先从所有标签的原始特征中得到一个共享空间，然后从共享空间中训练一个标签的线性模型。为了识别具有关键特征的标签实例包，作者训练了一个用于划分实例等级的分类模型，并且选择具有最大预测值的实例作为关键实例。为了使学习的高效性，作者使用了随机梯度下降（SGD）来优化排序损失。在SGD的每步中，MIML-Fast随即采样了一个元组，其中包含一个包，一个包相关的标签和一个不相关的标签，并且优化模型，使其排在不相关标签之前。与目前最先进的MIML方向相比，MIML-Fast具有很强的竞争性，并且在大数据集上的性能提高了100倍。</p>
<p>虽然大多数现有方法主要侧重于改进方法的泛化性，但MIML学习的另一个重要任务是理解输入模式和输出标签语义之间的相关性，即利用实例和标签之间的对应关系。作者的方法能够根据实例级别的预测来得到每个标签最典型的实例。</p>
<p>此外，我们的方法尝试利用标签的子关系来处理复杂的标签。通过对每个标签自适应多个模型，该方法可以区分复杂标签中嵌入的不同子概念。</p>
<h1 id="相关工作">相关工作</h1>
<p>作者首先回顾了两个与MIML相关的学习框架，分别是多标签多系和多实例学习。</p>
<p>在多标签学习中，每个对象都可以表示为一个实例，并且这个实例有多个相关的标签。在过去许多年中多标签学习已经被很好的研究，并且提出了许多算法。目前的方法可以被分为两类，分别是问题转换法和算法适配法。在问题转换法中，最简单的方法就是把多标签任务分解为一串二分类问题，每一个问题对应一个标签，每个标签不具有相关性。而其他方法则试图把多标签任务转化为多分类问题，其中每个类都都可能是一个标签的子集。在算法适配法中尝试将流行的学习技术应用到多标签中。代表方法包括Boosting风格算法AdaBoost.MH，惰性学习算法ML-KNN，基于决策树的算法ML-DT，基于神经网络的算法。此外，弱监督的多标签学习已经在研究，无论是主动查询还是局部标注。近年来，如果利用标签之间的关系越来越引起研究者的兴趣。这方面的工作包括利用标签结构的先验知识和自动挖掘标签之间的相关性。</p>
<p>多实例学习最初是由Dietterich等人提出的，应用于药物活性预测。在该框架下，每个对象都可以用一组实例来进行描述，并与一个标签进行关联。学习任务在没有实例注释的情况下是弱监督的，因此具有相当大的挑战性。许多流程的机器学习方法已经用于多实例的表达。例如，决策树算法MITI，核方法Mi-Kernel和边缘化的Mi-Kernel，惰性学习算法Citation-knn和Bayesian-knn，以及集成方法HSMILE。</p>
<p>MIML学习是一个更为通用的框架。如文献中所述，多学习框架的存在是由于在表达现实世界的对象中存在歧义性。MIML同时考虑了输入和输出空间的歧义性，因此更自然和方便地处理涉及此类对象的任务。图2总结了四种学习框架之间的差异。</p>
<img src="/fast-multi-instance-label/four-different-learning-frameworks.png" class title="四种学习框架之间的差异">
<p>图2：四种学习框架之间的差异</p>
<p>过去几年许多MIML方法被提出。例如，MIMLSVM会把MIML问题转化为单实例多标签任务来解决。MIMLBoost把MIML问题转化为多实例单标签任务来解决。一种MIML的生成模型被杨等人提出。文献中还提出了最近邻和神经网络方法。查等人提出了一种用于MIML图像标注的隐藏条件随机场模型。Briggs等人提出了在MIML标识时的优化排序损失。在文献中，作者试图通过为每个带聚类的标签构造一个原型来发现哪些模式触发了MIML学习中的哪些标签。最近，有一些研究试图将MIML框架扩展到新的环境中，包括类扩展、多视图学习和实例聚类。现有的MIML方法已经成功应用在许多场景中，由于计算量大，所以大多数应用于中等规模的数据。要处理大规模的数据，MIML就需要高效的方法。</p>
<p>在查阅前期的文章中，一些新的研究被提出。如文献31中提出了一种判别概率模型。该方法侧重于实例的标注，而不是在MIML环境下的包标注，并且提出了一种计算实例标签后验概率的动态规划方法。在文献12中，作者提出了一种深度MIML模型，该模型自动学习实例的描述，而不是手工设计实例的描述。在文献43中，另一个解决MIML学习的名叫MIML-FCN+的深度模型被提出。在文献1中，提出了一种具有多变量性能度量的可扩展优化方法。此外，激活标签查询和新类别发现也在MIML学习环境下被研究。</p>
<p>有一些研究试图通过排名来实现分类。在文献40中，一个类似的技术被用于优化图片标注中的WARP损失；然而，它处理的是单实例单标签问题，这与我们的MIML问题大不相同。在文献55中，提出了一种基于聚类的复杂概念中子概念发现的方法。然而，该方法关注的是但标签学习，也与我们的MIML任务不同。在MIML-Fast方法中利用标签信息，使用监督模型而不是启发式聚类来发现子概念。</p>
<h1 id="miml-fast方法">MIML-Fast方法</h1>
<p>假设<span class="math inline">\(\left\{\left(X_{1}, Y_{1}\right),\left(X_{2}, Y_{2}\right), \cdots,\left(X_{n}, Y_{n}\right)\right\}\)</span>是一个MIML的数据集，其中有<span class="math inline">\(n\)</span>个实例集合包，每个包<span class="math inline">\(X_{i}\)</span>都有<span class="math inline">\(z_{i}\)</span>个实例<span class="math inline">\(\left\{\mathbf{x}_{i, 1}, \mathbf{x}_{i, 2}, \cdots, \mathbf{x}_{i, z_{i}}\right\}\)</span>，而<span class="math inline">\(Y_{i}\)</span>包含了<span class="math inline">\(X_{i}\)</span>所有相关的标签，并且这些标签可能是所有可能标签<span class="math inline">\(\left\{y_{1}, y_{2} \cdots y_{L}\right\}\)</span>的一个子集。在本节的其余部分中，作者将首先介绍所提出的MIML预测模型，然后介绍学习任务的目标。其次，介绍提出的快速算法来优化目标函数。最后，为了进一步利用实例的标签信息，提出了一种改进的算法。</p>
<h2 id="预测模型">预测模型</h2>
<p>首先讨论如何在实例等级上建立分类模型，然后尝试从实例预测结果中得到实例包所对应的标签集。要处理多标签问题，最简单的方法就是为每个不相关的标签训练一个模型，并把多标签问题转化为一串单标签问题。然而，这种方法将会丢失标签之间相关性的信息，因为这种方法独立的对待标签，并且忽略标签之间的相关性信息。在作者的方法中，我们将模型表示为两个组件的组合。第一个组件是学习一个从原始特征空间到低维空间的线性映射，该空间与所有标签共享。第二个组件是基于这个共享空间学习一组特定的标签模型。这两个组件以交互方式优化，以适应所有标签中的训练示例。</p>
<p>从形式上来说，给定实例<span class="math inline">\(\mathbf{x}\)</span>，定义一个在标签<span class="math inline">\(l\)</span>上的分类模型</p>
<p><span class="math display">\[f_{l}(\mathbf{x})=\mathbf{w}_{l}^{\top} W_{0} \mathbf{x}\]</span></p>
<p>其中<span class="math inline">\(W_{0}\)</span>是一个把原始特征向量映射到共享空间的<span class="math inline">\(m \times d\)</span>维矩阵，<span class="math inline">\(\mathbf{w}_{l}\)</span>是标签<span class="math inline">\(l\)</span>的<span class="math inline">\(m\)</span>维权重向量。<span class="math inline">\(d\)</span>和<span class="math inline">\(m\)</span>分别是特征空间和共享空间的维数。图3展示了两层模型。这种模型有两个显著的优势。首先，通常<span class="math inline">\(m \ll d\)</span>，需要学习变量数从<span class="math inline">\(d \times L\)</span>降到了<span class="math inline">\((d+L) \times m\)</span>。这将是的内存和计算消耗都显著降低。其次，标签之间的关系也可以被利用。每个标签的实例包都有助于共享空间的优化，并且具有强关系的标签可以相互帮助。例如，假设<span class="math inline">\(l\)</span>是罕见的标签，由于只有很少的正例，所以该标签能难训练一个精准的模型，然而在我们的模型中，通过拟合其他标签的实例包，共享空间（<span class="math inline">\(W_{0}\)</span>）可以得到足够的训练，因为通过具有较好训练结果的<span class="math inline">\(W_{0}\)</span>来优化<span class="math inline">\(\mathbf{w}_{l}\)</span>要容易的多。</p>
<img src="/fast-multi-instance-label/two-level.png" class title="两层模型">
<p>图3：两层模型：<span class="math inline">\(W_{0}\)</span>表示从原始特征向量映射到共享子空间，<span class="math inline">\(\mathbf{w}_{l}\)</span>是标签<span class="math inline">\(l\)</span>的权重向量</p>
<p>然后作者改进模型来处理复杂标签中子关系。在MIML学习任务中，对象通常具有复杂的语义，因此可以为内容多样的实例包分配相同的标签。图4展示了一个在图像分类任务中具有多个子概念的复杂标签的例子。在图中，不同形状（圆形、方形、三角形、星形）表示属于不同标签的实例集，不同颜色表示不同子概念。正如所看到的，同一标签山的图像内容可以是一座沙山、一座雪山或一座树木覆盖的山。很难训练单个模型（对应于虚线红线）将内容如此多样化的图像分类到同一类别中。相反，作者建议一个标签学习多个模型，为每个子概念学习一个模型，并自动决定一个示例属于哪个子概念。每个子概念的模型（对应于黑色实线）都非常简单，并且可以很容易的拟合数据。假设每个标签有<span class="math inline">\(K\)</span>个子概念。对于具有标签<span class="math inline">\(l\)</span>的实例，首先检查<span class="math inline">\(K\)</span>个模型的预测值，然后选择具有最大预测值的子概念来自动确定所属的子概念。现在可以将具有标签<span class="math inline">\(l\)</span>的实例<span class="math inline">\(\mathbf{x}\)</span>预测重新定义为：</p>
<p><span class="math display">\[\begin{equation}
f_{l}(\mathbf{x})=\max _{k=1 \cdots K} f_{l, k}(\mathbf{x})=\max _{k=1 \cdots K} \mathbf{w}_{l, k}^{\top} W_{0} \mathbf{x}
\end{equation}\]</span></p>
<p>其中<span class="math inline">\(\mathbf{w} \iota, k\)</span>表示标签<span class="math inline">\(l\)</span>的第<span class="math inline">\(k\)</span>的子概念。需要注意的是，虽然假设每个标签有<span class="math inline">\(K\)</span>个子概念，但是允许空的子概念，即简单标签的示例可以仅分布在几个甚至一个子概念中。</p>
<img src="/fast-multi-instance-label/sub-concepts.png" class title="一个子概念的例子">
<p>图4：一个子概念的例子：带有“山”标签的图像可以是沙山、雪山或树木覆盖的山。</p>
<p>然后，接下来是如何从实例等级模型中获得每个包的预测。通常假设当且仅当至少包含一个正实例时，那么这个包才是正例。在这个假设下，一个在标签<span class="math inline">\(l\)</span>的包<span class="math inline">\(X\)</span>的预测能够被定义为在该包中所有实例预测的最大值：</p>
<p><span class="math display">\[f_{l}(X)=\max _{\mathbf{x} \in X} f_{l}(\mathbf{x})\]</span></p>
<p>称最大预测的实例为具有标签<span class="math inline">\(l\)</span>的包<span class="math inline">\(X\)</span>的典型实例。</p>
<h2 id="目标函数">目标函数</h2>
<p>作者把MIML分类问题转化为一个标签的排序问题，其目的是将所有实例的相关标签排在不相关标签之前。根据前面章节介绍的模型，对于实例<span class="math inline">\(X\)</span>和一个相关标签集<span class="math inline">\(l\)</span>，可以定义<span class="math inline">\(R(X, l)\)</span>为：</p>
<p><span class="math display">\[\begin{equation}
R(X, l)=\sum_{j \in \bar{Y}} I\left[f_{j}(X)&gt;f_{l}(X)\right]
\end{equation}\]</span></p>
<p>其中<span class="math inline">\(\bar{Y}\)</span>表示<span class="math inline">\(X\)</span>的不相关标签集，而<span class="math inline">\(I[\cdot]\)</span>是一个指示函数，当参数为真时返回1，否则就返回0。本质上，<span class="math inline">\(R(X, l)\)</span>计算的是在排在包<span class="math inline">\(X\)</span>的相关标签<span class="math inline">\(l\)</span>之前有多少个不相关的标签。</p>
<p>基于<span class="math inline">\(R(X, l)\)</span>就可以定义实例<span class="math inline">\(X\)</span>的相关标签<span class="math inline">\(l\)</span>的排序误差损失函数：</p>
<p><span class="math display">\[\begin{equation}
\epsilon(X, l)=\sum_{i=1}^{R(X, l)} \frac{1}{i}
\end{equation}\]</span></p>
<p>很明显，如果标签<span class="math inline">\(l\)</span>的排序很低，那么就说明排序误差<span class="math inline">\(\epsilon\)</span>很大。最后，在所有数据集上就可以得到排序误差损失为：</p>
<p><span class="math display">\[\text { Rank Error }=\sum_{i=1}^{n} \sum_{l \in Y_{i}} \epsilon(X, l)\]</span></p>
<p>基于式子2，排序误差损失函数<span class="math inline">\(\epsilon(X, l)\)</span>可以扩展到所有不相关标签<span class="math inline">\(\bar{Y}\)</span>：</p>
<p><span class="math display">\[\begin{equation}
\epsilon(X, l)=\sum_{j \in \bar{Y}} \epsilon(X, l) \frac{I\left[f_{j}(X)&gt;f_{l}(X)\right]}{R(X, l)}
\end{equation}\]</span></p>
<p>由于非凸性和不连续性，直接优化上述方程是相当困难的，因为这样的优化常常导致NP难问题。相反，我们研究了下面的Hinge损失，它被证明是所有凸代理损失中的最佳选择：</p>
<p><span class="math display">\[\begin{equation}
\Psi(X, l)=\sum_{j \in \bar{Y}} \epsilon(X, l) \frac{\left|1+f_{j}(X)-f_{l}(X)\right|_{+}}{R(X, l)}
\end{equation}\]</span></p>
<p>其中，当<span class="math inline">\(q \geq 0\)</span>时，<span class="math inline">\(|q|_{+}=q\)</span>，否则<span class="math inline">\(|q|_{+}=0\)</span>。代理损失函数<span class="math inline">\(\Psi(X, l)\)</span>可以视为<span class="math inline">\(\epsilon(X, l)\)</span>损失函数的上界，因为<span class="math inline">\(I[q] \leq|1+q|_{+}\)</span>。</p>
<p>为了将相关标签排在不相关标签之前，我们只需要最小化训练数据的排序损失，从而得到以下目标函数：</p>
<p><span class="math display">\[\begin{equation}
\min \sum_{i=1}^{n} \sum_{l \in Y_{i}} \Psi\left(X_{i}, l\right)
\end{equation}\]</span></p>
<h2 id="算法">算法</h2>
<p>作者使用随即梯度下降（SGD）来最小化排序损失函数。在SGD每次迭代过程中，作者随即采样一个实例包<span class="math inline">\(X\)</span>，一组相关标签<span class="math inline">\(y\)</span>，以及一组不相关标签<span class="math inline">\(\bar{y} \in \bar{Y}\)</span>，形成一个元组<span class="math inline">\((X, y, \bar{y})\)</span>，就可以得到损失函数：</p>
<p><span class="math display">\[\begin{equation}
\mathcal{L}(X, y, \bar{y})=\epsilon(X, y)\left|1+f_{\bar{y}}(X)-f_{y}(X)\right|_{+}
\end{equation}\]</span></p>
<p>结下来通过引理来揭示<span class="math inline">\(\Psi(X, y)\)</span>和<span class="math inline">\(\mathcal{L}(X, y, \bar{y})\)</span>之间的关系。</p>
<p><strong>引理1：</strong><span class="math inline">\(\Psi(X, y)=E_{\bar{y}}[\mathcal{L}(X, y, \bar{y})]\)</span>，其中<span class="math inline">\(E[\cdot]\)</span>表示<span class="math inline">\(\bar{Y}\)</span>上均匀分布的期望值。</p>
<p>证明：这个引理来源于在<span class="math inline">\(\bar{Y}\)</span>中随机选择<span class="math inline">\(\bar{y}\)</span>的概率是<span class="math inline">\(\frac{1}{R(X, y)}\)</span>。</p>
<p>要最小化<span class="math inline">\(\mathcal{L}(X, y, \bar{y})\)</span>，就需要提前计算<span class="math inline">\(R(X, y)\)</span>，即必须对每个<span class="math inline">\(\bar{y} \in \bar{Y}\)</span>比较<span class="math inline">\(f_{y}(X)\)</span>和<span class="math inline">\(f_{\bar{y}}(X)\)</span>，而当可能的标签数量很大时，这可能会很耗时。因此，受到Weston等人的启发，作者使用近似来估计<span class="math inline">\(R(X, y)\)</span>。特别是在SGD的每步迭代中中，作者随即从不相关的标签集<span class="math inline">\(\bar{Y}\)</span>逐个随机采样标签集，直到<span class="math inline">\(\bar{y}\)</span>标签出现。如果某个标签出现在<span class="math inline">\(y\)</span>标签之前，那么就称<span class="math inline">\(\bar{y}\)</span>为错误标签，例如<span class="math inline">\(f_{\bar{y}}(X)&gt;f_{y}(X)-1\)</span>。为了不是损失函数不失一般性，假设第一个错误标签被发现在第<span class="math inline">\(v\)</span>步的采样过程，然后<span class="math inline">\(R(X, y)\)</span>可以按照下面的引理通过<span class="math inline">\(\lfloor|\bar{Y}| / v\rfloor\)</span>进行近似：</p>
<p><strong>引理2：</strong>作者使用<span class="math inline">\(\theta\)</span>表示一个随机事件，<span class="math inline">\(\theta=i\)</span>表示第一个错误标签的事件在第i个采样步骤。就可以得到：</p>
<p><span class="math display">\[\frac{R(X, y)}{|\bar{Y}|} \approx E_{\theta}\left[\frac{1}{\theta}\right]\]</span></p>
<p>证明：为了方便，作者设置<span class="math inline">\(p=\frac{R(X, y)}{|\bar{Y}|}\)</span>，并且假定<span class="math inline">\(0 &lt; p &lt; 1\)</span>。可以很容易得出，当<span class="math inline">\(i \geq 1\)</span>时的概率：</p>
<p><span class="math display">\[\operatorname{Pr}[\theta=i]=(1-p)^{i-1} p\]</span></p>
<p>并且可以得到</p>
<p><span class="math display">\[
\begin{aligned}
E_{\theta}\left[\frac{1}{\theta}\right] &amp;=\sum_{i=1}^{\infty} \frac{1}{i} p(1-p)^{i-1}=\frac{p}{1-p} \sum_{i=1}^{\infty} \frac{1}{i}(1-p)^{i} \\
&amp;=\frac{-p}{1-p} \ln (1-(1-p)) \approx p
\end{aligned}
\]</span></p>
<p>其中，<span class="math inline">\(\sum_{i=1}^{\infty} \frac{1}{i}(1-p)^{i}=-\ln (p)\)</span>，并且<span class="math inline">\(\ln (1+q) \approx q\)</span>。证明完成。</p>
<p>作者假设在第<span class="math inline">\(t\)</span>步的SGD迭代中得到采样的元组为<span class="math inline">\((X, y, \bar{y})\)</span>，其中相关标签为<span class="math inline">\(y\)</span>，关键实例为<span class="math inline">\(x\)</span>，并在第<span class="math inline">\(k\)</span>个子概念上获得了最大预测，而在标签<span class="math inline">\(\bar{y}\)</span>上，实例<span class="math inline">\(\bar{x}\)</span>在第<span class="math inline">\(k\)</span>个子概念上获得了最大的预测。然后就可以得到了元组的近似排序损失：</p>
<p><span class="math display">\[
\mathcal{L}(X, y, \bar{y})=\epsilon(X, y)\left|1+f_{\bar{y}}(X)-f_{y}(X)\right|_{+} \approx \left\{\begin{matrix}
0 &amp; \bar{y} \neq \text{violated}\\
S_{\bar{Y}, v}\left(1+\left[\mathbf{w}_{\bar{y}, \bar{k}}^{t}\right]^{\top} W_{0}^{t} \overline{\mathbf{x}}-\left[\mathbf{w}_{y, k}^{t}\right]^{\top} W_{0}^{t} \mathbf{x}\right) &amp; \bar{y} = \text{violated}
\end{matrix}\right.
\]</span></p>
<p>这里为了方便介绍，作者引入了<span class="math inline">\(S_{\bar{Y}, v}=\sum_{i=1}^{\left\lfloor\frac{|\bar{Y}|}{v}\right\rfloor} \frac{1}{i}\)</span>。因此，如果对错误标签<span class="math inline">\(\bar{y}\)</span>进行采样，作者根据下面三个参数进行梯度下降：</p>
<p><span class="math display">\[\begin{equation}
W_{0}^{t+1}=W_{0}^{t}-\gamma_{t} S_{\bar{Y}, v}\left(\mathbf{w}_{\bar{y}, \bar{k}}^{t} \overline{\mathbf{x}}^{\top}-\mathbf{w}_{y, k}^{t} \mathbf{x}^{\top}\right)
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
\mathbf{w}_{y, k}^{t+1}=\mathbf{w}_{y, k}^{t}+\gamma_{t} S_{\bar{Y}, v} W_{0}^{t} \mathbf{x}
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
\mathbf{w}_{\bar{y}, \bar{k}}^{t+1}=\mathbf{w}_{\bar{y}, \bar{k}}^{t}-\gamma_{t} S_{\bar{Y}, v} W_{0}^{t} \overline{\mathbf{x}}
\end{equation}\]</span></p>
<p>其中<span class="math inline">\(\gamma_{t}\)</span>是第<span class="math inline">\(t\)</span>次迭代的SGD步长。在更新完参数后，<span class="math inline">\(\mathbf{w}_{y, k}\)</span>和<span class="math inline">\(\mathbf{w}_{\bar{y}, \bar{k}}\)</span>，以及<span class="math inline">\(W_{0}\)</span>的每列进行归一化，使其L2范式小于常数<span class="math inline">\(C\)</span>。</p>
<p>MIML-Fast伪代码如算法1所示。首先，<span class="math inline">\(W_{0}\)</span>的每列和所有标签<span class="math inline">\(y\)</span>的权重<span class="math inline">\(\mathbf{w}_{y}^{k}\)</span>，以及所有子概念<span class="math inline">\(k\)</span>将被随机初始化，并且平均值为0，标准差为<span class="math inline">\(1 / \sqrt{d}\)</span>。然后在SGD的每步迭代中，元组<span class="math inline">\((X, y, \bar{y})\)</span>是随即采样的，并且他们相应的代表实例和子概念会被识别出来。之后，按照式8-10执行梯度下降来更新这三个参数<span class="math inline">\(W_{0}\)</span>、<span class="math inline">\(\mathbf{w}_{y, k}\)</span>和<span class="math inline">\(\mathbf{w}_{\bar{y}, \bar{k}}\)</span>。最后，对更新后的参数进行归一化，使其范数小于常数<span class="math inline">\(C\)</span>。重复此过程，直到达到某些停止标准。在作者的实验中，作者从训练数据中抽取一小部分样本组成一个验证集，如果验证集上的排名损失不再减少，则停止训练。</p>
<img src="/fast-multi-instance-label/mimlfast-alg.png" class title="MIML-Fast伪代码">
<p>在算法的测试阶段，对于测试包<span class="math inline">\(X_{\text {test }}\)</span>，可以得到在每个标签上的预测，并且得到相应所有标签的排序。对于单标签分类问题，可以非常简单的通过选择预测值中最大的一个就可以得到<span class="math inline">\(X_{\text {test }}\)</span>的标签。但是，在多标签学习中，包<span class="math inline">\(X_{\text {test }}\)</span>可能至少一个以上的相关标签，并且不知道因该从标签排序列表中选择多少个标签作为相关标签。为了解决这个问题，作者为每个包赋予一个虚拟的标签（表示为<span class="math inline">\(\hat{y}\)</span>），并且训练模型中，所有无关标签在虚拟标签之后，所有相关标签排在虚拟标签之前。为了实现这一想法，作者特别考虑了构造不相关标签集<span class="math inline">\(\bar{Y}\)</span>。具体来说，当实例包<span class="math inline">\(X\)</span>及其标签<span class="math inline">\(y\)</span>被采样时（在算法1的第6行中），算法将首先检查<span class="math inline">\(y\)</span>是否是虚拟标签。如果<span class="math inline">\(y=\hat{y}\)</span>，那么<span class="math inline">\(\bar{Y}\)</span>包含所有的不相关标签，否则<span class="math inline">\(\bar{Y}\)</span>既包含虚拟标签，也包含不相关标签。使用这种方法，模型将被训练在相关标签和不相关标签之间排列虚拟标签。对于测试实例包，选择预测值大于假标签上预测值的标签作为相关标签。</p>
<p>最后，给出了算法收敛速度的一些理论保证。具体如下：</p>
<p><span class="math display">\[\mathcal{L}_{t}\left(W_{0}, \mathbf{w}_{y, k}, \mathbf{w}_{\bar{y}, \bar{k}}\right)=S_{\bar{Y}, v}\left(1+\mathbf{w}_{\bar{y}, \bar{k}}^{\top} W_{0} \overline{\mathbf{x}}_{t}-\mathbf{w}_{y, k}^{\top} W_{0} \mathbf{x}_{t}\right)_{+}\]</span></p>
<p>上式表示用模型参数<span class="math inline">\(W_{0}\)</span>、<span class="math inline">\(\mathbf{w}_{y, k}\)</span>和<span class="math inline">\(\mathbf{w}_{\bar{y}, \bar{k}}\)</span>和</p>
<p><span class="math display">\[\left(W_{0}^{*}, \mathbf{w}_{l, k}^{*}\right) \in \arg \min \sum_{t} \mathcal{L}_{t}\left(W_{0}, \mathbf{w}_{y, k}, \mathbf{w}_{\bar{y}, \bar{k}}\right)\]</span></p>
<p>进行的第<span class="math inline">\(t\)</span>次SGD迭代损失是最优解，我们有以下定理。</p>
<p><strong>定理1：</strong></p>
<p>证明：</p>
<p>通过选择合适的初始值和简单的计算。这个定理如所愿。</p>
<h2 id="使用实例标签进行学习">使用实例标签进行学习</h2>
<p>上面介绍的算法可以为每个标签识别一个实例包的典型实例。在一些MIML任务中，除了实例包标签外，每个标签的关键实例是已知的。例如，图像的标记任务中，不仅要为图像标记，还要为每个标记标识相应的区域。在这种情况下，调整算法来进一步利用这些监督信息。基本思想是将每个相关标签的关键实例排在其他实例之前。形式上，给定一个包实例<span class="math inline">\(X\)</span>及其相关标签中的一个<span class="math inline">\(y\)</span>，其代表实例<span class="math inline">\(\mathbf{x}^{*}\)</span>的排序损失可以定义为：</p>
<p><span class="math display">\[\begin{equation}
\xi\left(y, \mathbf{x}^{*}\right)=\sum_{i=1}^{R\left(y, \mathbf{x}^{*}\right)} \frac{1}{i}
\end{equation}\]</span></p>
<p>其中</p>
<p><span class="math display">\[\begin{equation}
R\left(y, \mathbf{x}^{*}\right)=\sum_{\mathbf{x} \in X} I\left[f_{y}(\mathbf{x})&gt;f_{y}\left(\mathbf{x}^{*}\right)\right]
\end{equation}\]</span></p>
<p>统计在标签<span class="math inline">\(y\)</span>上的代表实例之前排列的实例数。显然，当代表实例<span class="math inline">\(\mathbf{x}^{*}\)</span>在标签<span class="math inline">\(y\)</span>上最好的预测值时，则误差为0。与标签排序类似，如果实例<span class="math inline">\(\tilde{\mathbf{x}}\)</span>排在典型实例<span class="math inline">\(\mathbf{X}^{*}\)</span>之前，那么将会引入代理排序损失函数如下：</p>
<p><span class="math display">\[\begin{equation}
\mathcal{J}\left(y, \mathbf{x}^{*}, \tilde{\mathbf{x}}\right)=\xi\left(y, \mathbf{x}^{*}\right)\left|1+f_{y}(\tilde{\mathbf{x}})-f_{y}\left(\mathbf{x}^{*}\right)\right|_{+}
\end{equation}\]</span></p>
<p>根据第3.3节中类似的偏差，我们可以使用以下梯度下降规则来更新模型：</p>
<p><span class="math display">\[\begin{equation}
W_{0}^{t+1}=W_{0}^{t}-\gamma_{t} \xi\left(y, \mathbf{x}^{*}\right) \mathbf{w}_{y, k}^{t}\left(\tilde{\mathbf{x}}^{\top}-\mathbf{x}^{* \top}\right)
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
\mathbf{w}_{y, k}^{t+1}=\mathbf{w}_{y, k}^{t}-\gamma_{t} \xi\left(y, \mathbf{x}^{*}\right) W_{0}^{t}\left(\tilde{\mathbf{x}}-\mathbf{x}^{*}\right)
\end{equation}\]</span></p>
<p>需要注意的是，由于一个实例包中的实例数量通常不多，因此可以根据公式11有效地计算<span class="math inline">\(\xi\left(y, \mathbf{x}^{*}\right)\)</span>。在实例包较大情况下，也可以通过引理2的得到近似值。总之，当实例标签集可用时，可以组合式子14-15和式子8-10来共同优化标签和实例的排名。</p>
<h1 id="实验">实验</h1>
<h1 id="结论">结论</h1>
<p>MIML是一种面向复杂对象的学习框架，在许多应用中都被证明是有效的。然而，现有的MIML方法通常过于耗时，无法处理大规模的问题。本文提出了快速学习的MIML-Fast方法，并用MIML实例进行了快速学习，扩展了我们的初步研究。一方面，通过基于两级线性模型的SGD优化近似排序损失，有效地提高了效率；另一方面，通过利用共享空间中的标签关系和发现复杂标签的子概念来实现有效性。此外，作者的方法可以自然地检测每个标签的关键实例，从而提供了一个发现输入模式和输出标签语义之间关系的机会。在未来，作者将尝试优化其他损失函数，而不是排名损失。此外，还将研究更大规模的问题。</p>
]]></content>
      <categories>
        <category>论文</category>
        <category>多标签学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>多标签学习</tag>
      </tags>
  </entry>
  <entry>
    <title>ROS问题集锦</title>
    <url>/ros-qa/</url>
    <content><![CDATA[<h1 id="ros机械臂控制问题集锦">ROS机械臂控制问题集锦</h1><p><a href="https://blog.csdn.net/weixin_39579805" target="_blank" rel="noopener">参考链接</a></p>]]></content>
      <categories>
        <category>ROS</category>
      </categories>
      <tags>
        <tag>ROS</tag>
        <tag>QA</tag>
      </tags>
  </entry>
  <entry>
    <title>Gazebo模型</title>
    <url>/ros-gazebo-model/</url>
    <content><![CDATA[<figure class="highlight http"><table><tr><td class="code"><pre><span class="line"><span class="attribute">https://bitbucket.org/osrf/gazebo_models/downloads/</span></span><br></pre></td></tr></table></figure><p>下载并解压缩后放到/home/taowenyin/.gazebo/models目录下</p>]]></content>
      <categories>
        <category>ROS</category>
      </categories>
      <tags>
        <tag>ROS</tag>
        <tag>Gazebo</tag>
      </tags>
  </entry>
  <entry>
    <title>ROS键盘检测</title>
    <url>/ros-keyboard/</url>
    <content><![CDATA[<blockquote>
<ul>
<li>1、进入官网下载keyboard_reader（<a href="https://github.com/UTNuclearRoboticsPublic/keyboard_reader" target="_blank" rel="noopener">官网</a>）。</li>
</ul>
</blockquote><blockquote>
<ul>
<li>2、复制keyboard_reader文件夹到ROS工作空间的src目录中。</li>
</ul>
</blockquote><blockquote>
<ul>
<li>3、修改keyboard_reader.cpp文件中的第118行内容。</li>
</ul>
</blockquote><blockquote>
<ul>
<li>4、修改keyboard_reader.cpp文件中的第51行内容，以选择正确的键盘。</li>
</ul>
</blockquote><a id="more"></a>



<blockquote>
<ul>
<li>5、在ROS工作空间中执行catkin_make指令进行编译。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>6、keyboard_reader项目的做法主要是创建一个“keyboard”话题，并发送自定义的Key.msg消息</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>7、创建订阅。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>8、修改package.xml，添加keyboard_reader依赖。</li>
</ul>
</blockquote>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">build_depend</span>&gt;</span>keyboard_reader<span class="tag">&lt;/<span class="name">build_depend</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">build_export_depend</span>&gt;</span>keyboard_reader<span class="tag">&lt;/<span class="name">build_export_depend</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">exec_depend</span>&gt;</span>keyboard_reader<span class="tag">&lt;/<span class="name">exec_depend</span>&gt;</span></span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>9、修改CMakeLists.txt，添加keyboard_reader依赖。</li>
</ul>
</blockquote>
<figure class="highlight makefile"><table><tr><td class="code"><pre><span class="line">find_package(catkin REQUIRED COMPONENTS</span><br><span class="line">  roscpp</span><br><span class="line">  rospy</span><br><span class="line">  std_msgs</span><br><span class="line">  keyboard_reader <span class="comment"># 添加的内容</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加MSG依赖</span></span><br><span class="line">add_dependencies(&lt;package_name&gt; &lt;depend_package_name&gt;_generate_messages_cpp)</span><br><span class="line">add_dependencies(ros_keyboard keyboard_reader_generate_messages_cpp)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>ROS</category>
      </categories>
      <tags>
        <tag>ROS</tag>
        <tag>调试</tag>
      </tags>
  </entry>
  <entry>
    <title>ROS节点调试</title>
    <url>/ros-debug/</url>
    <content><![CDATA[<blockquote>
<ul>
<li>1、将下面两行加入到CMakeLists.txt中</li>
</ul>
</blockquote><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">set</span> (CMAKE_CXX_FLAGS <span class="string">"<span class="variable">$&#123;CMAKE_CXX_FLAGS&#125;</span> -g"</span>)</span><br><span class="line"><span class="built_in">set</span> (CMAKE_VERBOSE_MAKEFILE ON)</span><br></pre></td></tr></table></figure><blockquote>
<ul>
<li>2、修改launch.json中的program</li>
</ul>
</blockquote><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="variable">$&#123;workspaceFolder&#125;</span>/devel/lib/&lt;package_name&gt;/&lt;node_name&gt;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>ROS</category>
      </categories>
      <tags>
        <tag>ROS</tag>
        <tag>调试</tag>
      </tags>
  </entry>
  <entry>
    <title>ROS串口标准库serial的编译与使用</title>
    <url>/ros-serial-compile/</url>
    <content><![CDATA[<p><a href="http://wjwwood.io/serial/" target="_blank" rel="noopener">参考链接</a></p><blockquote>
<ul>
<li>1、下载serial库</li>
</ul>
</blockquote><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/wjwwood/serial.git</span><br></pre></td></tr></table></figure><blockquote>
<ul>
<li>2、编译serial库</li>
</ul>
</blockquote><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install doxygen</span><br><span class="line"><span class="built_in">cd</span> serial</span><br><span class="line">make</span><br><span class="line">make <span class="built_in">test</span></span><br><span class="line">make docs</span><br><span class="line">make install</span><br><span class="line">make uninstall（卸载）</span><br></pre></td></tr></table></figure><a id="more"></a>




<blockquote>
<ul>
<li>3、复制到ros库</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cp -R /tmp/usr/<span class="built_in">local</span>/include/ /opt/ros/kinetic/</span><br><span class="line">cp -R /tmp/usr/<span class="built_in">local</span>/lib/ /opt/ros/kinetic/</span><br><span class="line">cp -R /tmp/usr/<span class="built_in">local</span>/share/ /opt/ros/kinetic/</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>4、创建serial依赖的功能包</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">catkin_create_pkg &lt;package_name&gt; serial std_msgs roscpp rospy</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>ROS</category>
      </categories>
      <tags>
        <tag>ROS</tag>
        <tag>串口通信</tag>
      </tags>
  </entry>
  <entry>
    <title>rosserial-stm32的编译与安装</title>
    <url>/ros-serial-stm32/</url>
    <content><![CDATA[<p><a href="https://blog.csdn.net/m0_38089090/article/details/79870815" target="_blank" rel="noopener">参考链接1</a></p><p><a href="https://blog.csdn.net/qq_37416258/article/details/84844051" target="_blank" rel="noopener">参考链接2</a></p><blockquote>
<ul>
<li>1、安装ROS串口库，指令如下。</li>
</ul>
</blockquote><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install ros-kinetic-rosserial</span><br></pre></td></tr></table></figure><blockquote>
<ul>
<li>2、进入官网下载rosserial_stm32（<a href="http://wiki.ros.org/rosserial" target="_blank" rel="noopener">官网</a>）。</li>
</ul>
</blockquote><blockquote>
<ul>
<li>3、把下载后的文件解压并重命名为rosserial_stm32，进入ROS的工作空间。</li>
</ul>
</blockquote><a id="more"></a>





<blockquote>
<ul>
<li>4、复制rosserial_stm32文件夹到ROS工作空间的src目录中。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>5、在ROS工作空间中执行catkin_make指令进行编译。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>【可能需要的步骤】执行安装指令，并载入环境安装目录中的环境变量，指令如下。</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">catkin_make install</span><br><span class="line"><span class="built_in">source</span> ROS工作空间/install/setup.bash</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>6、进入STM32CubeMx创建的项目目录，并执行如下指令。</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rosrun rosserial_stm32 make_libraries.py .</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>7、修改Inc目录下的STM32Hardware.h文件，并把头文件中的“stm32f3xx_halxxx.h”修改为“stm32f1xx_halxxx.h”。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>8、修改main.c-&gt;main.cpp。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>9、修改CMakeLists.txt中的PROJECT(HelloSTM32ROS C CXX ASM)为PROJECT(HelloSTM32ROS CXX C ASM)。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>10、修改Inc目录下的STM32Hardware.h文件中的定时器和串口对象。</li>
</ul>
</blockquote>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">extern</span> TIM_HandleTypeDef htim2;</span><br><span class="line"><span class="keyword">extern</span> UART_HandleTypeDef huart1;</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>11、添加roscore文件夹和mainpp.cpp、mainpp.h，并完成相应内容。（注：文件名和文件夹名可以自定义）</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>12、使用该库需要启动rosserial作为数据传递的中介。</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rosrun rosserial_python serial_node.py</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>ROS</category>
      </categories>
      <tags>
        <tag>ROS</tag>
        <tag>串口通信</tag>
      </tags>
  </entry>
  <entry>
    <title>Anaconda环境下的ROS配置</title>
    <url>/ros-anaconda/</url>
    <content><![CDATA[<p>1、安装Anaconda</p><p>2、创建Python2.7的环境catkin_ws（可以自行修改成需要的名字）</p><p>3、执行环境切换命令<code>source activate catkin_ws</code></p><p>4、安装相关依赖<code>pip install -U rospkg</code></p><p>5、在PyCharm中添加ros-python</p><p>（1）File-&gt;Settings-&gt;Project Interpreter-&gt;右边小齿轮-&gt;Show All-&gt;选中正在使用的python</p><a id="more"></a>





<p>（2）点击右边最下面的图标，打开Interpreter Path</p>
<p>（3）添加<code>/opt/ros/indigo/lib/python2.7/dist-packages</code></p>
<p>6、编写代码，所有的Python顶部必须添加<code>#!/usr/bin/env python</code></p>
]]></content>
      <categories>
        <category>ROS</category>
      </categories>
      <tags>
        <tag>ROS</tag>
        <tag>环境配置</tag>
      </tags>
  </entry>
  <entry>
    <title>优化理论和应用</title>
    <url>/optimization-theory-applications/</url>
    <content><![CDATA[<h1 id="介绍">介绍</h1><h2 id="优化问题">优化问题</h2><p><span class="math display">\[\begin{matrix}
    minimize &amp; f_{0}\left ( \mathbf{x} \right ) &amp; \\ 
    st. &amp; f_{i}\left ( \mathbf{x} \right ) \leq b_{i} &amp; i=1,\cdots ,M\\ 
    variables &amp; \mathbf{x} &amp; 
\end{matrix}\]</span></p><a id="more"></a>


<p><span class="math inline">\(\mathbf{x}=\left(x_{1}, \dots, x_{n}\right)^{\top}\)</span>：优化变量</p>
<p><span class="math inline">\(f_{0}: \mathbb{R}^{n} \rightarrow \mathbb{R}\)</span>：目标函数</p>
<p><span class="math inline">\(f_{i}: \mathbb{R}^{n} \rightarrow \mathbb{R}, i=1, \ldots, M\)</span>：约束函数</p>
<p>最优解：在满足约束的情况下，<span class="math inline">\(\mathbf{X}^{\star}\)</span>是<span class="math inline">\(f_{0}\)</span>中所有向量的最小值。</p>
<p>可以高效可靠解决的问题：</p>
<blockquote>
<ul>
<li>1、线性规划问题</li>
<li>2、最小二乘问题</li>
<li>3、凸优化问题</li>
</ul>
</blockquote>
<h2 id="线性规划问题">线性规划问题</h2>
<p><span class="math display">\[\begin{matrix}
    minimize &amp; \mathbf{c}^{\top} \mathbf{x} &amp; \\ 
    st. &amp; \mathbf{a}_{i}^{\top} \mathbf{x} \leq b_{i} &amp; i=1,\cdots ,M\\ 
    variables &amp; \mathbf{x} &amp; 
\end{matrix}\]</span></p>
<h3 id="整数线性规划问题">整数线性规划问题</h3>
<p><span class="math display">\[\begin{matrix}
    minimize &amp; \mathbf{c}^{\top} \mathbf{x} &amp; \\ 
    st. &amp; \mathbf{a}_{i}^{\top} \mathbf{x} \leq b_{i} &amp; i=1,\cdots ,M\\ 
    &amp; \mathbf{x} \in \mathbb{Z}^{n} &amp; \\ 
    variables &amp; \mathbf{x} &amp; 
\end{matrix}\]</span></p>
<h2 id="最小二乘问题">最小二乘问题</h2>
<p><span class="math display">\[\begin{matrix}
    minimize &amp; \left \| \mathbf{A}\mathbf{x}-\mathbf{b} \right \|_{2}^{2} &amp; \\ 
    variables &amp; \mathbf{x} &amp; 
\end{matrix}\]</span></p>
<h2 id="凸优化问题">凸优化问题</h2>
<p><span class="math display">\[\begin{matrix}
    minimize &amp; f_{0}(\mathbf{x}) &amp; \\ 
    st. &amp; f_{i}(\mathbf{x}) \leq b_{i} &amp; i=1,\cdots ,M\\ 
    variables &amp; \mathbf{x} &amp; 
\end{matrix}\]</span></p>
<blockquote>
<ul>
<li>目标函数和约束函数都是凸函数。</li>
</ul>
</blockquote>
<p><span class="math inline">\(f_{i}(\alpha \mathbf{x}+\beta \mathbf{y}) \leq \alpha f_{i}(\mathbf{x})+\beta f_{i}(\mathbf{y})\)</span>， 假如<span class="math inline">\(\alpha+\beta=1, \alpha \geq 0, \beta \geq 0\)</span></p>
<blockquote>
<ul>
<li>最小二乘和特殊情况下的线性规划是凸优化问题。</li>
</ul>
</blockquote>
<h2 id="优化模型分类">优化模型分类</h2>
<blockquote>
<ul>
<li>1、按照解决方法：数值优化和分析优化。</li>
<li>2、按照约束条件：无约束优化和有约束优化，其中有约束优化分为集约束优化、等式约束优化、不等式约束优化。</li>
<li>3、按照信息完整性：正态优化和信息不确定性的优化，其中信息不确定性的优化分为随机优化和鲁棒优化。</li>
<li>4、按照问题属性：线性规划和非线性规划，其中非线性规划分为凸优化、二次规划、一般非线性规划。</li>
<li>5、按照目标：单目标优化和多目标优化。</li>
</ul>
</blockquote>
<h1 id="基础知识">基础知识</h1>
<h2 id="符号说明">符号说明</h2>
<p>1、<span class="math inline">\(\mathbb{X}\)</span>是一个集合，<span class="math inline">\(x \in \mathbb{X}\)</span>表示<span class="math inline">\(x\)</span>是<span class="math inline">\(\mathbb{X}\)</span>中的元素。</p>
<p>2、<span class="math inline">\(\{x 1, x 2, x 3, \ldots\}\)</span>和<span class="math inline">\(\{x: x \in \mathbb{R}, x&gt;5\}\)</span>都可以表示一个集合。</p>
<p>3、<span class="math inline">\(\mathbb{X}\)</span>和<span class="math inline">\(\mathbb{Y}\)</span>都是集合，如果<span class="math inline">\(\mathbb{X} \subset \mathbb{Y}\)</span>，那么表示<span class="math inline">\(\mathbb{X}\)</span>是<span class="math inline">\(\mathbb{Y}\)</span>的子集。</p>
<p>4、<span class="math inline">\(f: \mathbb{X} \rightarrow \mathbb{Y}\)</span>表示<span class="math inline">\(f\)</span>是从集合<span class="math inline">\(\mathbb{X}\)</span>到<span class="math inline">\(\mathbb{Y}\)</span>的函数。</p>
<p>5、<span class="math inline">\(\overset{\triangle}{=}\)</span>表示“定义为”。</p>
<p>6、假设定义n阶向量<span class="math inline">\(\mathbf{a}=\left[\begin{array}{c}{a_{1}} \\ {a_{2}} \\ {\vdots} \\ {a_{n}}\end{array}\right]\)</span>，那么<span class="math inline">\(\mathbf{a}^{\top}=\left[a_{1}, a_{2}, \ldots, a_{n}\right]\)</span>。</p>
<h2 id="线性相关">线性相关</h2>
<p>向量集<span class="math inline">\(\left \{ \mathbf{a_{1}},\mathbf{a_{2}},\cdots ,\mathbf{a_{k}} \right \}\)</span>是线性相关，当且仅当集合中的一个向量可以表示为其他向量的线性组合。</p>
<p>必要条件：<span class="math inline">\(\alpha_{1}\mathbf{a_{1}}+\alpha_{2}\mathbf{a_{2}}+\cdots +\alpha_{k}\mathbf{a_{k}}=\mathbf{0}\)</span>，并且其中至少有一个<span class="math inline">\(\alpha_{i} \neq 0\)</span>，否则就是不相关。</p>
<p>充分条件：<span class="math inline">\(\left ( -1 \right )\mathbf{a_{1}}+\alpha_{2}\mathbf{a_{2}}+\cdots +\alpha_{k}\mathbf{a_{k}}=\mathbf{0}\)</span></p>
<h2 id="二次型函数">二次型函数</h2>
<p><span class="math display">\[f\left ( \mathbf{x} \right )=\mathbf{x}^{\top}\mathbf{Q}\mathbf{x}\]</span></p>
<p>对于任意非零向量<span class="math inline">\(\mathbf{x}\)</span>，都有<span class="math inline">\(\mathbf{x}^{\top}\mathbf{Q}\mathbf{x} &gt; 0\)</span>，并且如果<span class="math inline">\(\mathbf{Q}=\mathbf{Q}^{T}\)</span>，那么<span class="math inline">\(\mathbf{Q}\)</span>是正定的。如果<span class="math inline">\(\mathbf{x}^{\top}\mathbf{Q}\mathbf{x} \geq 0\)</span>，那么<span class="math inline">\(\mathbf{Q}\)</span>是半正定的。相反，还是负定和半负定。</p>
<h3 id="二次规划">二次规划</h3>
<p><span class="math display">\[\begin{matrix}
    minimize &amp; \mathbf{x}^{\top}\mathbf{Q}\mathbf{x} + \mathbf{c}^{\top}\mathbf{x} \\ 
    st. &amp; \mathbf{Ax}\leq \mathbf{b}\\ 
\end{matrix}\]</span></p>
<h3 id="线性规划最优控制">线性规划最优控制</h3>
<p><span class="math display">\[\begin{matrix}
    minimize &amp; J\left(u, x_{0}, t_{0}, t_{f}\right)=\int_{t_{0}}^{t_{f}}\left[x^{T}(t) Q(t) x(t)+u^{T}(t) R(t) u(t)\right] d t+x\left(t_{f}\right)^{T} S x\left(t_{f}\right) &amp; \\ 
    st. &amp; \dot{x}=A(t) x+B(t) u(t)\\ 
\end{matrix}\]</span></p>
<h2 id="矩阵">矩阵</h2>
<h3 id="矩阵范数">矩阵范数</h3>
<p><span class="math display">\[\left \| \mathbf{A} \right \|_{F}=\left ( \sum_{i=1}^{m}\sum_{j=1}^{n}\left ( a_{ij} \right )^{2} \right )^{\frac{1}{2}}\]</span></p>
<h3 id="线性方程">线性方程</h3>
<p>假设有矩阵$<span class="math inline">\(\mathbf{A}=\left[\mathbf{a}_{1}, \mathbf{a}_{2}, \ldots, \mathbf{a}_{n}\right]\)</span>，那么就可以用<span class="math inline">\(\mathbf{A x}=\mathbf{b}\)</span>表示一个线性方程。</p>
<h3 id="内积">内积</h3>
<p>假设<span class="math inline">\(\mathbf{x}, \mathbf{y} \in \mathbb{R}^{n}\)</span>，那么内积就可以表示为<span class="math inline">\(\left \langle \mathbf{x}, \mathbf{y} \right \rangle=\sum_{i=1}^{n} x_{i} y_{i}=\mathbf{x}^{T} \mathbf{y}\)</span></p>
<h3 id="特征值和特征矩阵">特征值和特征矩阵</h3>
<p>令<span class="math inline">\(\mathbf{A}\)</span>是<span class="math inline">\(n \times n\)</span>的矩阵，<span class="math inline">\(\lambda\)</span>是一个标量，<span class="math inline">\(\mathbf{v}\)</span>是一个非0矩阵，如果<span class="math inline">\(\mathbf{Av}=\lambda \mathbf{v}\)</span>，那么<span class="math inline">\(\lambda\)</span>就是特征值，就是特征矩阵。</p>
<h2 id="几何概念">几何概念</h2>
<h3 id="线段">线段</h3>
<p>线段的定义：<span class="math inline">\(\mathbf{z}=\alpha \mathbf{x}+(1-\alpha) \mathbf{y}\)</span>，并且<span class="math inline">\(\alpha \in \left [ 0,1 \right ]\)</span>。</p>
<h3 id="超平面">超平面</h3>
<p>假设<span class="math inline">\(u_{1}, u_{2}, \ldots, u_{n}, v \in \mathbb{R}\)</span>，其中至少有一个<span class="math inline">\(u_{i}\)</span>是非零，并且所有点的集合<span class="math inline">\(\mathbf{x}=\left[x_{1}, x_{2}, \ldots, x_{n}\right]^{T}\)</span>组成的线性方程</p>
<p><span class="math display">\[u_{1} x_{1}+u_{2} x_{2}+\ldots+u_{n} x_{n}=v\]</span></p>
<p>称为超平面。</p>
<p>当<span class="math inline">\(n=2\)</span>时，超平面为一条直线。</p>
<p>当<span class="math inline">\(n=3\)</span>时，超平面为一个平面。</p>
<p>即超平面为源空间维度的<span class="math inline">\(n-1\)</span>。</p>
<p>当<span class="math inline">\(u_{1} x_{1}+u_{2} x_{2}+\ldots+u_{n} x_{n} \geq v\)</span>，表示正半空间。<span class="math inline">\(H_{+}=\left\{\mathbf{x} \in \mathbb{R}^{n}: \mathbf{u}^{T} \mathbf{x} \geq v\right\}\)</span>。</p>
<p>当<span class="math inline">\(u_{1} x_{1}+u_{2} x_{2}+\ldots+u_{n} x_{n} \leq v\)</span>，表示正半空间。<span class="math inline">\(H_{-}=\left\{\mathbf{x} \in \mathbb{R}^{n}: \mathbf{u}^{T} \mathbf{x} \leq v\right\}\)</span>。</p>
<h3 id="凸集">凸集</h3>
<p>假设<span class="math inline">\(\Theta \in \mathbb{R}^{n}\)</span>是一个集合，那么如果集合中的任意两个向量<span class="math inline">\(\mathbf{u}\)</span>和<span class="math inline">\(\mathbf{v}\)</span>组成的线段中的点也在集合中，那么该集合为凸集，即<span class="math inline">\(\alpha \mathbf{u}+(1-\alpha) \mathbf{v} \in \Theta\)</span>，且<span class="math inline">\(\alpha \in[0,1]\)</span>。</p>
<p>凸集的例子：空集、由一个点组成的集合、一条直线和线段、子空间、超平面、半空间、<span class="math inline">\(\mathbb{R}^{n}\)</span>。</p>
<h3 id="凸包">凸包</h3>
<p><span class="math inline">\(\textbf{conv} \mathbb{C}=\left \{ \theta_{1} \mathbf{x}_{1}+\cdots+\theta_{k} \mathbf{x}_{k} \mid \mathbf{x}_{i} \in \mathbb{C}, \theta_{i} \geq 0, i=1, \ldots, k,\theta_{1}+\cdots+\theta_{k}=1 \right \}\)</span></p>
<h3 id="领域">领域</h3>
<p>点<span class="math inline">\(\mathbf{x} \in \mathbb{R}^{n}\)</span>的邻域可以定义为<span class="math inline">\(\left\{\mathbf{y} \in \mathbb{R}^{n}:\|\mathbf{y}-\mathbf{x}\|&lt;\varepsilon\right\}\)</span>，其中<span class="math inline">\(\varepsilon\)</span>为正值。</p>
<h2 id="微积分">微积分</h2>
<p>1、给定<span class="math inline">\(f: \mathbb{R}^{n} \rightarrow \mathbb{R}\)</span>，那么<span class="math inline">\(f\)</span>的梯度是一个函数<span class="math inline">\(\nabla f: \mathbb{R}^{n} \rightarrow \mathbb{R}^{n}\)</span>，记作<span class="math inline">\(\nabla f(\mathbf{x})=\left[\begin{array}{c}{\frac{\partial f}{\partial x_{1}}(\mathbf{x})} \\ {\vdots} \\ {\frac{\partial f}{\partial x_{n}}(\mathbf{x})}\end{array}\right]\)</span>，其中<span class="math inline">\(\nabla f(\mathbf{x})\)</span>是一个在<span class="math inline">\(\mathbb{R}^{n}\)</span>中的向量，记作<span class="math inline">\(\nabla f(\mathbf{x})=D f(\mathbf{x})^{\top}\)</span>。</p>
<p>2、给定<span class="math inline">\(f: \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}\)</span>，<span class="math inline">\(f=\left[f_{1}, \ldots, f_{m}\right]^{\top}\)</span>，那么<span class="math inline">\(f\)</span>的微分方程<span class="math inline">\(D f: \mathbb{R}^{n} \rightarrow \mathbb{R}^{m \times n}\)</span>为<span class="math inline">\(D f(x)=\left[\begin{array}{ccc}{\frac{\partial f_{1}}{\partial x_{1}}(\mathbf{x})} &amp; {\dots} &amp; {\frac{\partial f_{1}}{\partial x_{n}}(\mathbf{x})} \\ {\vdots} &amp; {} &amp; {\vdots} \\ {\frac{\partial f_{m}}{\partial x_{1}}(\mathbf{x})} &amp; {\cdots} &amp; {\frac{\partial f_{m}}{\partial x_{n}}(\mathbf{x})}\end{array}\right]\)</span>。</p>
<p>3、如果<span class="math inline">\(f\)</span>是二阶可微，那么<span class="math inline">\(\mathbf{F}=D^{2} f=\left[\begin{array}{cccc}{\frac{\partial^{2} f}{\partial x_{1}^{2}}} &amp; {\frac{\partial^{2} f}{\partial x_{2} \partial x_{1}}} &amp; {\cdots} &amp; {\frac{\partial^{2} f}{\partial x_{n} \partial x_{1}}} \\ {\frac{\partial^{2} f}{\partial x_{1} \partial x_{2}}} &amp; {\frac{\partial^{2} f}{\partial x_{2}^{2}}} &amp; {\cdots} &amp; {\frac{\partial^{2} f}{\partial x_{n} \partial x_{1}}} \\ {\vdots} &amp; {\vdots} &amp; {\ddots} &amp; {\vdots} \\ {\frac{\partial^{2} f}{\partial x_{1} \partial x_{n}}} &amp; {\frac{\partial^{2} f}{\partial x_{2} \partial x_{n}}} &amp; {\cdots} &amp; {\frac{\partial^{2} f}{\partial x_{n}^{2}}}\end{array}\right]\)</span>，该矩阵称为<span class="math inline">\(f\)</span>的黑塞矩阵（Hessian）。</p>
<h2 id="水平集和梯度">水平集和梯度</h2>
<p><span class="math inline">\(\nabla f\left(\mathbf{x}_{0}\right)\)</span>是<span class="math inline">\(f\)</span>在<span class="math inline">\(\mathbf{x}_{0}\)</span>点处最大增长率方向。</p>
<h1 id="无约束优化问题">无约束优化问题</h1>
<h2 id="局部极小点x">局部极小点x</h2>
<p><span class="math inline">\(\mathbf{x}^{\ast}\)</span>是局部极小点的条件是<span class="math inline">\(\nabla f\left(\mathbf{x}^{\ast}\right)=0\)</span>。</p>
<h2 id="可行方向">可行方向</h2>
<p>如果存在<span class="math inline">\(\alpha_{0}&gt;0\)</span>，并且满足<span class="math inline">\(\alpha \in\left[0, \alpha_{0}\right]\)</span>，使得<span class="math inline">\(\mathbf{x}+\alpha \mathbf{d} \in \Omega\)</span>，那么<span class="math inline">\(\mathbf{d} \in \mathbb{R}^{n}, \mathbf{d} \neq 0\)</span>就是<span class="math inline">\(\mathbf{x} \in \Omega\)</span>中的可行方向。</p>
<h3 id="方向导数">方向导数</h3>
<p>当可行方向<span class="math inline">\(\mathbf{d}\)</span>是单位向量时，那么函数<span class="math inline">\(f\)</span>在<span class="math inline">\(\mathbf{x}\)</span>处沿方向<span class="math inline">\(\mathbf{d}\)</span>的增长率可以用内积表示，即<span class="math inline">\(\frac{\partial f}{\partial \mathbf{d}}(\mathbf{x})=\mathbf{d}^{\top} \nabla f(\mathbf{x})=\nabla f(\mathbf{x})^{\top} \mathbf{d}=\left \langle \nabla f(\mathbf{x}), \mathbf{d} \right \rangle\)</span>。</p>
<h3 id="fonc一阶必要条件">FONC（一阶必要条件）</h3>
<p>如果<span class="math inline">\(\mathbf{x}^{\ast}\)</span>是函数<span class="math inline">\(f\)</span>在<span class="math inline">\(\Omega\)</span>上的局部极小点，那么对于<span class="math inline">\(\mathbf{x}^{\ast}\)</span>处的任意可行方向<span class="math inline">\(\mathbf{d}\)</span>都有<span class="math inline">\(\mathbf{d}^{T} \nabla f\left(\mathbf{x}^{\ast}\right) \geq 0\)</span>。</p>
<h3 id="sonc二阶必要条件">SONC（二阶必要条件）</h3>
<p>如果<span class="math inline">\(f\)</span>在约束集<span class="math inline">\(\Omega\)</span>上二阶连续可导，并且<span class="math inline">\(\mathbf{x}^{\ast}\)</span>是函数<span class="math inline">\(f\)</span>在<span class="math inline">\(\Omega\)</span>上的局部极小点，<span class="math inline">\(\mathbf{d}\)</span>是<span class="math inline">\(\mathbf{x}^{\ast}\)</span>处的可行方向，并且<span class="math inline">\(\mathbf{d}^{T} \nabla f\left(\mathbf{x}^{\ast}\right) = 0\)</span>，则<span class="math inline">\(\mathbf{d}^{T} F\left(\mathbf{x}^{\ast}\right) \mathbf{d} \geq 0\)</span>，其中<span class="math inline">\(F\)</span>为函数<span class="math inline">\(f\)</span>的黑塞矩阵。</p>
<h3 id="sosc二阶充分条件">SOSC（二阶充分条件）</h3>
<p>如果<span class="math inline">\(f\)</span>在约束集<span class="math inline">\(\Omega\)</span>上二阶连续可导，如果同时满足<span class="math inline">\(\nabla f\left(\mathbf{x}^{\ast}\right)=0\)</span>和<span class="math inline">\(F\left(\mathbf{x}^{\ast}\right)&gt;0\)</span>，那么<span class="math inline">\(\mathbf{X}^{\ast}\)</span>为严格局部极小点。</p>
<h1 id="一维搜索方法">一维搜索方法</h1>
<h2 id="介绍-1">介绍</h2>
<p>该方法主要使用迭代搜索算法或线搜法，基本流程是：</p>
<blockquote>
<ul>
<li>1、赋一个初值<span class="math inline">\(x^{\left ( 0 \right )}\)</span>。</li>
<li>2、生成迭代序列<span class="math inline">\(x^{\left ( 1 \right )},x^{\left ( 2 \right )},\cdots\)</span>。</li>
<li>3、每次迭代，下一个<span class="math inline">\(x^{\left ( k+1 \right )}\)</span>依赖于上一个<span class="math inline">\(x^{\left ( k \right )}\)</span>。</li>
</ul>
</blockquote>
<p>目标函数<span class="math inline">\(f\)</span>，一阶导数<span class="math inline">\({f}&#39;\)</span>，二阶导数<span class="math inline">\({f}&#39;&#39;\)</span>。</p>
<p>常用的一维搜索算法：</p>
<blockquote>
<ul>
<li>1、黄金分割法。（只使用目标函数<span class="math inline">\(f\)</span>）</li>
<li>2、斐波那契数列方法。（只使用目标函数<span class="math inline">\(f\)</span>）</li>
<li>3、二分法。（只使用目标函数的一阶导数<span class="math inline">\({f}&#39;\)</span>）</li>
<li>4、割线法。（只使用目标函数的一阶导数<span class="math inline">\({f}&#39;\)</span>）</li>
<li>5、牛顿法。（只使用目标函数的一阶导数<span class="math inline">\({f}&#39;\)</span>和二阶导数<span class="math inline">\({f}&#39;&#39;\)</span>）</li>
</ul>
</blockquote>
<h2 id="泰勒公式">泰勒公式</h2>
<p>泰勒公式是许多数值方法和优化模型的基础。</p>
<p>假设<span class="math inline">\(f: \mathbb{R} \rightarrow \mathbb{R}\)</span>在区间<span class="math inline">\([a, b]\)</span>上是<span class="math inline">\(m\)</span>次连续可微函数。假设<span class="math inline">\(h=[b-a]\)</span>，那么<span class="math inline">\(f\)</span>的第<span class="math inline">\(i\)</span>阶导数<span class="math inline">\(f^{(i)}\)</span>为</p>
<p><span class="math display">\[f(b)=f(a)+\frac{h}{1 !} f^{(1)}(a)+\frac{h^{2}}{2 !} f^{(2)}(a)+\ldots+\frac{h^{m-1}}{(m-1) !} f^{(m-1)}(a)+R_{m}\]</span></p>
<p>其中<span class="math inline">\(R_{m}\)</span>为余项</p>
<p><span class="math display">\[R_{m}=\frac{h^{m}(1-\theta)^{m-1}}{(m-1) !} f^{(m)}(a+\theta h)=\frac{h^{m}}{m !}\left(a+\theta^{\prime} h\right)\]</span></p>
<p>其中<span class="math inline">\(\theta, \theta^{\prime} \in(0,1)\)</span>。</p>
<h2 id="牛顿法牛顿切线法">牛顿法（牛顿切线法）</h2>
<p>牛顿法主要有两方面的应用</p>
<p>1、求方程根：使用一阶导数求实值函数的根。</p>
<p>2、优化：使用一阶和二阶导数求一个实值函数优化器的逼近。</p>
<blockquote>
<p>1、目标：</p>
</blockquote>
<p>求<span class="math inline">\(x^{\ast}\)</span>，使得<span class="math inline">\(g\left(x^{\ast}\right)=0\)</span></p>
<blockquote>
<p>2、核心思想：</p>
</blockquote>
<p>基于泰勒公式展开，构建新的函数来逼近原函数。</p>
<p><span class="math display">\[f(x)=g\left(x_{0}\right)+\left(x-x_{0}\right) g^{\prime}\left(x_{0}\right)\]</span></p>
<p>由于<span class="math inline">\(f(x)\)</span>是<span class="math inline">\(g\left(x\right)\)</span>的近似。因此，要求的<span class="math inline">\(f(x)\)</span>极小点，那么就要满足一阶必要条件，即<span class="math inline">\(f(x)=0\)</span>，那么上式就可以得到</p>
<p><span class="math display">\[x=x_{0}-\frac{g\left(x_{0}\right)}{g^{\prime}\left(x_{0}\right)}\]</span></p>
<p>从而得到整个迭代过程为</p>
<p><span class="math display">\[x_{k+1}=x_{k}-\frac{g\left(x_{k}\right)}{g^{\prime}\left(x_{k}\right)}\]</span></p>
<blockquote>
<p>3、几何角度</p>
</blockquote>
<p>几何上来说，就是找到<span class="math inline">\(x^{\left ( k \right )}\)</span>出的切线与<span class="math inline">\(x\)</span>轴的交点作为<span class="math inline">\(x^{\left ( k+1 \right )}\)</span>，然后再以<span class="math inline">\(x^{\left ( k+1 \right )}\)</span>点的切线找到<span class="math inline">\(x^{\left ( k+2 \right )}\)</span>，一次类推最终逼近极值点。</p>
<blockquote>
<p>4、注意事项</p>
</blockquote>
<p>如果<span class="math inline">\(\frac{g\left(x_{k}\right)}{g^{\prime}\left(x_{k}\right)}\)</span>的比值不是足够小时，牛顿法可能就会失效，因为会错过极小值，所以初始点的选择很重要。</p>
<h2 id="牛顿优化法">牛顿优化法</h2>
<p>构建一个在<span class="math inline">\(x^{(k)}\)</span>点的原函数、一阶函数和二界函数，并且构建一个逼近于原函数<span class="math inline">\(f(x)\)</span>的近似函数</p>
<p><span class="math display">\[q(x)=f\left(x^{(k)}\right)+f^{\prime}\left(x^{(k)}\right)\left(x-x^{(k)}\right)+\frac{1}{2} f^{\prime \prime}\left(x^{(k)}\right)\left(x-x^{(k)}\right)^{2}\]</span></p>
<p>其中<span class="math inline">\(q(x)\)</span>与<span class="math inline">\(f(x)\)</span>的一阶和二阶导数相比配</p>
<p><span class="math inline">\(q\left(x^{(k)}\right)=f\left(x^{(k)}\right)\)</span></p>
<p><span class="math inline">\(q^{\prime}\left(x^{(k)}\right)=f^{\prime}\left(x^{(k)}\right)\)</span></p>
<p><span class="math inline">\(q^{\prime \prime}\left(x^{(k)}\right)=f^{\prime \prime}\left(x^{(k)}\right)\)</span></p>
<p>原先去<span class="math inline">\(f\)</span>的最小值，现在去近似函数<span class="math inline">\(q\)</span>的最小值。</p>
<p>按照一阶必要条件（FONC）可以得到</p>
<p><span class="math display">\[q^{\prime}(x)=f^{\prime}\left(x^{(k)}\right)+f^{\prime \prime}\left(x^{(k)}\right)\left(x-x^{(k)}\right)=0\]</span></p>
<p>令<span class="math inline">\(x=x^{(k+1)}\)</span>，那么就可以得到</p>
<p><span class="math display">\[x^{(k+1)}=x^{(k)}-\frac{f^{\prime}\left(x^{(k)}\right)}{f^{\prime \prime}\left(x^{(k)}\right)}\]</span></p>
<p>注意，对于区间内任何<span class="math inline">\(x\)</span>都有<span class="math inline">\(f^{\prime \prime}(x)&gt;0\)</span>，那么牛顿法能正常工作，但如果对于一些<span class="math inline">\(x\)</span>造成<span class="math inline">\(f^{\prime \prime}(x)&lt;0\)</span>，那么牛顿法可能就会收敛到极大点，而不是极小点。</p>
<h2 id="割线法">割线法</h2>
<p>如果函数二阶不可导，那么二阶导数就可以近似为</p>
<p><span class="math display">\[f^{\prime \prime}\left(x^{(k)}\right) \approx \frac{f^{\prime}\left(x^{(k)}\right)-f^{\prime}\left(x^{(k-1)}\right)}{x^{(k)}-x^{(k-1)}}\]</span></p>
<p>因此，就可以得到割线法公式</p>
<p><span class="math display">\[x^{(k+1)}=x^{(k)}-\frac{x^{(k)}-x^{(k-1)}}{f^{\prime}\left(x^{(k)}\right)-f^{\prime}\left(x^{(k-1)}\right)} f^{\prime}\left(x^{(k)}\right)\]</span></p>
<p>该方法需要有两个初始值，分别是<span class="math inline">\(x^{(k)}\)</span>和<span class="math inline">\(x^{(k-1)}\)</span>。</p>
<h2 id="多维优化问题中的一维搜索">多维优化问题中的一维搜索</h2>
<p>一维搜索方法在多维优化问题中扮演非常重要的角色。特别是对于多维优化问题的迭代求解算法，通常每次迭代都会包括一维搜索过程。</p>
<p>令目标函数<span class="math inline">\(f: \mathbb{R}^{n} \rightarrow \mathbb{R}\)</span>，寻找<span class="math inline">\(f\)</span>极小值的迭代算法为</p>
<p><span class="math display">\[\mathbf{x}^{(k+1)}=\mathbf{x}^{(k)}+\alpha_{k} \mathbf{d}^{(k)}\]</span></p>
<p>其中<span class="math inline">\(\mathbf{x}^{(0)}\)</span>为给定的初始点，<span class="math inline">\(\alpha_{k} \geq 0\)</span>为步长，其确定方式为使下面函数最小</p>
<p><span class="math display">\[\phi_{k}(\alpha)=f\left(\mathbf{x}^{(k)}+\alpha \mathbf{d}^{(k)}\right)\]</span></p>
<p>其中<span class="math inline">\(\mathbf{d}\)</span>为搜索方向。通过一维搜索确定<span class="math inline">\(\alpha_{k}\)</span>后，可以使得<span class="math inline">\(f\left(\mathbf{x}^{(k+1)}\right) &lt; f\left(\mathbf{x}^{(k)}\right)\)</span>。</p>
<h1 id="梯度方法">梯度方法</h1>
<p>常用的梯度方法</p>
<blockquote>
<p>梯度下降</p>
</blockquote>
<blockquote>
<ul>
<li>1、固定步长的梯度下降</li>
<li>2、最速梯度下降</li>
<li>3、随机梯度下降</li>
</ul>
</blockquote>
<blockquote>
<p>共轭梯度</p>
</blockquote>
<h2 id="介绍-2">介绍</h2>
<p>因为<span class="math inline">\(\mathbf{d}=\nabla f(\mathbf{x})\)</span>是增长率最大的方向，因此<span class="math inline">\(\mathbf{d}=-\nabla f(\mathbf{x})\)</span>就是下降最快的方向。</p>
<p>梯度下降算法的核心实现：</p>
<p><span class="math display">\[\mathbf{x}^{(k+1)}=\mathbf{x}^{(k)}-\alpha_{k} \nabla f\left(\mathbf{x}^{(k)}\right)\]</span></p>
<p>其中<span class="math inline">\(\alpha_{k}\)</span>是步长，<span class="math inline">\(\nabla f\left(\mathbf{x}^{(k)}\right)\)</span>是方向。</p>
<p>1、<span class="math inline">\(\alpha_{k}\)</span>选择的影响：</p>
<blockquote>
<p>当<span class="math inline">\(\alpha_{k}\)</span>太小，那么迭代的次数就会变多。</p>
</blockquote>
<blockquote>
<p>当<span class="math inline">\(\alpha_{k}\)</span>太大，那么就会在最优点之间来回震荡。</p>
</blockquote>
<p>2、<span class="math inline">\(\alpha_{k}\)</span>的选择有许多方法：</p>
<blockquote>
<p>可以在每次迭代中使<span class="math inline">\(\alpha_{k}\)</span>固定，也可以让<span class="math inline">\(\alpha_{k}\)</span>随迭代变化而变化。</p>
</blockquote>
<blockquote>
<p>动态计算<span class="math inline">\(\alpha_{k}\)</span></p>
</blockquote>
<p><span class="math display">\[\alpha_{k}=\arg \min _{\alpha \geq 0} f\left(\mathbf{x}^{(\mathbf{k})}-\alpha \nabla f\left(\mathbf{x}^{(k)}\right)\right)\]</span></p>
<h2 id="最速下降法">最速下降法</h2>
<blockquote>
<p>该方法时一个梯度法。</p>
</blockquote>
<blockquote>
<p>选择步长以实现目标函数在每个单独步骤中的最大减少量。</p>
</blockquote>
<p><span class="math display">\[\alpha_{k}=\arg \min _{\alpha \geq 0} f\left(\mathbf{x}^{(\mathbf{k})}-\alpha \nabla f\left(\mathbf{x}^{(\mathbf{k})}\right)\right)\]</span></p>
<p>最速下降法的基本过程：x</p>
<blockquote>
<ul>
<li>1、每一步以<span class="math inline">\(\mathbf{x}^{(\mathbf{k})}\)</span>开始。</li>
<li>2、通过线搜法沿着方向<span class="math inline">\(-\nabla f\left(\mathbf{x}^{(\mathbf{k})}\right)\)</span>获取最小的<span class="math inline">\(\mathbf{x}^{(\mathbf{k}+\mathbf{1})}\)</span>。</li>
</ul>
</blockquote>
<p>在最速下降法中，假如<span class="math inline">\(\left\{\mathbf{x}^{(\mathbf{k})}\right\}_{k=0}^{\infty}\)</span>是每次迭代得到的值，那么对于每个<span class="math inline">\(k\)</span>，都有<span class="math inline">\(\mathbf{x}^{(\mathbf{k}+\mathbf{1})}-\mathbf{x}^{(\mathbf{k})}\)</span>与<span class="math inline">\(\mathbf{x}^{(\mathbf{k}+2)}-\mathbf{x}^{(\mathbf{k}+1)}\)</span>正交。</p>
<p>当<span class="math inline">\(\mathbf{x}^{(\mathbf{k}+\mathbf{1})}=\mathbf{x}^{(\mathbf{k})}\)</span>，那么这种情况就是算法停止的条件。</p>
<blockquote>
<p>梯度收敛的总结</p>
</blockquote>
<p>1、如果目标函数<span class="math inline">\(f\)</span>是凸的，则通过满足Wolfe条件的线搜索来选择步长，则相应的梯度法是全局收敛的。</p>
<p>2、如果目标函数是二次型<span class="math inline">\(f(\mathbf{x})=\frac{1}{2} \mathbf{x}^{T} Q \mathbf{x}-\mathbf{b}^{T} \mathbf{x}\)</span>，并且<span class="math inline">\(Q\)</span>是正定的，那么最速梯度法是全局收敛。</p>
<p>3、如果目标函数是二次型<span class="math inline">\(f(\mathbf{x})=\frac{1}{2} \mathbf{x}^{T} Q \mathbf{x}-\mathbf{b}^{T} \mathbf{x}\)</span>，并且<span class="math inline">\(Q\)</span>是正定的，那么固定步长梯度法也是全局收敛，并且<span class="math inline">\(0&lt;\alpha&lt;\frac{2}{\lambda_{\max }(Q)}\)</span>。</p>
<p>给定一个序列<span class="math inline">\(\left\{\mathbf{x}^{(k)}\right\}\)</span>，该序列收敛到<span class="math inline">\(\mathbf{x}^{\ast}\)</span>。当<span class="math inline">\(\lim _{k \rightarrow \infty}\left\|\mathbf{x}^{(k)}-\mathbf{x}^{*}\right\|=0\)</span>时，那么收敛的阶数是<span class="math inline">\(p\)</span>，并且<span class="math inline">\(p \in \mathbb{R}\)</span>。</p>
<p>当<span class="math inline">\(p=1\)</span>（一阶收敛），并且<span class="math inline">\(\lim _{k \rightarrow \infty} \frac{\left\|\mathbf{x}^{(k+1)}-\mathbf{x}^{*}\right\|}{\left\|\mathbf{x}^{(k)}-\mathbf{x}^{*}\right\|^{p}}=1\)</span>，那么收敛是次线性的。</p>
<p>当<span class="math inline">\(p=1\)</span>（一阶收敛），并且<span class="math inline">\(\lim _{k \rightarrow \infty} \frac{\left\|\mathbf{x}^{(k+1)}-\mathbf{x}^{*}\right\|}{\left\|\mathbf{x}^{(k)}-\mathbf{x}^{*}\right\|^{p}}&lt;1\)</span>，那么收敛是线性的。</p>
<p>当<span class="math inline">\(p&gt;1\)</span>，那么收敛是超线性的。</p>
<p>当<span class="math inline">\(p=2\)</span>（二阶收敛），那么收敛是二次型的。</p>
<p>当<span class="math inline">\(p=3\)</span>（三阶收敛），那么收敛是立方的。</p>
<blockquote>
<p>例子</p>
</blockquote>
<p>假设<span class="math inline">\(x^{\left ( k \right )}=\frac{1}{k}\)</span>，并且<span class="math inline">\(x^{\left ( k \right )} \rightarrow 0\)</span>，那么<span class="math inline">\(\frac{\left|x^{(k+1)}\right|}{\left|x^{(k)}\right|^{p}}=\frac{\frac{1}{k+1}}{\frac{1}{k^{p}}}=\frac{k^{p}}{k+1}\)</span></p>
<p>当<span class="math inline">\(p &gt; 1\)</span>时，上式趋近于<span class="math inline">\(\infty\)</span>。</p>
<p>当<span class="math inline">\(p &lt; 1\)</span>时，上式收敛到0。</p>
<p>当<span class="math inline">\(p = 1\)</span>时，上式收敛到1。</p>
<p>因此，收敛的阶数为1。</p>
<h1 id="牛顿法">牛顿法</h1>
<h2 id="基本思想">基本思想</h2>
<p>1、给定一个初值点。</p>
<p>2、构造目标函数的二次逼近，该目标函数与该点上的第一和第二导数值相匹配。</p>
<p>3、最小化近似二次函数代替原来的目标函数。</p>
<p>4、使用近似函数的极小值作为起点，迭代地重复该过程。</p>
<h2 id="牛顿法没有步阶大小或者步阶为1">牛顿法（没有步阶大小，或者步阶为1）</h2>
<p>1、给定<span class="math inline">\(f: \mathbb{R}^{n} \rightarrow \mathbb{R}\)</span>，并且当前迭代为<span class="math inline">\(\mathbf{x}^{(k)}\)</span>。</p>
<p>2、通过二次型近似函数<span class="math inline">\(f\)</span>求<span class="math inline">\(\mathbf{x}^{(k+1)}\)</span>。</p>
<p><span class="math display">\[q(\mathbf{x})=f\left(\mathbf{x}^{(k)}\right)+\left(\mathbf{x}-\mathbf{x}^{(k)}\right)^{\top} \mathbf{g}^{(k)}+\frac{1}{2}\left(\mathbf{x}-\mathbf{x}^{(k)}\right)^{\top} \mathbf{F}\left(\mathbf{x}^{(k)}\right)\left(\mathbf{x}-\mathbf{x}^{(k)}\right)\]</span></p>
<p>3、最小化<span class="math inline">\(q\)</span>来迭代下一个<span class="math inline">\(\mathbf{x}^{(k+1)}\)</span>。</p>
<p>4、假设<span class="math inline">\(\mathbf{g}^{(k)}=\nabla f\left(\mathbf{x}^{(k)}\right)\)</span>。通过一阶必要条件，可以知道<span class="math inline">\(\nabla q\left(\mathbf{x}^{(k)}\right)=0\)</span>。</p>
<p><span class="math display">\[\nabla q\left(\mathbf{x}^{(k)}\right)=\mathbf{g}^{(k)}+\mathbf{F}\left(\mathbf{x}^{(k)}\right)\left(\mathbf{x}-\mathbf{x}^{(k)}\right)=0\]</span></p>
<p>5、牛顿算法</p>
<p><span class="math display">\[\mathbf{x}^{(k+1)}=\mathbf{x}^{(k)}-\mathbf{F}\left(\mathbf{x}^{(k)}\right)^{-1} \mathbf{g}^{(k)}\]</span></p>
<p>搜索方向</p>
<p><span class="math display">\[\mathbf{d}^{(k)}=-\mathbf{F}\left(\mathbf{x}^{(k)}\right)^{-1} \mathbf{g}^{(k)}=\mathbf{x}^{(k+1)}-\mathbf{x}^{(k)}\]</span></p>
<h2 id="levenberg-marquardt修正">Levenberg-Marquardt修正</h2>
<p>如果黑塞矩阵<span class="math inline">\(\mathbf{F}\left(\mathbf{x}^{(k)}\right)\)</span>不正定，那么搜索方向就会使下降方向<span class="math inline">\(\mathbf{d}^{(k)}=-\mathbf{F}\left(\mathbf{x}^{(k)}\right)^{-1} \mathbf{g}^{(k)}\)</span>可能不会是下降方向，因此Levenberg-Marquardt修正解决了这个问题。</p>
<p><span class="math display">\[\mathbf{x}^{(k+1)}=\mathbf{x}^{(k)}-\alpha_{k}\left(\mathbf{F}\left(\mathbf{x}^{(k)}\right)+\mu_{k} \mathbf{I}\right)^{-1} \mathbf{g}^{(k)}\]</span></p>
<p>其中<span class="math inline">\(\mu_{k} \geq 0\)</span>，<span class="math inline">\(F\)</span>为对称矩阵。</p>
<p>1、<span class="math inline">\(\mu_{k} \rightarrow 0\)</span>：Levenberg-Marquardt修正逐步接近牛顿法。</p>
<p>2、<span class="math inline">\(\mu_{k} \rightarrow \infty\)</span>：Levenberg-Marquardt修正表现出步长较小时的梯度方法的特征。</p>
<p>实际应用中，<span class="math inline">\(\mu_{k}\)</span>的初始值较小，然后逐步增加，直到出现下降特征，即<span class="math inline">\(f\left ( \mathbf{x}^{k+1} \right ) &lt; f\left ( \mathbf{x}^{k} \right )\)</span>为止。</p>
<h2 id="牛顿法总结">牛顿法总结</h2>
<p>1、如果起始点与最小值足够近，牛顿法的效果会很好。</p>
<p>2、可以合并一个步长以确保下降</p>
<p>3、对于二次型，一步收敛。</p>
<h1 id="共轭方向法">共轭方向法</h1>
<h2 id="介绍-3">介绍</h2>
<p>共轭方向法的效率和计算复杂度位于最速下降法和牛顿法之间。</p>
<p>共轭方向法有如下特征：</p>
<p>1、对于<span class="math inline">\(n\)</span>维二次型问题，能够在<span class="math inline">\(n\)</span>步内得到结果。</p>
<p>2、共轭方向法的典型代表共轭梯度法不需要计算黑塞矩阵。</p>
<p>3、不需要存储<span class="math inline">\(n \times n\)</span>矩阵，也不需要求逆。</p>
<p>4、比最速下降法复杂。</p>
<p>通常情况下比最速下降法性能更好，但不如牛顿法。迭代搜索方法效率的关键因素是每次迭代的搜索方向。对于某些函数，最佳搜索方向是Q-共轭方向。</p>
<h2 id="共轭向量">共轭向量</h2>
<p>1、给定对称举证<span class="math inline">\(\mathbf{Q} \in \mathbb{R}^{n \times n}\)</span>。</p>
<p>2、如果两个向量<span class="math inline">\(\mathbf{d}(1)\)</span>和<span class="math inline">\(\mathbf{d}(2)\)</span>是Q共轭，那么<span class="math inline">\(\mathbf{d}^{(1) \top} \mathbf{Q} \mathbf{d}^{(2)}=0\)</span>。</p>
<p>3、假设<span class="math inline">\(\mathbf{Q}\)</span>是对称<span class="math inline">\({n \times n}\)</span>举证，那么对于方向<span class="math inline">\(\mathbf{d}^{(0)}, \mathbf{d}^{(1)}, \mathbf{d}^{(2)}, \ldots, \mathbf{d}^{(m)}\)</span>如果是Q共轭的话，那么对于所有<span class="math inline">\(i \neq j\)</span>都有<span class="math inline">\(\mathbf{d}^{(i) \top} \mathbf{Q} \mathbf{d}^{(j)}=0\)</span>。</p>
<p>4、如果<span class="math inline">\(\mathbf{Q}=\mathbf{I}\)</span>，那么共轭变为正交。</p>
<h2 id="共轭方向算法">共轭方向算法</h2>
<p>考虑算法<span class="math inline">\(\mathbf{x}^{(k+1)}=\mathbf{x}^{(k)}+\alpha_{k} \mathbf{d}^{(k)}\)</span>，其中<span class="math inline">\(\alpha_{k}=\arg \min _{\alpha \geq 0} f\left(\mathbf{x}^{(k)}+\alpha \mathbf{d}^{(k)}\right)\)</span>。</p>
<p>应用到二次型则是<span class="math inline">\(f(\mathbf{x})=\frac{1}{2} \mathbf{x}^{\top} \mathbf{Q} \mathbf{x}-\mathbf{x}^{T} \mathbf{b}\)</span>，其中<span class="math inline">\(\mathbf{Q}=\mathbf{Q}^{\top} &gt; 0\)</span>。</p>
<p>如果给定起始点<span class="math inline">\(\mathbf{x}^{\left ( 0 \right )}\)</span>，和一组关于<span class="math inline">\(\mathbf{Q}\)</span>的共轭方向<span class="math inline">\(\mathbf{d}^{(0)}, \mathbf{d}^{(1)}, \ldots, \mathbf{d}^{(n-1)}\)</span>，那么迭代公式为（<span class="math inline">\(k \geq 0\)</span>表示迭代次数）：</p>
<p><span class="math display">\[\mathbf{g}^{\left ( k \right )}=\nabla f\left ( \mathbf{x}^{k} \right )=\mathbf{Q}\mathbf{x}^{\left ( k \right )}-\mathbf{b}\]</span></p>
<p><span class="math display">\[\alpha_{k}=-\frac{\mathbf{g}^{(k) \top} \mathbf{d}^{(k)}}{\mathbf{d}^{(k) \top} \mathbf{Q} \mathbf{d}^{(k)}}\]</span></p>
<p><span class="math display">\[\mathbf{x}^{\left ( k+1 \right )}=\mathbf{x}^{\left ( k \right )}+\alpha_{k}\mathbf{d}^{\left ( k \right )}\]</span></p>
<p>对于二次型来说，共轭方向算法有如下优势：对于任意的起始点<span class="math inline">\(\mathbf{x}^{(0)}\)</span>，基本的共轭方向算法都能在<span class="math inline">\(n\)</span>次迭代之内收敛到唯一的全局极小点<span class="math inline">\(\mathbf{x}^{\star}\)</span>，即<span class="math inline">\(\mathbf{x}^{(n)}=\mathbf{x}^{\star}\)</span>。</p>
<h2 id="共轭梯度法">共轭梯度法</h2>
<p>1、共轭梯度法在计算过程中计算方向。</p>
<p>2、在每个迭代，搜索方向方向都是前一个搜索方向和当前梯度的线性组合。</p>
<p>3、使其与前面产生的搜索方向组成<span class="math inline">\(\mathbf{Q}\)</span>共轭方向。</p>
<blockquote>
<p>算法流程</p>
</blockquote>
<p>1、考虑二次型目标函数：<span class="math inline">\(f(\mathbf{x})=\frac{1}{2} \mathbf{x}^{\top} \mathbf{Q} \mathbf{x}-\mathbf{x}^{\top} \mathbf{b}, \mathbf{x} \in \mathbb{R}\)</span></p>
<p>2、起始点<span class="math inline">\(\mathbf{x}^{(0)}\)</span>，搜索方向采用最速下降法的方向，即函数<span class="math inline">\(f\)</span>在<span class="math inline">\(\mathbf{x}^{(0)}\)</span>处的梯度负方向：<span class="math inline">\(\mathbf{d}^{(0)}=-\mathbf{g}^{(0)}\)</span>。</p>
<p>3、因此<span class="math inline">\(\mathbf{x}^{(1)}=\mathbf{x}^{(0)}+\alpha_{0} \mathbf{d}^{(0)}\)</span>，并且<span class="math inline">\(\alpha_{0}=\arg \min _{\alpha \geq 0} f\left(\mathbf{x}^{(0)}+\alpha \mathbf{d}^{(0)}\right)=-\frac{\mathbf{g}^{(0) \top} \mathbf{d}^{(0)}}{\mathbf{d}^{(0) \top} \mathbf{Q} \mathbf{d}^{(0)}}\)</span>。</p>
<p>4、接下来，搜索方向<span class="math inline">\(\mathbf{d}^{(1)}\)</span>应该和<span class="math inline">\(\mathbf{d}^{(0)}\)</span>关于<span class="math inline">\(\mathbf{Q}\)</span>共轭。</p>
<p>5、将<span class="math inline">\(\mathbf{d}^{(1)}\)</span>写成<span class="math inline">\(\mathbf{g}^{(1)}\)</span>和<span class="math inline">\(\mathbf{d}^{(0)}\)</span>的线性组合：<span class="math inline">\(\mathbf{d}^{(1)}=-\mathbf{g}^{(1)}+\beta_{0} \mathbf{d}^{(0)}\)</span>。</p>
<p>6、<span class="math inline">\(\beta_{k}\)</span>可以使<span class="math inline">\(\mathbf{d}^{(k+1)}\)</span>和<span class="math inline">\(\mathbf{d}^{(0)}, \mathbf{d}^{(1)}, \ldots, \mathbf{d}^{(k)}\)</span>组成<span class="math inline">\(\mathbf{Q}\)</span>共轭方向：<span class="math inline">\(\beta_{k}=\frac{\mathbf{g}^{(k+1) \top} \mathbf{Q} \mathbf{d}^{(k)}}{\mathbf{d}^{(k) \top} \mathbf{Q} \mathbf{d}^{(k)}}\)</span>。</p>
<blockquote>
<p>三种修正公式，将<span class="math inline">\(\mathbf{Q}\)</span>从<span class="math inline">\(\beta\)</span>计算中消除</p>
</blockquote>
<p>1、Hestenes-Stiefel公式：<span class="math inline">\(\beta_{k}=\frac{\mathbf{g}^{(k+1) \top}\left[\mathbf{g}^{(k+1)-\mathbf{g}^{(k)}}\right]}{\mathbf{d}^{(k) \top}\left[\mathbf{g}^{(k+1)}-\mathbf{g}^{(k)}\right]}\)</span></p>
<p>2、Polak-Ribiere公式：<span class="math inline">\(\beta_{k}=\frac{\mathrm{g}^{(k+1) \top}\left[\mathrm{g}^{(k+1)}-\mathrm{g}(k)\right]}{\mathrm{g}^{(k+1) \top} \mathrm{g}^{(k)}}\)</span></p>
<p>3、Fletcher-Reeves公式：<span class="math inline">\(\beta_{k}=\frac{\mathbf{g}^{(k+1) \top} \mathbf{g}^{(k+1)}}{\mathbf{g}^{(k) \top} \mathbf{g}^{(k)}}\)</span></p>
<h3 id="共轭梯度法在非二次型问题">共轭梯度法在非二次型问题</h3>
<p>假如<span class="math inline">\(f\)</span>是二次型，那么上面三个修正公式等价，如果不是二次型，那么算法不能在<span class="math inline">\(n\)</span>步内进行收敛，并且随着迭代的进行搜索方向将不与<span class="math inline">\(\mathbf{Q}\)</span>共轭。常用的解决方法是经过<span class="math inline">\(n\)</span>或<span class="math inline">\(n+1\)</span>次迭代后，重新将搜索方向初始化为目标函数梯度的负方向，然后继续搜索知道满足停止规则。</p>
<p>对于非二次型目标函数中，一维搜索精度非常重要，并且一维搜索精度是共轭梯度法性能的关键，如果采用不精确的一维搜索方法，那么建议使用Hestenes-Stiefel公式。</p>
<p>Powell公式（Polak-Ribiere公式修正）</p>
<p><span class="math display">\[\beta_{k}=\max \left[0, \frac{\mathbf{g}^{(k+1) \top}\left[\mathbf{g}^{(k+1)} - \mathbf{g}^{(k)}\right]}{\mathbf{g}^{(k) \top} \mathbf{g}^{(k)}}\right]\]</span></p>
<h1 id="拟牛顿法">拟牛顿法</h1>
<h2 id="回顾牛顿法">回顾牛顿法</h2>
<p>牛顿法</p>
<p><span class="math display">\[\mathbf{x}^{(k+1)}=\mathbf{x}^{(k)}-F\left(\mathbf{x}^{(k)}\right)^{-1} \mathbf{g}^{(k)}\]</span></p>
<p>保证下降性能的改进</p>
<p><span class="math display">\[\mathbf{x}^{(k+1)}=\mathbf{x}^{(k)}-\alpha_{k} F\left(\mathbf{x}^{(k)}\right)^{-1} \mathbf{g}^{(k)}\]</span></p>
<p>且</p>
<p><span class="math display">\[\alpha_{k}=\arg \min _{\alpha \geq 0} f\left(\mathbf{x}^{(k)}-\alpha F\left(\mathbf{x}^{(k)}\right)^{-1} \mathbf{g}^{(k)}\right)\]</span></p>
<p>牛顿法特性：</p>
<p>1、如果起始点足够接近最小值，那么牛顿法就会快速收敛。</p>
<p>2、需要计算黑塞矩阵的逆。</p>
<h2 id="基本思想-1">基本思想</h2>
<p>1、拟牛顿法：只是用梯度来近似黑塞逆。</p>
<p>2、使用<span class="math inline">\(\mathbf{H}_{k}\)</span>来代替原先牛顿算法中的黑塞逆矩阵。</p>
<p>3、拟牛顿算法把牛顿算法近似为<span class="math inline">\(\mathbf{x}^{(k+1)}=\mathbf{x}^{(k)}-\alpha_{k} \mathbf{H}_{k} \mathbf{g}^{(k)}\)</span></p>
<p>4、在每次迭代时，通过<span class="math inline">\(\mathbf{x}^{(k)}\)</span>、<span class="math inline">\(\mathbf{x}^{(k+1)}\)</span>、<span class="math inline">\(\mathbf{g}^{(k)}\)</span>、<span class="math inline">\(\mathbf{g}^{(k+1)}\)</span>、<span class="math inline">\(\mathbf{H}_{k}\)</span>来计算并更新得到<span class="math inline">\(\mathbf{H}^{(k+1)}\)</span>。</p>
<h2 id="拟牛顿法-1">拟牛顿法</h2>
<p>1、<span class="math inline">\(\mathbf{H}_{k}\)</span>是黑塞逆的近似。</p>
<p>2、<span class="math inline">\(\mathbf{H}_{k}\)</span>模仿了<span class="math inline">\(\mathbf{F}\left(\mathbf{x}^{(k)}\right)^{-1}\)</span>的三个特性。</p>
<blockquote>
<ul>
<li><span class="math inline">\(\mathbf{H}_{k}\)</span>是对称矩阵。</li>
<li><span class="math inline">\(\mathbf{H}_{k}\)</span>是正定的，保证下降特性。</li>
<li><span class="math inline">\(\mathbf{H}_{k}\)</span>具有“割线”特性。</li>
</ul>
</blockquote>
<p>3、这是<span class="math inline">\(\mathbf{H}_{k}\)</span>序列满足的条件，也是拟牛顿法的基础。</p>
<blockquote>
<p>拟牛顿法算法</p>
</blockquote>
<p><span class="math display">\[\begin{aligned}
&amp;\mathbf{d}^{(k)}=-\mathbf{H}_{k} \mathbf{g}^{(k)}\\
&amp;\alpha_{k}=\arg \min _{\alpha \geq 0} f\left(\mathbf{x}^{(k)}+\alpha \mathbf{d}^{(k)}\right)=-\frac{\mathbf{g}^{(k) T} \mathbf{d}^{(k)}}{\mathbf{d}^{(k) T} Q \mathbf{d}^{(k)}}\\
&amp;\mathbf{x}^{(k+1)}=\mathbf{x}^{(k)}+\alpha_{k} \mathbf{d}^{(k)}
\end{aligned}\]</span></p>
<p>其中<span class="math inline">\(\mathbf{H}_{0}, \mathbf{H}_{1}, \dots\)</span>是对称矩阵。</p>
<p>在二次型情况下，上述矩阵必须满足</p>
<p><span class="math display">\[\mathbf{H}_{k+1} \Delta \mathbf{g}^{(i)}=\Delta \mathbf{x}^{(i)}, \quad 0 \leq i \leq k\]</span></p>
<p>拟牛顿法是共轭方向法，因此如果将拟牛顿法应用到二次型中，并且黑塞矩阵<span class="math inline">\(\mathbf{Q}=\mathbf{Q}^{T}\)</span>，对于<span class="math inline">\(0 \leq k &lt; n-1\)</span>，就有</p>
<p><span class="math display">\[\mathbf{H}_{k+1} \Delta \mathbf{g}^{(i)}=\Delta \mathbf{x}^{(i)}, \quad 0 \leq i \leq k\]</span></p>
<p>其中<span class="math inline">\(\mathbf{H}_{k+1} = \mathbf{H}_{k+1}^{\top}\)</span>。如果<span class="math inline">\(\alpha_{i} \neq 0,0 \leq i \leq k\)</span>，那么<span class="math inline">\(\mathbf{d}^{(0)}, \ldots, \mathbf{d}^{(k+1)}\)</span>是<span class="math inline">\(\mathbf{Q}\)</span>共轭。</p>
<p>生成<span class="math inline">\(\mathbf{H}_{k}\)</span>有三个方法，分别是</p>
<blockquote>
<ul>
<li>1、秩1法</li>
<li>2、DFP法</li>
<li>3、BFGS法</li>
</ul>
</blockquote>
<p>并且这些方法都有下面的形式</p>
<p><span class="math display">\[\mathbf{H}_{k+1}=\mathbf{H}_{k}+\mathbf{U}_{k}\]</span></p>
<p>其中<span class="math inline">\(\mathbf{U}_{k}\)</span>使用<span class="math inline">\(\mathbf{H}_{k}\)</span>、<span class="math inline">\(\Delta \mathbf{g}^{(k)}\)</span>和<span class="math inline">\(\Delta \mathbf{x}^{(k)}\)</span>计算得到。</p>
<h2 id="秩1法">秩1法</h2>
<blockquote>
<p>公式</p>
</blockquote>
<p><span class="math display">\[\mathbf{U}_{k}=\alpha_{k} \mathbf{z}^{(k)} \mathbf{z}^{(k) T}\]</span></p>
<p>其中<span class="math inline">\(\alpha_{k} \in \mathbb{R}\)</span>，并且<span class="math inline">\(\mathbf{z}^{(k)} \in \mathbb{R}^{n}\)</span>。同时</p>
<p><span class="math display">\[\operatorname{rank} \mathbf{z}^{(k)} \mathbf{z}^{(k) T}=\operatorname{rank}\left(\left[\begin{array}{c}
{z_{1}^{(k)}} \\
{\vdots} \\
{z_{n}^{(k)}}
\end{array}\right]\left[\begin{array}{ccc}
{z_{1}^{(k)}} &amp; {\cdots} &amp; {z_{n}^{(k)}}
\end{array}\right]\right)=1\]</span></p>
<p>根据推导得到</p>
<p><span class="math display">\[\mathbf{U}_{k}=\frac{\left(\Delta \mathbf{x}^{(k)}-\mathbf{H}_{k} \Delta \mathbf{g}^{(k)}\right)\left(\Delta \mathbf{x}^{(k)}-\mathbf{H}_{k} \Delta \mathbf{g}^{(k)}\right)^{T}}{\left(\Delta \mathbf{x}^{(k)}-\mathbf{H}_{k} \Delta \mathbf{g}^{(k)}\right)^{T} \Delta \mathbf{g}^{(k)}}\]</span></p>
<p>所以</p>
<p><span class="math display">\[\alpha_{k}=\frac{1}{\left(\Delta \mathbf{x}^{(k)}-\mathbf{H}_{k} \Delta \mathbf{g}^{(k)}\right)^{T} \Delta \mathbf{g}^{(k)}}\]</span></p>
<p><span class="math display">\[\mathbf{z}^{(k)}=\Delta \mathbf{x}^{(k)}-\mathbf{H}_{k} \Delta \mathbf{g}^{(k)}\]</span></p>
<blockquote>
<p>总结整个流程</p>
</blockquote>
<p>1、首先把目标函数<span class="math inline">\(f\)</span>转化为二次型函数<span class="math inline">\(f(\mathbf{x})=\frac{1}{2} \mathbf{x}^{T} Q \mathbf{x}-\mathbf{b}^{T} \mathbf{x}\)</span>。</p>
<p>2、根据二次型函数获得函数<span class="math inline">\(f\)</span>在<span class="math inline">\(\mathbf{x}^{(k)}\)</span>的梯度<span class="math inline">\(\mathbf{g}^{(k)}\)</span>。</p>
<p>3、根据<span class="math inline">\(\mathbf{x}\)</span>的维度<span class="math inline">\(n\)</span>创建<span class="math inline">\(n\)</span>维度的单位矩阵<span class="math inline">\(\mathbf{H}_{0}\)</span>。</p>
<p>4、根据<span class="math inline">\(\mathbf{d}^{(k)}=-\mathbf{H}_{k} \mathbf{g}^{(k)}\)</span>计算方向向量<span class="math inline">\(\mathbf{d}^{(0)}\)</span>。</p>
<p>5、由于目标函数为二次型，因此根据步长公式<span class="math inline">\(\alpha_{k}=\arg \min _{\alpha \geq 0} f\left(\mathbf{x}^{(k)}+\alpha \mathbf{d}^{(k)}\right)=-\frac{\mathbf{g}^{(k) T} \mathbf{d}^{(k)}}{\mathbf{d}^{(k) T} Q \mathbf{d}^{(k)}}\)</span>得到步长<span class="math inline">\(\alpha_{0}\)</span>。</p>
<p>6、根据<span class="math inline">\(\mathbf{x}^{(k+1)}=\mathbf{x}^{(k)}+\alpha_{k} \mathbf{d}^{(k)}\)</span>计算得到<span class="math inline">\(\mathbf{x}^{(1)}\)</span>。</p>
<p>7、根据上式计算得到<span class="math inline">\(\Delta \mathbf{x}^{(0)}\)</span>，<span class="math inline">\(\Delta \mathbf{x}^{(k)}=\alpha_{k} \mathbf{d}^{(k)}\)</span>。</p>
<p>8、根据二次型公式计算梯度得到<span class="math inline">\(\mathbf{g}^{(1)}\)</span>，<span class="math inline">\(\mathbf{g}^{(k)}=Q \mathbf{x}^{(k)}\)</span>.</p>
<p>9、根据<span class="math inline">\(\mathbf{g}^{(1)}\)</span>得到<span class="math inline">\(\Delta \mathbf{g}^{(0)}\)</span>，<span class="math inline">\(\mathbf{g}^{(k+1)}-\mathbf{g}^{(k)}\)</span>。</p>
<p>10、根据秩1法得到<span class="math inline">\(\alpha_{k}\)</span>，<span class="math inline">\(\Delta \mathbf{g}^{(0) T}\left(\Delta \mathbf{x}^{(0)}-\mathbf{H}_{0} \Delta \mathbf{g}^{(0)}\right)\)</span>。</p>
<p>11、计算得到<span class="math inline">\(\mathbf{U}_{k}\)</span>。</p>
<p>12、得到<span class="math inline">\(\mathbf{d}^{(1)}\)</span>的方向向量。</p>
<p>13、从第五步开始迭代执行<span class="math inline">\(n\)</span>次得到<span class="math inline">\(\mathbf{x}^{*}\)</span>。</p>
<blockquote>
<p>不足</p>
</blockquote>
<p>1、<span class="math inline">\(\mathbf{H}_{k}\)</span>不一定是正定，这将导致<span class="math inline">\(\mathbf{d}^{(k)}=-\mathbf{H}_{k} \mathbf{g}^{(k)}\)</span>不一定是下降方向。</p>
<p>2、当<span class="math inline">\(\Delta \mathbf{g}^{(k) T}\left(\Delta \mathbf{x}^{(k)}-\mathbf{H}_{k} \Delta \mathbf{g}^{(k)}\right) \approx 0\)</span>时，<span class="math inline">\(\mathbf{H}_{k+1}\)</span>的计算有困难。</p>
<h2 id="dfp算法变尺度算法">DFP算法（变尺度算法）</h2>
<blockquote>
<p>公式</p>
</blockquote>
<p><span class="math display">\[\mathbf{U}_{k}=\frac{\Delta \mathbf{x}^{(k)} \Delta \mathbf{x}^{(k) T}}{\Delta \mathbf{x}^{(k) T} \Delta \mathbf{g}^{(k)}}-\frac{\mathbf{H}_{k} \Delta \mathbf{g}^{(k)} \Delta \mathbf{g}^{(k) T} \mathbf{H}_{k}}{\Delta \mathbf{g}^{(k) T} \mathbf{H}_{k} \Delta \mathbf{g}^{(k)}}\]</span></p>
<p>所以</p>
<p><span class="math display">\[\mathbf{H}_{k+1}=\mathbf{H}_{k}+\frac{\Delta \mathbf{x}^{(k)} \Delta \mathbf{x}^{(k) T}}{\Delta \mathbf{x}^{(k) T} \Delta \mathbf{g}^{(k)}}-\frac{\mathbf{H}_{k} \Delta \mathbf{g}^{(k)} \Delta \mathbf{g}^{(k) T} \mathbf{H}_{k}}{\Delta \mathbf{g}^{(k) T} \mathbf{H}_{k} \Delta \mathbf{g}^{(k)}}\]</span></p>
<blockquote>
<p>特性</p>
</blockquote>
<p>1、DFP算法是拟牛顿算法（满足拟牛顿条件）。</p>
<p>2、当DFP算法用于二次型时，并且黑塞举证<span class="math inline">\(\mathbf{Q}=\mathbf{Q}^{T}\)</span>，那么就有</p>
<p><span class="math display">\[\mathbf{H}_{k+1} \Delta \mathbf{g}^{(i)}=\Delta \mathbf{x}^{(i)}, \quad 0 \leq i \leq k\]</span></p>
<p>3、DFP算法也是一个共轭方向算法。</p>
<p>4、定理：如果<span class="math inline">\(\mathbf{g}^{(k)} \neq 0\)</span>，那么在DFP中，<span class="math inline">\(\mathbf{H}_{k}\)</span>一定是正定矩阵。</p>
<p>5、DFP使得<span class="math inline">\(\mathbf{H}_{k}\)</span>具有正定性。</p>
<p>6、DFP优于秩1算法。</p>
<p>7、DFP算法在某些情况下可能会有问题（例如：非常大的非二次问题）。</p>
<h2 id="bfgs算法">BFGS算法</h2>
<blockquote>
<p>公式</p>
</blockquote>
<p><span class="math display">\[\mathbf{U}_{k}=\left(1+\frac{\Delta \mathbf{g}^{(k) T} \mathbf{H}_{k} \Delta \mathbf{g}^{(k) T}}{\Delta \mathbf{g}^{(k) T} \Delta \mathbf{x}^{(k)}}\right) \frac{\Delta \mathbf{x}^{(k)} \Delta \mathbf{x}^{(k) T}}{\Delta \mathbf{x}^{(k) T} \Delta \mathbf{g}^{(k)}}-\frac{\mathbf{H}_{k} \Delta \mathbf{g}^{(k)} \Delta \mathbf{x}^{(k) T}+\left(\mathbf{H}_{k} \Delta \mathbf{g}^{(k)} \Delta \mathbf{x}^{(k) T}\right)^{T}}{\Delta \mathbf{g}^{(k) T} \Delta \mathbf{x}^{(k)}}\]</span></p>
<p><span class="math display">\[\mathbf{H}_{k+1}=\mathbf{H}_{k}+\mathbf{U}_{k}\]</span></p>
<blockquote>
<p>特性</p>
</blockquote>
<p>1、BFGS使用互补性，从DFP中导出。</p>
<p>2、考虑拟牛顿条件，Hessian逆的近似应满足</p>
<p><span class="math display">\[\mathbf{H}_{k+1} \Delta \mathbf{g}^{(i)}=\Delta \mathbf{x}^{(i)}, \quad 0 \leq i \leq k\]</span></p>
<p>3、令<span class="math inline">\(\mathbf{B}_{k}\)</span>是一个黑塞矩阵的近似（<span class="math inline">\(\mathbf{B}_{k}^{-1}=\mathbf{H}_{k}\)</span>）,那么就可以得到</p>
<p><span class="math display">\[\mathbf{B}_{k+1} \Delta \mathbf{x}^{(i)}=\Delta \mathbf{g}^{(i)}, \quad 0 \leq i \leq k\]</span></p>
<p>将上述条件称为“互补拟牛顿”条件。</p>
<p>之前的公式对于更新<span class="math inline">\(\mathbf{B}_{k+1}\)</span>不是很有用，因为我们需要的是逆黑塞</p>
<p><span class="math display">\[\mathbf{H}_{k+1}^{B F G S}=\left(\mathbf{B}_{k+1}\right)^{-1}\]</span></p>
<p>存在一些计算逆的技巧，如下面的Sherman Morrison公式所示</p>
<p>引理：如果矩阵<span class="math inline">\(\mathbf{A}\)</span>不是奇异矩阵，<span class="math inline">\(\mathbf{u}\)</span>和<span class="math inline">\(\mathbf{v}\)</span>是列向量，满足<span class="math inline">\(1+\mathbf{v}^{T} \mathbf{A}^{-1} \mathbf{u} \neq 0\)</span>，那么<span class="math inline">\(\mathbf{A}+\mathbf{u} \mathbf{v}^{T}\)</span>非奇异，其逆矩阵可以用<span class="math inline">\(\mathbf{A}^{-1}\)</span>来表示</p>
<p><span class="math display">\[\left(\mathbf{A}+\mathbf{u v}^{T}\right)^{-1}=\mathbf{A}^{-1}-\frac{\left(\mathbf{A}^{-1} \mathbf{u}\right)\left(\mathbf{v}^{T} \mathbf{A}^{-1}\right)}{1+\mathbf{v}^{T} \mathbf{A}^{-1} \mathbf{u}}\]</span></p>
<p>因此可以知道如果<span class="math inline">\(\mathbf{A}^{-1}\)</span>已知，</p>
<p>如果<span class="math inline">\(\mathbf{A}^{-1}\)</span>是已知的，那么这个公式提供了一种“数值廉价”的方法来计算由矩阵<span class="math inline">\(\mathbf{A}\)</span>校正的<span class="math inline">\(\mathbf{u} \mathbf{v}^{T}\)</span>的逆。</p>
<p>此时，上面的公式就变为</p>
<p><span class="math display">\[\mathbf{B}_{k+1}=\mathbf{B}_{k}+\mathbf{u}_{1} \mathbf{v}_{1}^{T}+\mathbf{u}_{2} \mathbf{v}_{2}^{T}\]</span></p>
<p>因此</p>
<p><span class="math display">\[\mathbf{B}_{k+1}^{-1}=\left(\mathbf{B}_{k}+\mathbf{u}_{1} \mathbf{v}_{1}^{T}+\mathbf{u}_{2} \mathbf{v}_{2}^{T}\right)^{-1}\]</span></p>
<p>将引理应用于$<em>{k+1}<span class="math inline">\(2次，并使用\)</span></em>{k}<span class="math inline">\(替换\)</span>_{k}^{-1}$，就可以得到</p>
<p><span class="math display">\[\mathbf{H}_{k+1}^{B F G S}=\mathbf{H}_{k}+\left(1+\frac{\Delta \mathbf{g}^{(k) T} \mathbf{H}_{k} \Delta \mathbf{g}^{(k) T}}{\Delta \mathbf{g}^{(k) T} \Delta \mathbf{x}^{(k)}}\right) \frac{\Delta \mathbf{x}^{(k)} \Delta \mathbf{x}^{(k) T}}{\Delta \mathbf{x}^{(k) T} \Delta \mathbf{g}^{(k)}}- \frac{\mathbf{H}_{k} \Delta \mathbf{g}^{(k)} \Delta \mathbf{x}^{(k) T}+\left(\mathbf{H}_{k} \Delta \mathbf{g}^{(k)} \Delta \mathbf{x}^{(k) T}\right)^{T}}{\Delta \mathbf{g}^{(k) T} \Delta \mathbf{x}^{(k)}}\]</span></p>
<p>从而更新了<span class="math inline">\(\mathbf{H}_{k}\)</span>。</p>
<p>1、BFGS是DFP的“补充”公式</p>
<p>2、由于互补性，BFGS公式继承了DFP的性质</p>
<blockquote>
<ul>
<li>满足拟牛顿条件。</li>
<li>BFGS具有共轭方向性质。</li>
<li>BFGS继承了DFP的正定性，即当<span class="math inline">\(\mathbf{g}^{(k)} \neq 0\)</span>，并且<span class="math inline">\(\mathbf{H}_{k}&gt;0\)</span>，那么<span class="math inline">\(\mathbf{H}_{k+1}^{B F G S}&gt;0\)</span>。</li>
</ul>
</blockquote>
<p>3、当直线搜索准确时，BFGS具有相当强的鲁棒性（可以节省直线搜索部分的时间）。</p>
<p>4、BFGS通常比DFP公式有效得多。</p>
<p>5、对于非二次问题，拟牛顿算法通常不会在<span class="math inline">\(n\)</span>步内收敛。</p>
<p>6、需要进行一些修改（例如，在每次迭代后将方向向量重新初始化为负梯度）。</p>
<p>7、广泛应用。</p>
<h1 id="线性规划">线性规划</h1>
<h2 id="标准型线性规划">标准型线性规划</h2>
<span class="math display">\[\begin{matrix}
    minimize &amp; \mathbf{c}^{T} \mathbf{x}\\ 
    st. &amp; \mathbf{A x}=\mathbf{b}\\ 
     &amp; \mathbf{x} \geq 0
\end{matrix}\]</span>
<p>其中<span class="math inline">\(\mathbf{A}\)</span>是<span class="math inline">\(m \times n\)</span>，<span class="math inline">\(m &lt; n\)</span>，<span class="math inline">\(\operatorname{rank}(\mathbf{A})=m\)</span>，并且<span class="math inline">\(\mathbf{b} \geq 0\)</span>。</p>
<p>如果把LP问题转化为标准型？</p>
<p>1、如果问题是最大化，则简单地将目标函数乘以-1以最小化。</p>
<p>2、如果<span class="math inline">\(\mathbf{A}\)</span>不是满秩，那么就移除一行或多行。</p>
<p>3、如果<span class="math inline">\(\mathbf{b}\)</span>的一个分量是负的，比如第<span class="math inline">\(i\)</span>个分量，将第<span class="math inline">\(i\)</span>个约束乘以-1，右边就得到正的。</p>
<h2 id="不等式约束的情况">不等式约束的情况</h2>
<h3 id="转化为标准型松弛变量">转化为标准型：松弛变量</h3>
<p>假设有一个不等式约束</p>
<p><span class="math display">\[a_{1} x_{1}+a_{2} x_{2}+\cdots+a_{n} x_{n} \leq b\]</span></p>
<p>通过引入松弛变量，可以将上述不等式约束转化为标准等式约束。</p>
<p>具体来说，上述约束相当于</p>
<p><span class="math display">\[\begin{matrix}
    a_{1} x_{1}+a_{2} x_{2}+\dots+a_{n} x_{n}+x_{n+1}=b \\ 
    x_{n+1} \geq 0
\end{matrix}\]</span></p>
<p>其中松弛变量为<span class="math inline">\(x_{n+1}\)</span>。</p>
<h3 id="转化为标准型剩余变量">转化为标准型：剩余变量</h3>
<p>假设有一个不等式约束</p>
<p><span class="math display">\[a_{1} x_{1}+a_{2} x_{2}+\cdots+a_{n} x_{n} \geq b\]</span></p>
<p>可以通过引入一个剩余变量将上述不等式约束转化为标准等式约束</p>
<p>具体来说，上述约束相当于</p>
<p><span class="math display">\[\begin{matrix}
    a_{1} x_{1}+a_{2} x_{2}+\cdots+a_{n} x_{n}-x_{n+1}=b \\ 
    x_{n+1} \geq 0
\end{matrix}\]</span></p>
<h3 id="转化为标准型非正变量">转化为标准型：非正变量</h3>
<p>假设变量中的一个（比如<span class="math inline">\(x_{1}\)</span>）有下面的约束</p>
<p><span class="math display">\[x_{1} \leq 0\]</span></p>
<p>可以通过将<span class="math inline">\(x_{1}\)</span>的每一次出现都改为它的负数，从而把这个变量转换成通常的非负变量<span class="math inline">\(x_{1}^{\prime}=-x_{1}\)</span></p>
<p>假如有约束</p>
<p><span class="math display">\[\begin{matrix}
    a_{1} x_{1}+a_{2} x_{2}+\cdots+a_{n} x_{n}=b \\ 
    x_{1} \leq 0
\end{matrix}\]</span></p>
<p>通过引入<span class="math inline">\(x_{1}^{\prime}=-x_{1}\)</span>得到下面的约束</p>
<p><span class="math display">\[\begin{matrix}
    -a_{1} x_{1}^{\prime}+a_{2} x_{2}+\cdots+a_{n} x_{n}=b \\ 
    x_{1}^{\prime} \geq 0
\end{matrix}\]</span></p>
<h3 id="转化为标准型自由变量">转化为标准型：自由变量</h3>
<p>假设变量中的一个（比如<span class="math inline">\(x_{1}\)</span>）没有非负性约束（例如不存在<span class="math inline">\(x_{1} \geq 0\)</span>约束）</p>
<p>通过引入变量<span class="math inline">\(u_{1} \geq 0\)</span>和<span class="math inline">\(v_{1} \geq 0\)</span>，并且使用<span class="math inline">\(u_{1}-v_{1}\)</span>来替代<span class="math inline">\(\mathcal{X}_{1}\)</span></p>
<p>例如有约束</p>
<p><span class="math display">\[a_{1} x_{1}+a_{2} x_{2}+\cdots+a_{n} x_{n}=b\]</span></p>
<p>那么就可以等价为</p>
<p><span class="math display">\[\begin{matrix}
    a_{1}\left(u_{1}-v_{1}\right)+a_{2} x_{2}+\cdots+a_{n} x_{n}=b \\ 
    u_{1}, v_{1} \geq 0
\end{matrix}\]</span></p>
<h2 id="基本解决方案">基本解决方案</h2>
<span class="math display">\[\begin{matrix}
    minimize &amp; \mathbf{c}^{T} \mathbf{x}\\ 
    st. &amp; \mathbf{A x}=\mathbf{b}\\ 
     &amp; \mathbf{x} \geq 0
\end{matrix}\]</span>
<p>其中<span class="math inline">\(\mathbf{A}\)</span>是<span class="math inline">\(m \times n\)</span>，<span class="math inline">\(m &lt; n\)</span>，<span class="math inline">\(\operatorname{rank}(\mathbf{A})=m\)</span>，并且<span class="math inline">\(\mathbf{b} \geq 0\)</span>。</p>
<p>线性方程组有解的条件</p>
<p>1、如果<span class="math inline">\(\operatorname{rank}[\mathbf{A}]=\operatorname{rank}[\mathbf{A}, \mathbf{b}]\)</span>，那么线性方程有解。</p>
<p>2、<span class="math inline">\(\operatorname{rank}[\mathbf{A}]=n\)</span>，那么线性方程的解不唯一。</p>
<p>3、<span class="math inline">\(\operatorname{rank}[\mathbf{A}] &lt; n\)</span>，那么线性方程有无穷解。</p>
<p>当<span class="math inline">\(m &lt; n\)</span>，有无穷多个点满足<span class="math inline">\(\mathbf{A x}=\mathbf{b}\)</span>。</p>
<p>如果有矩阵<span class="math inline">\(\mathbf{A}\)</span>，以及矩阵<span class="math inline">\(\mathbf{A}\)</span>中的一组<span class="math inline">\(m\)</span>个线性无关列向量，那么<span class="math inline">\(\mathbf{A}=[\mathbf{B}, \mathbf{D}]\)</span>，其中<span class="math inline">\(\mathbf{D}\)</span>是<span class="math inline">\(m \times(n-m)\)</span>维矩阵。</p>
<p>那么就可以求解方程<span class="math inline">\(\mathbf{B} \mathbf{x}_{B}=\mathbf{b}\)</span>，并且方程的解为<span class="math inline">\(\mathbf{x}_{B}=\mathbf{B}^{-1} \mathbf{b}\)</span>。</p>
<p>让<span class="math inline">\(\mathbf{x}=\left[\mathbf{x}_{B}^{T}, \mathbf{0}^{T}\right]^{T}\)</span>，那么<span class="math inline">\(\mathbf{x}\)</span>就是方程<span class="math inline">\(\mathbf{A x}=\mathbf{b}\)</span>的解。</p>
<p>1、如果<span class="math inline">\(\mathbf{x}\)</span>满足<span class="math inline">\(\mathbf{A x}=\mathbf{b}\)</span>，那么<span class="math inline">\(\mathbf{x}\)</span>就是基本解。</p>
<p>2、如果<span class="math inline">\(\mathbf{x}\)</span>满足<span class="math inline">\(\mathbf{A x}=\mathbf{b}\)</span>，且<span class="math inline">\(\mathbf{x} \geq 0\)</span>，那么<span class="math inline">\(\mathbf{x} \geq 0\)</span>是可行解，可行解也是基本可行解。</p>
<p>3、如果<span class="math inline">\(\mathbf{x}\)</span>中有一些为0，那么基本可行解就是退化的基本解。</p>
<p>4、如果<span class="math inline">\(\mathbf{x}\)</span>满足基本可行解，但其中有一些为0，那么就是退化的基本可行解。</p>
<h2 id="基本解的性质">基本解的性质</h2>
<blockquote>
<p>定义</p>
</blockquote>
<p>对于任何满足约束条件<span class="math inline">\(\mathbf{A x}=\mathbf{b}\)</span>，<span class="math inline">\(\mathbf{x} \geq 0\)</span>的向量<span class="math inline">\(\mathbf{x}\)</span>，如果它能够使目标函数<span class="math inline">\(\mathbf{c}^{T} \mathbf{x}\)</span>取得极小值，那么就将其称为最有可行解。如果最有可行解是基本解，那么它就是最有基本可行解。</p>
<blockquote>
<p>定理</p>
</blockquote>
<p>1、如果存在可行解，那么一定存在基本可行解。</p>
<p>2、如果存在最优可行解，那么一定存在最优基本可行解。</p>
<p>有个定理后</p>
<p>1、线性规划的基本定理将求解线性规划问题的任务简化为搜索基本可行解的任务。</p>
<p>2、只需要检查最优性的基本可行解。</p>
<p>3、基本解的数量最大可以是</p>
<p><span class="math display">\[\left(\begin{array}{c}
{n} \\
{m}
\end{array}\right)=\frac{n !}{m !(n-m) !}\]</span></p>
<p>其中<span class="math inline">\(m\)</span>是秩，<span class="math inline">\(n\)</span>是维度。</p>
<p>4、注意，虽然这个数字是有限的，但它可能非常大。</p>
<h2 id="几何视角下的线性规划">几何视角下的线性规划</h2>
<blockquote>
<p>定理：如果可行集<span class="math inline">\(\mathbf{A x}=\mathbf{b}, \quad \mathbf{x} \geq 0\)</span>存在，那么它一定是凸集。</p>
</blockquote>
<p>1、约束集的极值点等价于基本可行解（BFS）</p>
<p>2、几何上，BFS对应于约束集的“角”点（顶点）。</p>
<p>3、如果可行集是非空的，那么它有一个顶点。</p>
<p>4、如果问题有一个极小值（最优），那么其中一个顶点就是一个极小值。</p>
<p>可以得到</p>
<p>1、极值点集等于基本可行解集。</p>
<p>2、结合LP的基本定理，可以看出求解LP问题只需要考察约束集的极值点。</p>
<h1 id="整数规划问题">整数规划问题</h1>
<p>整数规划的一般形式</p>
<span class="math display">\[\begin{matrix}
    minimize &amp; f(\mathbf{x})\\ 
    st. &amp; \mathbf{g}(\mathbf{x}) \leq 0\\ 
     &amp; \mathbf{x} \in \mathbb{Z}^{n}
\end{matrix}\]</span>
<p>整数规划问题是一个NP难问题。</p>
<h2 id="整数线性规划">整数线性规划</h2>
<span class="math display">\[\begin{matrix}
    minimize &amp; \mathbf{c}^{\top} \mathbf{x}\\ 
    st. &amp; \mathbf{Ax}=\mathbf{b}\\ 
     &amp; \mathbf{x} \geq \mathbf{0}\\
     &amp; \mathbf{x} \in \mathbb{Z}^{n}\\
\end{matrix}\]</span>
<h2 id="解决ilp的方法">解决ILP的方法</h2>
<p>1、把ILP变为LP问题</p>
<p>2、特殊情况：幺模矩阵</p>
<p>3、精确方法</p>
<blockquote>
<ul>
<li>1、割平面法</li>
<li>2、动态规划法</li>
<li>3、分支边界法</li>
<li>4、分支切割法</li>
</ul>
</blockquote>
<p>4、启发式方法</p>
<h2 id="幺模矩阵">幺模矩阵</h2>
<blockquote>
<p>定义：对于<span class="math inline">\(m \times n\)</span>的整型矩阵<span class="math inline">\(\mathbf{A} \in \mathbb{Z}^{m \times n}(m 《 n)\)</span>，如果其所有<span class="math inline">\(m\)</span>阶非零子式为<span class="math inline">\(\pm 1\)</span>，那么矩阵<span class="math inline">\(\mathbf{A}\)</span>就是幺模矩阵。</p>
</blockquote>
<p>假设线性方程<span class="math inline">\(\mathbf{A x}=\mathbf{b}\)</span>，其中<span class="math inline">\(\mathbf{A} \in \mathbb{Z}^{m \times n}(m \leq n)\)</span>，<span class="math inline">\(\mathbf{B}\)</span>是一个基矩阵（由<span class="math inline">\(m\)</span>个线性无关的列向量组成<span class="math inline">\(m \times m\)</span>矩阵），<span class="math inline">\(\mathbf{A}\)</span>的幺模矩阵是<span class="math inline">\(|\operatorname{det} \mathbf{B}|=1\)</span>的所有<span class="math inline">\(\mathbf{B}\)</span>。</p>
<blockquote>
<p>引理：对于线性方程<span class="math inline">\(\mathbf{A x}=\mathbf{b}\)</span>，其中<span class="math inline">\(\mathbf{A} \in \mathbb{Z}^{m \times n}(m \leq n)\)</span>是幺模矩阵，并且<span class="math inline">\(\mathbf{b} \in \mathbb{Z}^{m}\)</span>，那么它的所有基本解都是整数解。</p>
</blockquote>
<blockquote>
<p>推论：如果线性规划的约束方程<span class="math inline">\(\mathbf{A x}=\mathbf{b}, \mathbf{x} \geq 0\)</span>，其中<span class="math inline">\(\mathbf{A}\)</span>是幺模矩阵，<span class="math inline">\(\mathbf{A} \in \mathbb{Z}^{m \times n}\)</span>，并且<span class="math inline">\(\mathbf{b} \in \mathbb{Z}^{m}\)</span>，那么所有的基本可行解都是整数。</p>
</blockquote>
<blockquote>
<p>定义：对于一个<span class="math inline">\(m \times n\)</span>的整数矩阵<span class="math inline">\(\mathbf{A} \in \mathbb{Z}^{m \times n}\)</span>，如果它的所有非零子式都是<span class="math inline">\(\pm 1\)</span>，那么矩阵<span class="math inline">\(\mathbf{A}\)</span>就是幺模矩阵。</p>
</blockquote>
<blockquote>
<p>推论：考虑线性规划的约束条件<span class="math inline">\([\mathbf{A}, \mathbf{I}] \mathbf{x}=\mathbf{b}, \mathbf{x} \geq 0\)</span>，其中<span class="math inline">\(\mathbf{A} \in \mathbb{Z}^{m \times n}\)</span>是完全幺模矩阵，<span class="math inline">\(\mathbf{b} \in \mathbb{Z}^{m}\)</span>，那么他的所有基本可行解都是整数解。</p>
</blockquote>
<h2 id="割平面法">割平面法</h2>
<h3 id="主要思想">主要思想</h3>
<p>1、通过LP松弛来解决ILP问题。</p>
<p>2、验证解</p>
<blockquote>
<ul>
<li>如果最优化解是整型，那么就得到解。</li>
<li>如果不是，那么就需要增加约束来移除在可行集中的非整数最优解。</li>
</ul>
</blockquote>
<p>3、重复该过程，直到最优解为整数向量。</p>
<h3 id="流程">流程</h3>
<p>1、引入向下取整操作，表示为<span class="math inline">\(\lfloor x\rfloor\)</span>。</p>
<p>2、定义整数线性规划问题</p>
<span class="math display">\[\begin{matrix}
    minimize &amp; \mathbf{c}^{\top} \mathbf{x}\\ 
    st. &amp; \mathbf{Ax}=\mathbf{b}\\ 
     &amp; \mathbf{x} \geq \mathbf{0}\\
     &amp; \mathbf{x} \in \mathbb{Z}^{n}\\
\end{matrix}\]</span>
<p>3、我们首先得到线性规划问题的最优基本可行解。</p>
<span class="math display">\[\begin{matrix}
    minimize &amp; \mathbf{c}^{T} \mathbf{x}\\ 
    st. &amp; \mathbf{A x}=\mathbf{b}\\ 
     &amp; \mathbf{x} \geq 0
\end{matrix}\]</span>
<p>4、假设前<span class="math inline">\(m\)</span>列向量组成了最有基本可行解的基矩阵，则相应的增广矩阵为</p>
<p><span class="math display">\[\begin{array}{cccccccccc}
{a_{1}} &amp; {a_{2}} &amp; {\cdots} &amp; {a_{i}} &amp; {\cdots} &amp; {a_{m}} &amp; {a_{m+1}} &amp; {\cdots} &amp; {a_{n}} &amp; {y_{0}} \\
{1} &amp; {0} &amp; {\cdots} &amp; {0} &amp; {\cdots} &amp; {0} &amp; {y_{1, m+1}} &amp; {\cdots} &amp; {y_{1, n}} &amp; {y_{10}} \\
{0} &amp; {1} &amp; {\cdots} &amp; {0} &amp; {\cdots} &amp; {0} &amp; {y_{2, m+1}} &amp; {\cdots} &amp; {y_{2, n}} &amp; {y_{20}} \\
{\vdots} &amp; {\vdots} &amp; {} &amp; {\vdots} &amp; {} &amp; {\vdots} &amp; {} &amp; {} &amp; {} &amp; {\vdots} \\
{0} &amp; {0} &amp; {\cdots} &amp; {1} &amp; {\cdots} &amp; {0} &amp; {y_{i, m+1}} &amp; {\cdots} &amp; {y_{i, n}} &amp; {y_{i 0}} \\
{\vdots} &amp; {\vdots} &amp; {} &amp; {\vdots} &amp; {} &amp; {\vdots} &amp; {} &amp; {} &amp; {\vdots} &amp; {\vdots} \\
{0} &amp; {0} &amp; {\cdots} &amp; {0} &amp; {\cdots} &amp; {1} &amp; {y_{m, m+1}} &amp; {\cdots} &amp; {y_{m, n}} &amp; {y_{m 0}}
\end{array}\]</span></p>
<p>5、假设最有基本可行解中的第<span class="math inline">\(i\)</span>个元素<span class="math inline">\(y_{i 0}\)</span>不是整数。注意，任何可行向量<span class="math inline">\(\mathbf{X}\)</span>都满足等式约束</p>
<p><span class="math display">\[x_{i}+\sum_{j=m+1}^{n} y_{i j} x_{j}=y_{i 0}\]</span></p>
<p>6、利用上式就可以构造出新增的约束条件。</p>
<blockquote>
<ul>
<li>从可行集中消除当前最优非整数解。</li>
<li>保持任何整数可行解。</li>
</ul>
</blockquote>
<p><span class="math display">\[x_{i}+\sum_{j=m+1}^{n}\left\lfloor y_{i j}\right\rfloor x_{j} \leq y_{i 0}\]</span></p>
<blockquote>
<ul>
<li>因为<span class="math inline">\(\left\lfloor y_{i j}\right\rfloor \leq y_{i j}\)</span>，所以对于任何满足上面等式约束的向量<span class="math inline">\(\mathbf{x} \geq 0\)</span>，也满足这一不等式约束。</li>
<li>任何可行解<span class="math inline">\(\mathbf{X}\)</span>都满足该不等式约束。</li>
</ul>
</blockquote>
<p>此外，对于任何整数可行向量<span class="math inline">\(\mathbf{x}\)</span>，不等式约束的左边都是整数，因此，任意整数可行解<span class="math inline">\(\mathbf{x}\)</span>还满足</p>
<p><span class="math display">\[x_{i}+\sum_{j=m+1}^{n}\left\lfloor y_{i j}\right\rfloor x_{j} \leq\left\lfloor y_{i 0}\right\rfloor\]</span></p>
<blockquote>
<ul>
<li>将前面的等式约束减去不等式约束，可以得到任意整数可行解<span class="math inline">\(\mathbf{x}\)</span>都满足的约束条件。</li>
</ul>
</blockquote>
<p><span class="math display">\[\sum_{j=m+1}^{n}\left(y_{i j}-\left\lfloor y_{i j}\right\rfloor\right) x_{j} \geq y_{i 0}-\left\lfloor y_{i 0}\right\rfloor\]</span></p>
<blockquote>
<ul>
<li>通过引入一个剩余变量<span class="math inline">\(x_{n+1}\)</span>可以把上面新的LP变为标准形式。</li>
</ul>
</blockquote>
<p><span class="math display">\[\sum_{j=m+1}^{n}\left(y_{i j}-\left\lfloor y_{i j}\right\rfloor\right) x_{j}-x_{n+1}=y_{i 0}-\left\lfloor y_{i 0}\right\rfloor\]</span></p>
<p>7、现在可以用单纯形法求解新的线性规划，并检查得到的最优基本可行解。</p>
<blockquote>
<ul>
<li>如果是整型，那么计算结束。</li>
<li>如果不是，那么就引入其他割面，并重复上面的过程。</li>
</ul>
</blockquote>
<p>注意，求解过程中引入的松弛变量，并不必须为整数。</p>
<h1 id="仅含等式约束的优化问题重新看">仅含等式约束的优化问题（重新看）</h1>
<p><span class="math display">\[\begin{matrix}
    minimize &amp; f\left ( \mathbf{x} \right ) &amp; \\ 
    st. &amp; \mathbf{h}_{i}(\mathbf{x})=\mathbf{0} &amp; i=1, \ldots, m
\end{matrix}\]</span></p>
<p>其中<span class="math inline">\(\mathbf{h}=\left[h_{1}, \dots, h_{m}\right]^{T}\)</span>，并且<span class="math inline">\(\mathbf{h}: \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}, m &lt; n\)</span>。如果函数<span class="math inline">\(\mathbf{h}\)</span>是连续可微的，那么<span class="math inline">\(\mathbf{h} \in \mathcal{C}^{1}\)</span>。</p>
<p>定义（<span class="math inline">\(m=1\)</span>的情况）：当<span class="math inline">\(\nabla h\left(\mathbf{x}^{*}\right) \neq 0\)</span>时，那么可行点<span class="math inline">\(\mathbf{x}^{*}\)</span>为该约束的正则点。</p>
<p>如果<span class="math inline">\(S\)</span>上的所有点都是正则点，那么曲面<span class="math inline">\(S\)</span>的维数是<span class="math inline">\(n-m\)</span>。</p>
<h2 id="拉格朗日条件">拉格朗日条件</h2>
<p>首先定义<span class="math inline">\(m=1\)</span>的简单情况</p>
<p><span class="math display">\[\begin{matrix}
    minimize &amp; f\left ( \mathbf{x} \right ) &amp; \\ 
    st. &amp; \mathbf{h}\left ( \mathbf{x} \right )=0
\end{matrix}\]</span></p>
<p>其中<span class="math inline">\(f: \mathbb{R}^{n} \rightarrow \mathbb{R}\)</span>，<span class="math inline">\(h: \mathbb{R}^{n} \rightarrow \mathbb{R}\)</span></p>
<p>（<span class="math inline">\(m=1\)</span>的情况）假设<span class="math inline">\(\mathbf{x}^{*}\)</span>局部最小值，并且是正则点。那么存在一个标量<span class="math inline">\(\lambda^{*}\)</span>，使得</p>
<p><span class="math display">\[\nabla f\left(\mathbf{x}^{*}\right)+\lambda^{*} \nabla h\left(\mathbf{x}^{*}\right)=0\]</span></p>
<p>换句话说，<span class="math inline">\(\nabla f\left(\mathbf{x}^{*}\right)\)</span>和<span class="math inline">\(\nabla h\left(\mathbf{x}^{*}\right)\)</span>是平行的，<span class="math inline">\(\lambda^{*}\)</span>称为拉格朗日乘子。</p>
<h1 id="含不等式约束的优化问题">含不等式约束的优化问题</h1>
<p>1、<span class="math inline">\(\mu^{*} \geq 0\)</span></p>
<p>2、<span class="math inline">\(D f\left(\mathbf{x}^{*}\right)+\mu^{* T} D \mathbf{g}\left(\mathbf{x}^{*}\right)=\mathbf{0}^{T}\)</span></p>
<p>3、<span class="math inline">\(\mu^{* T} \mathbf{g}\left(\mathbf{x}^{*}\right)=0\)</span></p>
<p>4、<span class="math inline">\(\mathbf{g}\left(\mathbf{x}^{*}\right) \leq 0\)</span></p>
<h1 id="凸优化问题-1">凸优化问题</h1>
<p>如果<span class="math inline">\(\Omega\)</span>是凸集，那么任意<span class="math inline">\(\mathbf{y}, \mathbf{z} \in \Omega\)</span>，并且<span class="math inline">\(\alpha \in(0,1)\)</span>，都有<span class="math inline">\(\alpha \mathbf{y}+(1-\alpha) \mathbf{z} \in \Omega\)</span>。</p>
<h2 id="证明omegamathbfx-mathbfx-geq-0是凸集">证明<span class="math inline">\(\Omega=\{\mathbf{x}: \mathbf{x} \geq 0\}\)</span>是凸集</h2>
<p>1、假设<span class="math inline">\(\mathbf{y}, \mathbf{z} \in \Omega\)</span>，并且<span class="math inline">\(\alpha \in(0,1)\)</span>。</p>
<p>2、那么<span class="math inline">\(\mathbf{x}=\alpha \mathbf{y}+(1-\alpha) \mathbf{z} \in \Omega\)</span>。</p>
<p>3、若要要属于<span class="math inline">\(\Omega\)</span>，那么<span class="math inline">\(\mathbf{x}\)</span>中每个都必须<span class="math inline">\(\geq 0\)</span>。</p>
<p>4、每个<span class="math inline">\(\mathbf{x}=\left[x_{1}, \ldots, x_{n}\right]^{T}\)</span>满足<span class="math inline">\(x_{i}=\alpha y_{i}+(1-\alpha) z_{i}\)</span>。</p>
<p>5、就可以知道<span class="math inline">\(y_{i}, z_{i}, \alpha, 1-\alpha \geq 0\)</span>。</p>
<p>6、因此，<span class="math inline">\(x_{i} \geq 0\)</span>，所以<span class="math inline">\(\mathbf{x} \geq 0\)</span>，即<span class="math inline">\(\mathbf{x} \in \Omega\)</span>，所以<span class="math inline">\(\Omega\)</span>是凸集。</p>
<h2 id="凸方程">凸方程</h2>
<blockquote>
<p>定理：如果一个方程<span class="math inline">\(f\)</span>在<span class="math inline">\(\Omega\)</span>上是凸函数，那么任何不同的<span class="math inline">\(\mathbf{x}, \mathbf{y} \in \Omega\)</span>，并且<span class="math inline">\(\alpha \in(0,1)\)</span>，都有</p>
</blockquote>
<p><span class="math display">\[f(\alpha \mathbf{x}+(1-\alpha) \mathbf{y}) \leq \alpha f(\mathbf{x})+(1-\alpha) f(\mathbf{y})\]</span></p>
<p>如果<span class="math inline">\(f\)</span>是严格凸函数，那么<span class="math inline">\(\leq\)</span>会替换为<span class="math inline">\(&lt;\)</span>。</p>
<h2 id="验证二次型的凸函数">验证二次型的凸函数</h2>
<blockquote>
<p>命题：假设有二次型函数<span class="math inline">\(f(\mathbf{x})=\mathbf{x}^{T} \mathbf{Q} \mathbf{x}\)</span>，并且<span class="math inline">\(\mathbf{Q}=\mathbf{Q}^{T}\)</span>。假如<span class="math inline">\(\Omega\)</span>为凸集，那么<span class="math inline">\(f\)</span>是在<span class="math inline">\(\Omega\)</span>上是凸集的条件是</p>
</blockquote>
<p><span class="math display">\[(\mathbf{x}-\mathbf{y})^{T} \mathbf{Q}(\mathbf{x}-\mathbf{y}) \geq 0\]</span></p>
<p>并且所有的<span class="math inline">\(\mathbf{x}, \mathbf{y} \in \Omega\)</span>。</p>
<h2 id="凸函数的另一个解释">凸函数的另一个解释</h2>
<blockquote>
<p>定理：函数<span class="math inline">\(f\)</span>是凸函数的条件是所有<span class="math inline">\(\mathbf{x}, \mathbf{y} \in \Omega\)</span>，并且</p>
</blockquote>
<p><span class="math display">\[f(\mathbf{y}) \geq f(\mathbf{x})+D f(\mathbf{x})(\mathbf{y}-\mathbf{x})\]</span></p>
<blockquote>
<p>定理：如果函数<span class="math inline">\(f\)</span>是凸函数，那么黑塞矩阵<span class="math inline">\(\mathbf{F}(\mathbf{x}) \geq 0\)</span>，并且<span class="math inline">\(\mathbf{x} \in \Omega\)</span>。</p>
</blockquote>
<h2 id="凸优化的fons">凸优化的FONS</h2>
<p>1、约束：对于所有的可行方向<span class="math inline">\(\mathbf{d}\)</span>，都有<span class="math inline">\(\mathbf{d}^{T} \nabla f\left(\mathbf{x}^{*}\right) \geq 0\)</span>。</p>
<p>2、<span class="math inline">\(\nabla f\left(\mathbf{x}^{*}\right)=0\)</span>。</p>
<p>3、满足朗格朗日条件<span class="math inline">\(\Omega=\{\mathbf{x}: \mathbf{h}(\mathbf{x})=0\}\)</span>。</p>
<p>4、满足KKT条件<span class="math inline">\(\Omega=\{\mathbf{x}: \mathbf{h}(\mathbf{x})=0, \mathbf{g}(\mathbf{x}) \leq 0\}\)</span>。</p>
]]></content>
      <categories>
        <category>课程</category>
      </categories>
  </entry>
  <entry>
    <title>数据预处理-数据转换</title>
    <url>/data-preprocessing/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>数据预处理</category>
      </categories>
      <tags>
        <tag>数据预处理</tag>
        <tag>数据转换</tag>
      </tags>
  </entry>
  <entry>
    <title>【2018】Deep Forest</title>
    <url>/deep-forest/</url>
    <content><![CDATA[<h1 id="摘要">摘要</h1><p>目前的深度学习模型大多建立在神经网络的基础上，即通过反向传播进行训练得到的多层参数化可微非线性模块。在本论文中，作者探讨了构建一个不可微模块的深度模型，并且作者推断深度神经网络的成功，其背后有三个特征，分别是逐层处理、模型中的特征转换和足够的模型复杂度。因此，作者提出了gcFroest方法，该方法能够生成具有上面三个特征的深度森林。该方法是一个决策树集成方法，相比于深度神经网络具有更少的参数，并且模型的复杂度能够通过数据依赖的方法来进行调节。实验表明，该方法对于超参具有很强的鲁棒性，在大多数情况下，即使不同领域的数据，该方法也可以在默认设置下获得较好的性能。该研究开创了不可微模块在深度学习中的应用，并且展示了不适用反向传播构建深度学习模块的可能性。</p><a id="more"></a>

<h1 id="介绍">介绍</h1>
<p>深度学习已经成为各领域的的热点。那么，什么是深度学习？从crowd得到的答案可能是”深度学习是使用深度神经网络的机器学习的子领域“。事实上，深度神经网络（DNNs）再视觉和语音上的成功导致了深度学习的兴起，并且当前所有的深度学习应用都基于神经网络进行构建，或者更专业的说，是通过反向传播训练的多层参数化可微非线性模型。</p>
<p>虽然深度神经网络很强大，但是它有许多的不足。首先，DNNs有许多超参，并且学习的性能依赖于详细的参数调整。事实上，已经有许多学者使用了卷积神经网络，但由于卷积层的结构，他们实际上使用了不同的学习模型来处理不同的操作。事实上，这个问题使得DNNs的训练非常困难，并使得DNNs更像一个艺术，而不是科学或工程，而且DNNs的理论分析也非常困难，因为有太多的干扰因素和几乎无限的参数组合。第二，众所周知，DNNs训练需要大量的训练数据，因此DNNs就很难应用于小规模训练数据量的任务，有时甚至中等规模的训练数据也会失败。需要注意的是，在大数据时代，由于数据标记的成本更好，许多实际的任务缺乏足够数量的数据标记，因此这也导致了DNNs在这些任务中性能较差。众所周知，神经网络是一个黑盒，即决策过程很难被理解，并且学习行为也很难进行理论分析。此外，在训练神经网络之前，并要把网络结构确认，并且预先确定模型的复杂度。作者猜测，深度模型的复杂度通常比深度模型要复杂的多，正如在最近的研究中表明许多DNNs性能的提高是通过快捷链接、剪枝、二值化等方法，因此这些操作都是原来的网络更加简单，并且降低了模型的复杂度。假如模型模型的复杂度能通过数据来动态决定，那就更好了。值得注意的是，尽管DNNs已经发展的很好，但在许多问题上DNNs的表现并不十分优秀，有时甚至不足，例如随机森林、XGBoost仍然在许多Kaggle比赛中获奖。</p>
<p>作者相信为了解决学习任务的复杂度问题，必须对学习模型进行深入研究。然而，目前的深度模型总是建立在神经网络的基础上。基于上面的原因，有充分的理由去探索非神经网络模式的深度模型，换句话说，考虑是否可以与其他模块一起实现深度学习，因为它们有自己的优势，如果能够深入，可能会显示出巨大的潜力。特别是，因为神经网络是多层参数化可微非线性模型，在现实世界中，不是所有的属性都是可微的或最好的模型都是可微的，在本文中，我们试图解决这个基本问题：</p>
<p>”深度学习是否可以使用不可微模型进行实现？“</p>
<p>本文的结果可以帮助理解更多重要的问题，如（1）深度模型是否就是DNNs（或者，深度模型只能够通过可微模型进行构建）；（2）训练深度模型时候可以不适用反向传播？（反向传播必须要可微）；（3）是否有可能有一个深度模型比现有的其他模型（如随机森林或XGBoost）在任务上的表现更好？事实上，机器学习社区已经开发了许多不可微的学习模块，并且理解这些基于不可微模型的深度模型结构将有利于解决这些模块是否可以在深层学习中使用的问题。</p>
<p>本文，作者扩展了他们的初步研究，并提出了使用gcForest（多粒度级联森林）方法来构建深度森林，该方法不同于使用神经网络的深度模型。该模型是一个新颖的具有级联结构的集成决策树，并通过森林进行表达学习。它的表达学习能力可以通过多粒度扫描进一步增强，这使gcForest具有上下文或结构感知能力。其中级联的等级可以被自动确定，使得模型的复杂度能够依赖于数据来进行确认，而不是在训练前手动确定，这使得gcForest也能够在小规模数据上也能表现的很好，并且使得用户能够根据可用的计算资源来控制训练成本。此外，gcForest相比DNNs具有更少的超参。更好的消息是，它的性能对超参数设置非常健壮；通过实验表明，在大多数情况下它能够在默认设置下就获得优异的性能，即使是不同域的不同数据。</p>
<h1 id="灵感">灵感</h1>
<h2 id="从dnns获取的灵感">从DNNs获取的灵感</h2>
<p>普遍认为，深度神经网络的成功关键是表达学习能力。那么在DNNs中，表达学习的关键是什么？作者相信是逐层处理。图1提供了一个说明，当层从底部向上时，更高层次的抽象特性会出现。</p>
<img src="/deep-forest/layer-by-layer-processing.png" class title="深度神经网络中的逐层处理">
<p>图1：深度神经网络中的逐层处理：从底部向上，更高层次的抽象特性会出现</p>
<p>如果其他问题不变，那么模型的复杂度越高（即模型容量更大）就会获得更强的学习能力，那么将DNNs的成功性归因于巨大的模型复杂度听起来是合理的。然而，但这不能解释为什么浅层网络不如深层网络来的成功，因为可以通过增加无限的隐藏层单元来提高浅层网络的复杂度。因此，模型的复杂度不能用来说明DNNs的成功。相反，作者推测逐层处理是DNNs中最重要的因素之一，因为在平面网络（例如单隐层网络）中，不管其复杂性有多大，都不具备逐层处理的特性。虽然作者没有严格的论证，但这一猜想为gcForest的设计提供了重要的启示。</p>
<p>但是对于其他的一些学习模型，如决策树和Boosting，这些模型也都是逐层处理，但是它们却没有DNNs成功。作者认为，最重要的区别因素是，与图1所示生成的新特征的DNNs相比，决策树和Boosting在学习过程中总是在原始特征上进行表达，而没有创建新的特征，换句话说，就是模型没有对特征进行转换。此外，DNNs可以被赋予任意高度的模型复杂度，而决策树和Boosting只具有有限的模型复杂度。虽然模型复杂度不能解释DNNs的成功，但它仍然是重要的因素，因为大型模型需要有大型的训练数据。</p>
<p>因此，作者推测DNNs背后三个重要因素是，逐层处理、模型内的特征转换，和足够的模型复杂度。作者将尝试将这些特性赋予非NN模型的深度模型。</p>
<h2 id="从集成学习获得的灵感">从集成学习获得的灵感</h2>
<p>集成学习是机器学习中的一个范式，即通过多个学习器进行训练，并且通过组合完成一个任务。众所周知，一个集成学习器能够达到更好的性能，比单一的学习器。</p>
<p>为了构建更好的集成学习器，每一个独立学习器必须精准，并且多样。只结合精确的学习器往往不如结合了一些准确的学习器和一些相对较弱的学习器，因为互补性比单纯的准确性更重要。实际上，从误差模糊度分解理论上导出了一个美丽的公式。</p>
<p><span class="math display">\[\begin{equation}
    E=\bar{E}-\bar{A}
\end{equation}\]</span></p>
<p>其中<span class="math inline">\(E\)</span>表示一个集成学习器的错误率，<span class="math inline">\(\bar{E}\)</span>表示在集成学习器中单独分类器的平均错误率，<span class="math inline">\(\bar{A}\)</span>表示各个分类器之间的平均歧义度，也被称为多样性。式子1表示，每个独立分类器越精确，并且多样性越高，那么集成学习器就更好。这为构建一个集成学习器提供了指导，但是该式子不能个作为优化的目标函数，因为歧义项在推导过程中是数学定义的，不能直接操作。后来，集成学习社区设计了许多多样性的度量指标，但是这些多样性的定义没有一个被广泛的接受。事实上，“什么是多样性？”仍是集成学习中的圣杯问题，近期的一些努力可以在其中找到。</p>
<p>在实际应用中，多样性增强的基本策略是在训练过程中引入基于启发式的随机性。粗略地说，主要有四类机制。</p>
<blockquote>
<ul>
<li>1、数据样本操作。它通过生成不同的数据样本来训练每一个学习器。例如，Bagging中的自主采样，AdaBoost中的连续重要性抽样。</li>
<li>2、输入特征操作。它通过生成二重特征子空间来训练每个学习器。例如，通过随机子空间方法为每个学习器随机选择一个子集特征。</li>
<li>3、学习参数操作。它通过为每个学习设置不同参数来生成不同的学习器。例如，为每个单独的神经网络设置不同的初始权值，而不同的分裂选择可以应用于每个单独的决策树。</li>
<li>4、输出表达操作。它通过使用不同的输出表征来产生不同的单个学习器。例如，通过ECOC方法通过纠错编码输出，而反转输出方法则随机改变一些训练实例的标签。</li>
</ul>
</blockquote>
<p>文献表明不同的机制可以一起使用。但值得注意的是，这些机制也不是一直有用。例如，数据样本处理对于稳定的学习器来说效果不好，因为它们的表现不会因为训练数据的轻微修改而显著改变。</p>
<p>下一节将会介绍gcForest，该方法可以被看作决策树的集成方法，几乎使用所有类别的机制来增强多样性。</p>
<h1 id="gcforest方法">gcForest方法</h1>
<h2 id="级联森林结构">级联森林结构</h2>
<p>深度神经网络的表达学习主要依赖于对原始特征的逐层处理。在这个认识的启发下，gcForest使用级联结构（如图2所示），即每个级联层都接收上一层特征信息处理结构，并把当前层的处理结果输出到下一层中。</p>
<img src="/deep-forest/cascade-forest-structure.png" class title="级联森林结构">
<p>图2：级联森林结构。假设每个层级都由两个随机森林（黑色）和两个完全随机森林（蓝色）组成。假设有三个类要预测，因此每个林将输出一个三维分类向量，然后将其连接起来以重新表达原始特征输入。</p>
<p>每层都是一个集成决策树森林，即一个集合。这里，作者包含了不同类型的森林以鼓励森林的多样性，因为多样性是集成结构的关键。为了简单，假设使用两个完全随机树森林和两个随机森林。每个完全随机树森林包含500个完全随机树，这些完全随机树都通过在每个树的结点随机选择一个特征进行分割，并且生长树直到只有叶子，每个叶节点仅包含相同类型的实例。同样的，每个随机森林包含500个树，这些树通过随机选择<span class="math inline">\(\sqrt{d}\)</span>个特征作为候选特征（<span class="math inline">\(d\)</span>是输入特征的数量），并且选择一个最大基尼指数作为分割点。在每个森林中树的数量是一个超参。</p>
<p>假定有一个实例，每个森林能够通过计算得到在每个叶节点上训练实例对于不同类别的概率，并对整个森林中的所有树进行平均，从而产生一个类别分布估计，如图3所示，其中红色部分是实例遍历到叶节点的路径。</p>
<img src="/deep-forest/traverses-path.png" class title="遍历路径">
<p>图3：类别向量生成。叶节点中不同标记表示不同类别。</p>
<p>类别的估计分布形成了一个类向量，并且与原始的特征向量连接后输入到下一级级联层中。例如，假设有三个类，那么每个森林中的每一个都会产生一个三维的类别向量，并且级联层的下一级将会收到<span class="math inline">\(12\left ( =3\times 4 \right )\)</span>个增强特征。</p>
<p>需要注意的是，作者采用了最简单的类别向量形式，即相关实例所在叶节点上的类别分布。但是很明显，这样少量的增强特征只能提供非常有限的增强信息，并且当原始特征向量是高维时，它很可能被淹没。通过实验中表明，这种简单的特征增强已经是有益的。假设有更多增强特征的引入，那么可以获得更好的效果。事实上，还可以结合更多的特征，例如表示先验分布的父节点的类别分布，以及表示互补分布的兄弟节点的类别分布等。这些可能可以在未来进行探索。</p>
<p>为了降低过拟合的风险，通过每个森林产生的类别向量都是通过<span class="math inline">\(k\)</span>份的交叉验证产生的。具体来说，每个实例将被用作训练数据<span class="math inline">\(k-1\)</span>次，并产生<span class="math inline">\(k-1\)</span>个类别向量，然后对这些向量进行平均，以产生最终类别向量作为下一级级联层的增强特征。在扩展新层后，整个级联结构的性能在验证集上进行评估，假设性能没有明显增加，那么训练过程终止，因此就确定了整个级联结构的数量。需要注意的是，当训练成本或者计算资源有限时，也可以使用训练误差而不是校验验证误差的方式来控制级联结构的增加。与大多数模型复杂度固定的深度神经网络相比，gcForest能够通过在适当的时候终止训练来自适应模型复杂度。这使得它能够适应不同规模的训练数据，而不仅仅限于大规模的训练数据。</p>
<h2 id="多粒度扫描">多粒度扫描</h2>
<p>深度神经网络非常适合处理特征之间的关系，如卷积神经网络能有效处理原始图像中关键像素之间的重要关系；循环神经网络能有效处理序列数据中关键的序列关系。受此启发，作者采用多粒度扫描的方法来增强级联森林。</p>
<img src="/deep-forest/multi-grained-scanning.png" class title="多粒度扫描">
<p>图4：使用滑动窗口扫描重新表示特征。假设有三个类别，原始特征为400维，滑动窗口为100维。</p>
<p>如图4所示，滑动窗口用于扫描原始特征。假设有400个原始特征，窗口大小为100。对于序列数据，通过滑动一个特征的窗口产生一个100维的特征向量，那么一共产生301个特征向量。假如原始特征具有空间相关性，那么一个<span class="math inline">\(20\times 20\)</span>的面板就由400个像素组成，然后一个<span class="math inline">\(10\times 10\)</span>的窗口能产生121个特征向量（即121个<span class="math inline">\(10\times 10\)</span>的面板）。从训练集的正例或负例中提取的特征向被视为正例或者负例，然后通过这些正例或负例生成类别向量，如3.1节所示，从相同大小的窗口中提取的实例被用于训练完全随机树森林和随机森林，然后生成类别向量，这些向量作为转换后的特征与原始特征进行拼接。如图4所示，假设有3个类别和一个100维的滑动窗口，那么每个森林产生301个3维的类别向量，从而得到与原始400维原始特征向量相对应的1806维变换特征向量。</p>
<p>对于从滑动窗口提取的实例，只需为这些实例分配原始训练示例标签，其中有些标签的分配是不正确的。例如，假设某个原始训练实例是“car”标签的正例，很明显许多提取的实例并不包含“car”，因此这些正例将会被分配为错误的标签。这实际上与翻转输出方法有关，翻转输出方法是集成多样性增强的输出表达操作的代表。</p>
<p>注意，当转换的特征向量太长而无法存储时，可以使用特征采样，例如，可以通过对滑动窗口扫描生成的实例进行子采样，因为完全随机的树不依赖于特征分割选择，而随机的林对不准确的特征分割选择不敏感。这种特征采样过程还与随机子空间方法有关，随机子空间方法是集成多样性增强中输入特征操作的代表。</p>
<p>图4只显示了一种尺寸的滑动窗口。通过使用多个不同大小的滑动窗口，将生成不同粒度的特征向量，如图5所示。</p>
<img src="/deep-forest/multiple-sizes-windows.png" class title="多尺度滑动窗口">
<p>图5：gcForest的整体过程。假设有三个分类需要预测，原始的特征有400维，并且有三个尺度的滑动窗口。</p>
<h2 id="总体过程和超参">总体过程和超参</h2>
<p>图5展示了gcForest的整体过程。假设原输入有400个原始特征，在多粒度扫描中有三个滑动窗口大小。如果有<span class="math inline">\(m\)</span>个训练实例，大小为100个特征的滑动窗口将会产生由<span class="math inline">\(301 \times m\)</span>个100维组成的训练数据集。这些数据将被用作训练完全随机树森林和随机森林，每个森林包含500棵树。假设由三个类别需要被预测，那么按照3.1的描述，就会有得到一个1806维的特征向量。这些训练数据的转换将被用于级联森林的第一级训练。</p>
<p>类似的，如果滑动窗口的大小分别是200和300个特征，那么将会为每个原始训练实例生成1206维和606维的特征向量。将转换后的特征向量与上一级生成的类别向量相结合，分别训练第二级和第三级森林。此过程将重复，直到验证性能收敛。换句话说，最终的模型是一个级联模型，每个级联节点都由多个级别组成，每个级别对应于一个扫描颗粒，如图5所示，第一个级联节点包含从层<span class="math inline">\(1_{A}\)</span>到层<span class="math inline">\(1_{C}\)</span>。对于不同的任务，用户可以在计算资源允许的情况下尝试更多的扫描颗粒。</p>
<p>给定一个测试实例，通过多粒度扫描得到其对应的转换特征表达，然后通过级联层直到最后一级。通过在最后一级聚合四个3维类向量，并用最大聚集值的类来获得NAR预测。最后的预测将通过在最后一级聚合四个3维类向量，并以最大聚集值的类别来获得。</p>
<p>表1总结了在实验环节使用的深度神经网络和gcForest的默认超参。</p>
<p>表1：默认设置和超参的总结。加粗部分是影响较大的超参；“？”表示默认值未知，或通常需要为不同的任务设置不同的参数。</p>
<img src="/deep-forest/hyper-parameters.png" class title="默认超参">
<h1 id="实验">实验</h1>
<h1 id="相关工作">相关工作</h1>
<p>gcForest是一种决策树集成方法。集成方法是一种强有力的机器学习技术，即对于一个任务集成了多个学习器。事实上，一些研究表明，利用具有深神经网络特征的随机森林等集成方法，其性能甚至优于单纯使用深神经网络，但是，作者使用集成方法目的并不是性能，而是构造一个非深度神经网络模式的深度模型。通过使用级联森林结构，作者希望赋予模型具有逐层处理、模型特征变换和足够的模型复杂度的特性。</p>
<p>在视觉任务中被广泛使用的随机森林是最成功的集成方法。近几年，人们发现完全随机树森林非常有用，如在异常检测中使用iForest，以及在数据流中处理新出现的分类的方法sencForest等。gcForest提供了另一个例子，展示了完全随机树森林的有用性。</p>
<p>许多研究都尝试在神经网络中连接随机森林，如将级联随机森林转换为卷积神经网络或利用随机森林来帮助初始化神经网络等。这些工作通常基于早期的将树与神经网络进行连接的研究，例如把树映射到网络、树结构的神经网络等。这些研究的目标都与本文不同。实际上，这些工作产生的最终模型都基于一个可微模型，而作者则是试图开发一个基于不可微模型的深度模型。</p>
<p>gcForest的多粒度扫描过程使用不同大小的滑动窗口来检测数据，这与小波等多分辨率检测过程有一定的关系。对于每个滑动窗口，都可以从一个训练实例中生成一组实例，这与多实例学习的bag生成器有关。特别是，图4的底部如果应用于图像，可以被视为<span class="math inline">\(SB\)</span>图像包生成器。</p>
<p>gcForest的级联过程也与Boosting有关，Boosting能够自动决定集成中的学习器的数量，尤其是级联Boosting过程在目标检测任务中取得了巨大的成功。注意的是，当使用多粒度时，gcForest级联结构中的每一层都包含了多个级，即多层级联结构，每一层都是一组合集。与以往对集成算法的研究不同，如使用Bagging作为基础学习器来Boosting，gcForest将同一级的集合用于特征的重新表达。</p>
<p>将一级的学习器输出作为另一个级学习器的输入是stacking的关系。基于对stacking的研究，作者使用交叉验证程序从一级输出生成下一级的输入。需要注意的是，stacking很容易造成过拟合，因此在深度模型中不能只使用stacking。</p>
<p>要构建一个好的集成模型，众所周知需要每个学习器都精确并且多样，但是没有被广泛接受的关于多样性的定义。因此，研究人员通常尝试通过启发式的方法来增强多样性，比如作者在每一个级中使用不同类型的森林来实现多样性。实际上，gcForest利用了所有四大类多样性增强机制。</p>
<p>作为一种基于树的方法，gcForest可能比深度神经网络更容易进行理论分析，尽管这超出了本文的范围。实际上，最近一些关于深度学习的理论研究，似乎更接近于基于树的模型。</p>
<h1 id="未来的问题">未来的问题</h1>
<p>一个重要的问题是增强在预测过程中的特征再表达。gcForest的实现目前采用最简单的类别向量形式，即相关实例所在叶节点上的类别分布。但是当原始特征向量为高维时，这样少量的增强特征很容易被淹没。显然，可能涉及更多的特征，例如表达先验分布的父节点的类分布、表达互补分布的兄弟节点、决策路径编码等。直观地说，更多的特征可以使更多的信息被合并，尽管不一定有助于泛化。此外，较长的类向量可以使联合多粒度扫描过程，从而获得更灵活的重新表示。最近，作者发现决策树森林可以作为自动编码器。这一方面说明了自编码能力并不像人们以前所认为的那样是神经网络的一个特殊性质；另一方面，它揭示了森林可以对丰富的信息进行编码，从而为丰富的特征重新表示提供了巨大的潜力。</p>
<p>另一个重要的问题是加速和减少内存消耗。按照4.8节，建立更大的深层森林可能会导致更好的泛化性能，而计算资源对于训练更大的模型至关重要。事实上，DNNs的成功很大程度上得益于GPUs的加速，但遗憾的是，森林结构并不适合GPUs。一种可能是一些新的计算设备，例如MIC（多集成核心）体系结构的Intel KNL；另一种可能是使用分布式计算实现。当多粒度扫描产生的转换特征向量太长而无法容纳时，可以执行特征采样；这不仅有助于减少存储量，而且还提供了另一个增强集合多样性的通道。这有点像将随机树森林与随机子空间相结合，这是另一种强大的集成方法。除了随机抽样，有趣的是探索更智能的采样策略，如BLB，或在适当的时候进行特征散列。Hard Negative Mining策略有助于提高泛化性能，提高Hard Negative Mining效率的努力也有助于多粒度扫描过程。通过在不同粒度扫描、类别向量生成、森林训练、完全随机树生成等过程中重用部分组件，可以进一步提高gcForest的效率。在学习模型较大的情况下，采用二次学习策略可以将模型缩小为较小的模型，这不仅有助于减少存储量，而且有助于提高预测效率。</p>
<p>使用完全随机的森林不仅有助于增强多样性，而且还提供了一个利用未标记数据的机会。请注意，完全随机树的生长不需要标签，而标签信息仅用于注释叶节点。直观地说，对于每个叶节点，如果要根据节点上的大多数集群对节点进行注释，则可能只需要一个标记示例；如果节点中的所有集群都不可忽略，则可能只需要每个集群一个标记示例。这也为gcForest提供了整合主动学习和（或）半监督学习策略的机会。</p>
<h1 id="结论">结论</h1>
<p>本文试图解决不可微模块能否实现深度学习的问题？作者推测，在深度神经网络的秘密背后，有三个关键特性，即逐层处理、模型特征变换和足够的模型复杂度，并且作者试图将这些特性赋予非神经网络风格的深模型。作者提出了一种基于决策树的深度模型gcForest方法，该方法能够构造深度森林，训练过程不依赖于反向传播。与深层神经网络相比，gcForest具有更少的超参数，并且在实验中，即使使用相同的参数设置，也可以在不同的领域获得优异的性能。值得注意的是，还有其他的可能性来建造深林。作为一项开创性的研究，作者在这个方向上只做了一些探索。事实上，本文最重要的价值在于，它可以为非神经网络式的深度学习或基于不可微模块的深度模型打开一扇门。</p>
<p>在实验中，作者发现gcForest能够在广泛的任务上实现与深度神经网络高度竞争的性能。但在某些图像任务中，其性能较差。一方面，我们认为gcForest的性能可以显著提高，例如，通过设计更好的特征重新表示方案，而不是使用当前简单的分类向量。另一方面，不可忽视的是，在深森林刚刚诞生的20多年里，诸如CNNs这样的深层神经网络模型已经被大量的研究人员（工程师）所研究。此外，图像任务是DNNs的杀手级应用。一般来说，要想在其杀手级应用程序上击败强大的技术是过于雄心勃勃的；例如，线性核支持向量机在文本分类方面仍然是最先进的，尽管DNN已经热了很多年。事实上，深度森林并不是为了取代深度神经网络而发展起来的；相反，当深度神经网络并不优越时，例如当DNNs性能低于随机森林和XGBoost时，它提供了一种替代方法。有很多任务可以发现深层森林是有用的。</p>
]]></content>
      <categories>
        <category>论文</category>
      </categories>
      <tags>
        <tag>集成学习</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>【2019】Multi-Label Learning with Deep Forest</title>
    <url>/ml-df/</url>
    <content><![CDATA[<h1 id="摘要">摘要</h1><p>在对标签学习中，每个实例都都与多个标签相关，因此多标签学习的关键就是在构建模型中如何利用标签之间的相关性。深度神经网络方法经常把特征和标签信息进行联合放到一个潜在的空间，从而挖掘出标签之间的相关性。然而，这些方法的成功高度依赖于模型深度的精确选择。深度森林（Deep Forest）是一个基于树模型集成的深度学习框架，并且不依赖于反向传播。作者认为深度森林模型的优势在于非常适合解决多标签问题。MLDF方法具有两个机制：</p><a id="more"></a>

<blockquote>
<ul>
<li>度量感知特征重用机制：以置信度作为依据，重用前一层的较好标签。</li>
<li>度量感知层增长机制：保证了MLDF模型复杂度随性能的度量而逐步增加。</li>
</ul>
</blockquote>
<p>MLDF将同时面对两个问题的挑战：</p>
<blockquote>
<ul>
<li>通过限制模型的复杂度来防止过拟合的问题。</li>
<li>因为在多标签任务中有许多不同的指标，所以需要根据用户需求优化性能指标。</li>
</ul>
</blockquote>
<p>实验表明，MLDF不仅优于在9个基准数据集上使用的6个方法，并且还能够在多标签学习中发现标签之间的相关性和其他相关特性。</p>
<h1 id="介绍">介绍</h1>
<p>在多标签学习中，每个实例都具有多个标签，并且多标签学习的任务就是为一个未知实例预测一组相关标签集。多标签学习被广泛的用于多种问题中，如文本分类、场景分类、基因组功能分类、视频分类、化学品分类等。多标签学习任务在现实问题中无处不在，引起了越来越多的研究关注。</p>
<p>一个最直接的方法就是把多标签学习问题转发为每个独立标签的二分类问题，并且这种方法已经在实践中被广泛的使用。这种方法的目的是能够充分使用现有的具有较高性能的单标签分类训练方法，但是当标签空间较大时，这种方法会带来较高的计算代价。此外，这种方法忽略了标签上的信息对于其他相关标签的学习具有一定帮助的事实，因此就限制了该方法的预测性能。发现标签之间的相关已经被证明是提高多标签学习性能的关键。因此，越来越多的多标签许欸小方法被提出来发现和探索标签之间的相关性。</p>
<p>不同于传统的多标签方法，深度神经网络模型通常会尝试学习一个新的特征空间，并在顶部使用一个多标签分类器。最早使用网络结构来解决多标签问题的是BP-MLL，该方法不仅把每个节点的输出作为一个二元分类任务，并且还利用了与网络自身结构相关的标签相关性。后来，又提出了一个基于BP-MLL的较为简单的神经网络方法，该方法使用熵损失替代了原先的排序损失，并且使用了深度神经网络技术，从而使得在大规模文本分类中取得了较好的效果。但是，深度神经网络通常需要大量的训练数据，因此就不适合小规模的数据集。</p>
<p>基于深度学习的逐层处理、模型特征变换和足够的模型复杂度的本质，提出了深度森林。深度森林是一个基于决策树的集成深度模型，但是在训练过程中不适用反向传播。具有级联结构的深度森林集成系统能够像深度神经网络模型一样进行表示学习。深度森林具有较少的超参，训练起来也更加容易。目前，它在大规模金融欺诈检测、图像、文本重建等一系列任务上都取得了优异的性能。虽然，深度森林已经被证明在传统分类任务中的有效性，但是在之前的工作中，深层森林应用于多标签学习的潜力并没有被注意到。</p>
<p>深度森林的成功主要依赖于集成方法中的逐层特征转换，以及多标签学习时如何例如标签之间的相关性。针对这两个事实，提出了多标签深度森林方法。简单说，MLDF使用不同的多标签树来构建DF的基本结构，并且标签的相关性可以通过逐层的表示学习来获得。由于许多的评价指标会被提出，所以多标签学习的评价要比传统的分类任务要复杂的多。同时，不同的用户有不同的需求，所以同一个算法在不同的度量方式下有不同运行方式。为了在特定的度量上获得更好的性能，提出了两种机制，分别是度量感知特征重用机制和度量感知层增长机制。基于置信度筛选的度量感知特征重用机制，重用了前一层的良好表示。度量感知层增长机制则是通过各种性能度量来控制模型的复杂性。这两个主要贡献可以概括为如下：</p>
<blockquote>
<ul>
<li>第一次把DF引入到多标签学习中。所提出的级联结构和两种度量感知机制，使得MLDF方法可以同时处理多标签学习中的两个主要问题：根据用户需求优化不同的性能度量，以及利用在多标签深度神经网络的模型中经常观察到的各层中的标签相关性来过度拟合。</li>
<li>实验表明，所提出的MLDF能够在9个基础数据集和6给多标签方法上达到最佳性能。此外，度量感知特征重用机制和度量感知层增长机制被证明是必要的。此外，实验也表明，在基础树模型的使用，以及防止过拟合方面具有很高的灵活性。</li>
</ul>
</blockquote>
<h1 id="准备工作">准备工作</h1>
<p>本节将介绍各种性能度量方法，然后介绍基于树的多标签方法，这些方法是MLDF的基础。</p>
<h2 id="多标签性能度量">多标签性能度量</h2>
<p>多标签分类任务是从训练集<span class="math inline">\(\left \{ \left ( x_{i},y_{i} \right )\mid 1\leq i\leq m,x_{i}\in \mathbb{R}^{d},y_{i}\in \left \{ 0,1 \right \}^l \right \}\)</span>中获取一个分类器<span class="math inline">\(H\)</span>。假设多标签学习模型首先产生一个实数方程<span class="math inline">\(F:\mathcal{X}\rightarrow \left [ 0,1 \right ]^{l}\)</span>，该函数是标签相关性的置信度函数。通过阈值化可以从函数<span class="math inline">\(F\)</span>获得多标签分类器<span class="math inline">\(H:\mathcal{X}\rightarrow \left \{ 0,1 \right \}^{l}\)</span>。</p>
<p>多标签学习的性能度量有许多种，本文中使用了6个被广泛使用的评价指标。表1显示的就是这个评价指标的数学表达，<span class="math inline">\(\mathcal{Y}\)</span>表示正确的标签集，<span class="math inline">\(\mathcal{Y}_{i}\)</span>表示第<span class="math inline">\(i\)</span>行的标签矩阵，'<span class="math inline">\(+\)</span>'（'<span class="math inline">\(-\)</span>'）表示相关性（不相关）标记。Hanmming损失和宏观AUC是基于标签的度量，而one-error、coverage、ranking损失和平均精度是基于实例的度量。<span class="math inline">\(f_{ij}\)</span>表示第<span class="math inline">\(i\)</span>个实例在第<span class="math inline">\(j\)</span>个标签上的置信度，<span class="math inline">\(h_{ij}\)</span>表示第<span class="math inline">\(i\)</span>个实例在第<span class="math inline">\(j\)</span>个标签上的预测结果，<span class="math inline">\(rank_{F}\left ( x_{i},j \right )\)</span>表示实例<span class="math inline">\(x_{i}\)</span>在第<span class="math inline">\(j\)</span>个标签上的排序。例如<span class="math inline">\(f\left ( x_{i} \right )=\left [ 0.2,0.8,0.4 \right ]\)</span>，那么当阈值为0.5时，就可以得到<span class="math inline">\(h\left ( x_{i},j \right )=\left [ 0,1,0 \right ]\)</span>。此外，还可以得到<span class="math inline">\(\mathcal{Y}_{i}^{-}=\left \{ 1,3 \right \}\)</span>和<span class="math inline">\(\mathcal{Y}_{i}^{+}=\left \{ 2 \right \}\)</span>。当<span class="math inline">\(f_{i2}=0.8\)</span>时，可以得到<span class="math inline">\(rank_{F}\left ( x_{i},2 \right )=1\)</span>。</p>
<p>表1：定义6个多标签性的损失函数，<span class="math inline">\(\downarrow\)</span>表示越低性能越好，<span class="math inline">\(\uparrow\)</span>表示越高性能越好</p>
<img src="/ml-df/six-loss-function.png" class title="六个损失函数">
<h2 id="基于树的多标签方法">基于树的多标签方法</h2>
<p>基于树的夺标方法，如ML-C4.5和PCT都是通过多分类树方法来进行适配。ML-C4.5从C4.5进行适配，允许树的叶中有多个标签，其熵计算公式通过对每个标签的熵求和得到。预测聚类树（PCT）递归地将所有样本划分为更小的聚类，同时向下移动树。在测试过程中，多标签树的叶节点通过计算相关实例所在叶节点上不同类别训练实例的概率，返回样本属于每个类别的概率向量。最后，保存在每个叶节点中的信息就是该实例属于哪个标签的概率。</p>
<p>一棵树的能力是很有限的，但是一群树就可以大大提高性能。随机森林的预测聚类树（RF-PCT）和随机森林的ML-C4.5（RFML-C4.5）都是使用PCT和ML-C4.5作为基础分类器的聚合。与随机森林相同，这些森林都是使用Bagging，并且选择不同的特征集来获得不同基础分类器的多样性。给定一个测试实例，森林将通过平均所有树的结果来生成标签分布的估计值。</p>
<h1 id="提出的方法">提出的方法</h1>
<h2 id="mldf的框架">MLDF的框架</h2>
<img src="/ml-df/mldf-framework.png" class title="MLDF框架">
<p>图1：MLDF框架，每层都集成了2个不同的森林，上面是黑色，下面是蓝色</p>
<p>图1是MLDF框架。不同的多标签森林（上面的黑色森林和下面的蓝色森林）被集成在MLDF中的每一层。每一层<span class="math inline">\(layer_{t}\)</span>会获取对应的表达结果<span class="math inline">\(H^{t}\)</span>，并把表达结果传递给度量感知特征重用部分，度量感知特征重用部分在不同性能度量指标的指导下，通过重用在<span class="math inline">\(layer_{t-1}\)</span>学习到的特征<span class="math inline">\(G^{t-1}\)</span>来更新自己。新的特征<span class="math inline">\(G^{t}\)</span>（蓝色部分）将会与原始输入特征（红色部分）进行串联，从而形成一个整体并输入到下一层中。</p>
<p>在MLDF中，每个层都是一个集成森林。其中至关重要的就是多样的森林增长方法，通过这些方法来提高集成森林的性能。在传统的多分类问题中，每个特征随机取一个分裂点的极端随机树被用在多粒度级联森林（gcForest）。对于多标签学习问题，通过改变生成树时提取节点的方法来适配该方法。在MLDF中，以RF-PCT（随机森林的预测聚类树）来作为森林的基础模块，并且通过两个在森林中不同的方法来生成树节点，一个是考虑了每个特征的所有可能的分割点（RF-PCT），另一个则是随机考虑一个分割点（ERF-PCT）。因此，其他多标签树方法能够被集成到每一层中，如RFML-C4.5。</p>
<p>在2.2节中，当给一个实例，那么森林将会产生一个标签分布估计，可以把这个标签分布估计作为实例对于每个标签的置信度。每一层中的表达学习将采用度量感知特征重用，并把原始的输入特征输入到下一层中。具有丰富标签信息的实值表达将会被输入到下一层中，以便在下一层中MLDF更好的利用标签之间的相关性。</p>
<p>假设森林已经完成了学习，如图1所示，整个预测的过程可以总结如下：</p>
<blockquote>
<ul>
<li>把实例预处理为标准矩阵<span class="math inline">\(X\)</span>。</li>
<li>把实例矩阵<span class="math inline">\(X\)</span>传递给第一层，并且得到第一层的表达<span class="math inline">\(H^{1}\)</span>。</li>
<li>通过采用度量感知特征重用机制获取了<span class="math inline">\(G^{1}\)</span>。</li>
<li>把<span class="math inline">\(G^{1}\)</span>与原始的输入特征<span class="math inline">\(X\)</span>进行串联，并且把它们输入到下一层。</li>
<li>经过多层后，得到最终的预测结果。</li>
</ul>
</blockquote>
<h2 id="度量感知特征重用">度量感知特征重用</h2>
<p>PCT的特征分割准则与性能度量没有直接的关系，虽然度量的方式不同，但是每层生成的表达<span class="math inline">\(H^{t}\)</span>是相同的。因此，在不同度量指导下提出了度量感知特征重用机制来增强每层的表达。度量感知特征重用机制的核心思想是，如果当前层的置信度低于训练中确定的阈值，则在当前层上部分重用前一层的较好表达，从而使度量性能更好。因此，问题就在于如何根据用户需求确定具体的多标签度量的置信度。受多类分类问题置信度计算方法的启发，设计了不同多标签度量下的置信度计算方法。</p>
<p>Hamming损失主要关注单比特的正确性，one-error关注接近于1的元素，而其他损失函数则关注每行或者每列的排名。因此，设计合理的方法计算各种测度的置信度是关键。表2总结了每个度量方法的计算方式。矩阵<span class="math inline">\(P\)</span>是<span class="math inline">\(H^{t}\)</span>的平均，并且元素<span class="math inline">\(p_{ij}\)</span>表示<span class="math inline">\(Pr\left [ \hat{y}_{ij}=1 \right ]\)</span>。为了方便，当基于实例（标签）进行度量时，将按照降序排列<span class="math inline">\(P\)</span>中的每一行（列）的元素。特别对于Hanmming损失而言，作者计算了比特为正例或负例的置信度最大值。例如，当预测向量<span class="math inline">\(p_{\cdot j}=\left [ 0.9,0.6,0.4,0.3 \right ]\)</span>时，则置信度<span class="math inline">\(\alpha_{j}=\frac{1}{4}\left ( 0.9+0.6+0.6+0.7 \right )=0.7\)</span>。对于排名损失，作者计算了排名损失为零的概率，这意味着正例签领先于负例标签。例如，当预测向量<span class="math inline">\(p_{\cdot j}=\left [ 0.9,0.6,0.4,0.3 \right ]\)</span>时，那么就有5种可能的基准排序导致排序损失为零。因此，这座通过对这五种情况的概率求和来得到置信度。而macro-AUC的置信度和平均精度使用相似的方法进行定义。</p>
<p>表2：6个多标签度量的置信度计算方法，<span class="math inline">\(p_{i \cdot}\)</span>或<span class="math inline">\(p_{\cdot j}\)</span>已经按降序排序</p>
<img src="/ml-df/six-measure-confidence.png" class title="置信度计算">
<p>算法1总结的了度量感知特征重用的计算过程。由于基于标签度量和基于实例度量的多样性，因此就需要分别处理它们。特别是，基于标签的度量在计算置信度时是基于<span class="math inline">\(H^{t}\)</span>，而基于实例的度量在计算置信度时是基于每一行。在置信度计算完成后，如果置信度<span class="math inline">\(\alpha^{t}\)</span>低于阈值，那么就需要修正前一层的表达<span class="math inline">\(G^{t-1}\)</span>，最后用修正后的表达来修更新当前层<span class="math inline">\(G^{t}\)</span>表达。</p>
<p>算法1：度量感知特征重用算法</p>
<img src="/ml-df/measure-aware-feature-reuse.png" class title="度量感知特征重用算法">
<p>整个度量感知特征重用的过程并不依赖于真正的标签，而是通过训练过程中确定的阈值来判断每层表达的优劣。算法2就是确认阈值的过程，当评价指标的结果在<span class="math inline">\(layer_{t}\)</span>变差时，那么就把置信度<span class="math inline">\(\alpha^{t}\)</span>保存至置信度集<span class="math inline">\(S\)</span>中。然后，基于置信度集<span class="math inline">\(S\)</span>的阈值<span class="math inline">\(\theta_{t}\)</span>就被确认。简单地说，我们可以取<span class="math inline">\(S\)</span>的平均值。因为，置信度与度量的含义是一直的，因此阈值<span class="math inline">\(\theta_{t}\)</span>能够在度量感知特征重用中被有效的利用。</p>
<p>算法2：度量阈值的确认</p>
<img src="/ml-df/determine-threshold.png" class title="度量阈值的确认">
<h2 id="度量感知层增长">度量感知层增长</h2>
<p>虽然度量感知特征重用可以在各种度量指标下有效的增强各层的表达，但是这种机制不能对层增长进行影响，同时降低在训练过程中发生的过拟合问题。为了降低过拟合，并控制模型的复杂度，作者提出了度量感知层增长机制。</p>
<p>假如使用相同的数据来填充森林，并且直接进行预测，那么过拟合的风险就会增加。MLDF使用K-Fold交叉验证来缓解这个问题。对于某一个Fold，根据在其他Fold中训练的实例来对当前森林进行训练，并对当前Fold进行预测。每一层的表达生成通过窗帘每一个森林的预测来得到。</p>
<p>MLDF采用逐层构建。算法3展示了在MLDF训练过程中使用度量感知层增长的过程。输入为具有最大深度<span class="math inline">\(T\)</span>的层，评价指标<span class="math inline">\(M\)</span>，以及训练数据集<span class="math inline">\(\left \{ X,Y \right \}\)</span>。假设模型已经增长到<span class="math inline">\(T\)</span>层，那么训练将会停止。通常来说，每一层都会选择一个RF-PCT和一个ERF-PCT，并且随机选择<span class="math inline">\(\sqrt{d}\)</span>个特征作为每个森林候选特征。如森林的数量和树的深度等个这些MLDF中训练森林用到的参数都会在训练之前预先确定。为了希望每个层能够学习不同的表达，因此就需要为第<span class="math inline">\(i\)</span>层设置森林增长时，每个树的最大深度，或预先设置树的数量。在初始步骤中，记录了训练数据在每层中的性能向量<span class="math inline">\(p\)</span>需要根据不同的度量进行初始化。在每层中，首先需要拟合森林（Line3），并且获取每层的表达<span class="math inline">\(H^{t}\)</span>（Line4）。因此，就需要确认阈值<span class="math inline">\(\theta_{t}\)</span>（Line5），并且通过度量感知特征重用机制生成信息的表达（Line6）。最后把层<span class="math inline">\(layer_{t}\)</span>加到模型集<span class="math inline">\(S\)</span>中（Line14）。</p>
<p>层的成长是进行度量感知完成。在拟合完一层后，就需要计算度量的评价值。当度量值在最近的三层中没有太好的表现时（Line11），MLDF中层将会停止生长。同时，在训练集中性能最好的那个层的索引将会被记录，用于后续的预测。按照奥卡姆剃刀准则（Occam's razor rule），当性能相似时，越简单的模型越好。不过，虽然模型的性能并没有明显的改进，但最终的模型集应该保留至模型集<span class="math inline">\(S=\left \{ layer_{1},layer_{2},\cdots ,layer_{L} \right \}\)</span>中，并且之后的层将被删除。</p>
<p>算法3：度量感知层增长</p>
<img src="/ml-df/measure-aware-layer-growth.png" class title="度量感知层增长">
<p>不同的度量代表不同的用户需求，此时就需要根据不同的情况设置一个<span class="math inline">\(M\)</span>来记录所需的度量内容。因此就可以获得特定度量所对应的模型。一方面，模型的复杂度由<span class="math inline">\(M\)</span>进行控制，另一方面，与其他在训练过程中不能明确地考虑性能指标方法相比（如PCT），MLDF则具有更灵活，具有更好性能的特点。</p>
<h1 id="结论">结论</h1>
<p>在未来，MLDF的效率将会进一步提高，通过在森林训练过程中重用一些组件。虽然MLDF可以利用高阶的标签相关性，但在实验中只显示了二阶相关性。作者将试图找到一种如何使用高阶相关性的方法。此外，作者还计划在MLDF中嵌入FastXML等极端多标签树方法，并测试其在极端规模的多标签问题上的性能。</p>
<h1 id="参考资料">参考资料</h1>
<p>[1] <a href="https://www.zhuanzhi.ai/document/430049ace146346a4859fa4c111b1a16" target="_blank" rel="noopener">周志华团队：深度森林挑战多标签学习，9大数据集超越传统方法</a></p>
]]></content>
      <categories>
        <category>论文</category>
        <category>多标签学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>多标签学习</tag>
      </tags>
  </entry>
  <entry>
    <title>【2019】基于事件分析的告知学生综合评价模型构建与研究</title>
    <url>/base-on-event-analysis/</url>
    <content><![CDATA[<p><strong>期刊：</strong>《职业技术教育》，2019，40(21)：38-43</p><p><strong>作者：</strong>陶文寅，潘嘹</p><h1 id="摘要">摘要</h1><p>以事件数据分析理论为切入点，通过评估学生在校事件分值，建立学生在校事件影响力估算公式，结合高职学生的行为特点，初步构建高职学生综合评价模型，并通过事件模拟评估该模型的有效性。结果表明，基于事件分析的高职学生综合评价模型能够准确、客观地评价高职学生的在校表现，有效克服了主观评价体系中的缺点，为定量分析高职学生的在校表现提供了理论基础。</p><a id="more"></a>



<p><strong>关键字：</strong>学生评价；事件数据分析；定量分析</p>
<p><a href="https://pan.baidu.com/s/1lYCjL8NGLF-gUN0G9inPag" target="_blank" rel="noopener">下载地址</a></p>
<p>提取码: che7</p>
]]></content>
      <categories>
        <category>发表</category>
      </categories>
      <tags>
        <tag>教育教学</tag>
      </tags>
  </entry>
  <entry>
    <title>高等工程应用数学</title>
    <url>/math/</url>
    <content><![CDATA[<h1 id="考试题目">考试题目</h1><h2 id="第一题">第一题</h2><p>设<span class="math inline">\(Z\)</span>表示整数集</p><blockquote>
<ul>
<li>1、在<span class="math inline">\(Z\)</span>上定义运算：<span class="math inline">\(a \circ b = a+b+4\)</span>，求证<span class="math inline">\(\mathbb{Z}\)</span>按<span class="math inline">\(\circ\)</span>运算作为一个群。</li>
<li>2、如果在<span class="math inline">\(Z\)</span>上定义运算<span class="math inline">\(a \odot b = a^{k}\)</span>，说明<span class="math inline">\(\mathbb{Z}\)</span>在<span class="math inline">\(\odot\)</span>运算上不是一个群。</li>
</ul>
</blockquote><a id="more"></a>



<p>（注：题目中的“<span class="math inline">\(+\)</span>”以及“<span class="math inline">\(a^{k}\)</span>”都是指通常意义下数的加法和乘方）</p>
<p>1、证明群：运算封闭、结合律、有单位元、有逆元。</p>
<blockquote>
<ul>
<li>证明运算封闭</li>
</ul>
</blockquote>
<p><span class="math inline">\(\because \forall a, b \in z\)</span></p>
<p><span class="math inline">\(\therefore a \circ b=a+b+4 \in z\)</span></p>
<blockquote>
<ul>
<li>证明结合律</li>
</ul>
</blockquote>
<p><span class="math inline">\(\forall a, b, c \in z\)</span></p>
<p><span class="math inline">\(\because (a \cdot b) \cdot c=(a+b+4) \cdot c=a+b+4+c+4=a+b+c+8\)</span></p>
<p><span class="math inline">\(a \cdot (b \cdot c)=a \cdot (b+c+4)=a+b+c+4+4=a+b+c+8\)</span></p>
<p><span class="math inline">\(\therefore (a \cdot b) \cdot c=a \cdot (b \cdot c)\)</span></p>
<blockquote>
<ul>
<li>证明有单位元</li>
</ul>
</blockquote>
<p>令单位元为<span class="math inline">\(e\)</span>，<span class="math inline">\(\forall a \in \mathbb{Z}\)</span>，证明<span class="math inline">\(a \circ e=e \circ a=a\)</span></p>
<p>则<span class="math inline">\(a \circ e=a+e+4=a\)</span>，所以<span class="math inline">\(e=-4\)</span></p>
<p><span class="math inline">\(\because e \circ a=-4+a+4=a\)</span></p>
<p><span class="math inline">\(\therefore a \circ e=e \circ a=a\)</span></p>
<blockquote>
<ul>
<li>证明有逆元</li>
</ul>
</blockquote>
<p>令逆元为<span class="math inline">\(b\)</span>，<span class="math inline">\(\forall a \in \mathbb{Z}\)</span>，证明<span class="math inline">\(a \circ b=b \circ a=e\)</span></p>
<p>则<span class="math inline">\(a \circ b=a+b+4=e=-4\)</span>，所以<span class="math inline">\(b=-a-8\)</span></p>
<p><span class="math inline">\(\because b \circ a=b+a+4=-a-8+a+4=-4\)</span></p>
<p><span class="math inline">\(\because a \circ b\)</span>在<span class="math inline">\(\mathbb{Z}\)</span>上构成群。</p>
<p>2、证明群：运算封闭、结合律、有单位元、有逆元只要有一个不符合就不是群。</p>
<p>令<span class="math inline">\(a=2,b=-2\)</span>，那么<span class="math inline">\(a \odot b=a^{b}=2^{-2}=\frac{1}{4} \notin \mathbb{Z}\)</span></p>
<p>所以<span class="math inline">\(a \odot b = a^{k}\)</span>在<span class="math inline">\(\mathbb{Z}\)</span>不是一个群</p>
<h2 id="第二题">第二题</h2>
<p>设<span class="math inline">\(R\)</span>是一个环，若<span class="math inline">\(\forall a \in R\)</span>，都有<span class="math inline">\(a^{2} = a\)</span>，则称R为布尔环</p>
<blockquote>
<ul>
<li>1、求证：若<span class="math inline">\(R\)</span>是布尔环，则<span class="math inline">\(\forall a \in R\)</span>，<span class="math inline">\(a+a=0\)</span>。</li>
<li>2、设<span class="math inline">\(X\)</span>是一个集合，<span class="math inline">\(\Gamma\)</span>表示由<span class="math inline">\(X\)</span>的全部子集作为元素得到的集合，定义<span class="math inline">\(\Gamma\)</span>中的两种运算，<span class="math inline">\(A+B=(A \setminus B) \cup (B \setminus A)\)</span>和<span class="math inline">\(A \ast B = A \cap B\)</span>，求证<span class="math inline">\(\Gamma\)</span>在这两个运算下构成一个环，并且是布尔环。（注：“<span class="math inline">\(\setminus\)</span>”表示两个集合做差集）</li>
</ul>
</blockquote>
<p>1、<span class="math inline">\(\forall a,b \in R\)</span>，则有<span class="math inline">\(a+b \in R\)</span></p>
<p><span class="math inline">\(\because R\)</span>是一个布尔环</p>
<p><span class="math inline">\(\therefore (a+b)^{2}=a+b\)</span></p>
<p>又<span class="math inline">\(\because (a+b)^{2}=a^{2}+ab+ba+b^{2}\)</span>，并且<span class="math inline">\(R\)</span>是布尔环</p>
<p><span class="math inline">\(\therefore a^{2}=a,b^{2}=b\)</span></p>
<p><span class="math inline">\(\therefore (a+b)^{2}=a^{2}+ab+ba+b^{2}=a+ab+ba+b=a+b\)</span></p>
<p><span class="math inline">\(\therefore ab+ba=0\)</span></p>
<p><span class="math inline">\(\therefore ab=-ba\)</span></p>
<p>令<span class="math inline">\(b=a\)</span>，则<span class="math inline">\(a^{2}=-a^{2}\)</span></p>
<p><span class="math inline">\(\therefore 2a^{2}=0\)</span></p>
<p><span class="math inline">\(\therefore 2a=0\)</span></p>
<p><span class="math inline">\(\therefore a+a=0\)</span></p>
<p>2、证明布尔环：加法是交换群、乘法是半群、所有元素都是幂等元。</p>
<p>（1）<span class="math inline">\(\left ( R,+ \right )\)</span>加法运算是交换群</p>
<blockquote>
<ul>
<li>证明运算封闭</li>
</ul>
</blockquote>
<p><span class="math inline">\(\because \forall A,B \in \Gamma\)</span></p>
<p><span class="math inline">\(\therefore (A \setminus B) \in \Gamma,(B \setminus A) \in \Gamma,(A \setminus B) \cup (B \setminus A) \in \Gamma\)</span></p>
<p><span class="math inline">\(\therefore \left ( A \cap B^{\mathrm{C}} \right ) \cup \left ( B \cap A^{\mathrm{C}} \right )=\left ( A \cup B \right ) \cap \left ( A \cap B \right )^{\mathrm{C}} \in \Gamma\)</span></p>
<blockquote>
<ul>
<li>证明结合律</li>
</ul>
</blockquote>
<p><span class="math inline">\(\forall A,B,C \in \Gamma\)</span></p>
<p><span class="math inline">\(\because \left ( A+B \right )+C=(A \setminus B) \cup (B \setminus A)+C=\left ( A \cup B \cup C \right ) \cap \left ( A \cap B \cap C \right )^{\mathrm{C}}\)</span></p>
<p><span class="math inline">\(\because A+\left ( B+C \right )=A+(B \setminus C) \cup (C \setminus B)=\left ( A \cup B \cup C \right ) \cap \left ( A \cap B \cap C \right )^{\mathrm{C}}\)</span></p>
<p><span class="math inline">\(\therefore \left ( A+B \right )+C=A+\left ( B+C \right )\)</span></p>
<blockquote>
<ul>
<li>证明有单位元</li>
</ul>
</blockquote>
<p>令<span class="math inline">\(\varnothing=e\)</span>，且<span class="math inline">\(\varnothing \in \Gamma\)</span></p>
<p><span class="math inline">\(\because A+\varnothing=(A \setminus \varnothing) \cup (\varnothing \setminus A)=A\)</span></p>
<p>又<span class="math inline">\(\because \varnothing+A=(\varnothing \setminus A) \cup (A \setminus \varnothing)=A\)</span></p>
<p><span class="math inline">\(\therefore A+\varnothing=\varnothing+A\)</span></p>
<p><span class="math inline">\(\therefore \varnothing\)</span>为单位元</p>
<blockquote>
<ul>
<li>证明有逆元</li>
</ul>
</blockquote>
<p>令逆元为<span class="math inline">\(B\)</span>，<span class="math inline">\(\forall A \in \mathbb{X}\)</span>，证明<span class="math inline">\(A \circ B=B \circ A=e=\varnothing\)</span></p>
<p>则<span class="math inline">\(A \circ B=A+B=(A \setminus B) \cup (B \setminus A)=\varnothing\)</span></p>
<p><span class="math inline">\(\therefore B=A^{\mathrm{C}}\)</span></p>
<p>同理可证<span class="math inline">\(B \circ A=\varnothing\)</span></p>
<p><span class="math inline">\(\therefore A \circ B=B \circ A=\varnothing\)</span></p>
<p>同理可证<span class="math inline">\(A \ast B=A \ast A^{\mathrm{C}}=A \cap A^{\mathrm{C}}=\varnothing\)</span></p>
<p>（2）<span class="math inline">\(\left ( R,\ast \right )\)</span>乘法运算是半群</p>
<p><span class="math inline">\(\because \forall A,B \in \Gamma\)</span></p>
<p><span class="math inline">\(\therefore A \ast B=A \cap B \in \Gamma\)</span></p>
<p><span class="math inline">\(\because \forall A,B,C \in \Gamma\)</span></p>
<p><span class="math inline">\(\therefore \left ( A \ast B \right ) \ast C=\left ( A \cap B \right ) \cap C=A \cap B \cap C\)</span></p>
<p><span class="math inline">\(\therefore A \ast \left ( B \ast C \right )=A \cap \left ( B \cap C \right ) =A \cap B \cap C\)</span></p>
<p><span class="math inline">\(\therefore A \ast \left ( B \ast C \right )=A \ast \left ( B \ast C \right )\)</span></p>
<p><span class="math inline">\(\therefore\)</span>乘法运算是半群</p>
<p>（3）所有元素都是幂等元</p>
<p><span class="math inline">\(\because \forall A \in \Gamma\)</span></p>
<p><span class="math inline">\(\therefore A \ast A=A \cap A=A\)</span></p>
<p><span class="math inline">\(\therefore \Gamma\)</span>中所有元素都是幂等元</p>
<p>综上所述，<span class="math inline">\(R\)</span>为布尔环。</p>
<h2 id="第三题">第三题</h2>
<p>设<span class="math inline">\(V\)</span>是<span class="math inline">\(n\)</span>维线性空间，<span class="math inline">\(W\)</span>是<span class="math inline">\(V\)</span>的一个线性真子空间。</p>
<blockquote>
<ul>
<li>1、写出商空间<span class="math inline">\(V/W\)</span>的定义（只要写出它的元素形式，加法、数乘的定义）。</li>
<li>2、若<span class="math inline">\(\alpha_{1}, \alpha_{2}, \cdots, \alpha_{m}\)</span>是<span class="math inline">\(W\)</span>的一组基，把它延拓为<span class="math inline">\(V\)</span>的一组基<span class="math inline">\(\alpha_{1}, \alpha_{2}, \cdots, \alpha_{m}, \alpha_{m+1}, \cdots, \alpha_{n}(n&gt;m)\)</span>，试写出商空间<span class="math inline">\(V/W\)</span>一组基，并验证它的线性无关性。</li>
</ul>
</blockquote>
<p>1、商空间<span class="math inline">\(V/W\)</span>的定义</p>
<p><span class="math inline">\(V/W=\left \{ \alpha +W \mid \alpha \in V \right \}\)</span></p>
<p>令<span class="math inline">\(\alpha +W=\hat{\alpha}\)</span>，对<span class="math inline">\(\forall \alpha ,\beta \in V\)</span></p>
<p>有<span class="math inline">\(\hat{\alpha}+\hat{\beta}=\left ( \alpha+W \right )+\left ( \beta+W \right )=\left ( \alpha+\beta \right )+W\)</span></p>
<p><span class="math inline">\(k\hat{\alpha}=k\left ( \alpha +W \right )=k\alpha +W\)</span></p>
<p>2、获取商空间<span class="math inline">\(V/W\)</span>一组基</p>
<p>对<span class="math inline">\(\forall \beta +W \in V/W\)</span>，有<span class="math inline">\(\beta=x_{1} \alpha_{1}+x_{2} \alpha_{2}+\cdots+x_{n} \alpha_{n}\)</span>，则</p>
<p><span class="math display">\[\begin{equation}
    \begin{matrix}
        \beta+W=\left(x_{1} \alpha_{1}+x_{2} \alpha_{2}+\cdots+x_{n} \alpha_{n}\right)+W \\
        =\left(x_{1} \alpha_{1}+W\right)+\cdots+\left(x_{m} \alpha_{m}+W\right)+\left(x_{m+1} \alpha_{m+1}+W\right)+\cdots+\left(x_{n} \alpha_{n}+W\right) \\
        =W+\cdots+W+x_{m+1}\left(\alpha_{m+1}+W\right)+\cdots+x_{n}\left(\alpha_{n}+W\right) \\
        =x_{m+1}\left(\alpha_{m+1}+W\right)+\cdots+x_{n}\left(\alpha_{n}+W\right) \\
        =x_{m+1}\hat{\alpha_{m+1}}+\cdots+x_{n}\hat{\alpha_{n}}
    \end{matrix}
\end{equation}\]</span></p>
<p>因此<span class="math inline">\(V/W\)</span>中任意向量都可以用<span class="math inline">\(\hat{\alpha_{m+1}},\cdots ,\hat{\alpha_{n}}\)</span>线性表示。</p>
<p>3、验证这组基的线性无关性<span class="math inline">\(k_{m+1}\hat{\alpha_{m+1}}+\cdots+k_{n}\hat{\alpha_{n}}=\hat{0}\)</span></p>
<p>设<span class="math inline">\(k_{m+1}\left(\alpha_{m+1}+W\right)+\cdots+k_{n}\left(\alpha_{n}+W\right)=W\)</span></p>
<p>则<span class="math inline">\(\left ( k_{m+1}\alpha_{m+1}+\cdots+k_{n}\alpha_{n} \right )+W=W\)</span></p>
<p><span class="math inline">\(\therefore k_{m+1}\alpha_{m+1}+\cdots+k_{n}\alpha_{n} \in W\)</span></p>
<p><span class="math inline">\(\therefore k_{m+1}\alpha_{m+1}+\cdots+k_{n}\alpha_{n} = -k_{1}\alpha_{1}-\cdots-k_{m}\alpha_{m}\)</span></p>
<p><span class="math inline">\(\therefore k_{1}\alpha_{1}+\cdots+k_{m}\alpha_{m}+k_{m+1}\alpha_{m+1}+\cdots+k_{n}\alpha_{n}=0\)</span></p>
<p>因为<span class="math inline">\(\alpha_{1}, \cdots, \alpha_{n}\)</span>是一组线性无关的基</p>
<p>所以<span class="math inline">\(k_{1}=k_{2}=\cdots=k_{n}=0\)</span></p>
<p>因此<span class="math inline">\(\hat{\alpha_{m+1}},\cdots,\hat{\alpha_{n}}\)</span>是<span class="math inline">\(V/W\)</span>的一组线性无关基。</p>
<h2 id="第四题">第四题</h2>
<p>设<span class="math inline">\(\left ( X, \rho \right )\)</span>是距离空间，令</p>
<p><span class="math display">\[\begin{equation}
    d(x, y)=\frac{\rho(x, y)}{1+\rho(x, y)}, \forall x, y \in X
\end{equation}\]</span></p>
<blockquote>
<ul>
<li>1、求证：<span class="math inline">\(\left ( X, d \right )\)</span>也是距离空间。</li>
<li>2、若<span class="math inline">\(X\)</span>中的点列<span class="math inline">\(\left\{x_{n}\right\}_{n=1}^{\infty}\)</span>按距离<span class="math inline">\(\rho\)</span>收敛到<span class="math inline">\(x\)</span>，求证<span class="math inline">\(\left\{x_{n}\right\}_{n=1}^{\infty}\)</span>也按距离<span class="math inline">\(d\)</span>收敛到<span class="math inline">\(x\)</span></li>
<li>3、若<span class="math inline">\(A\)</span>是<span class="math inline">\(\left ( X, d \right )\)</span>中的闭集，求证：<span class="math inline">\(A\)</span>也是<span class="math inline">\(\left ( X, \rho \right )\)</span>中的闭集。</li>
</ul>
</blockquote>
<p>1、证明<span class="math inline">\(\left ( X, d \right )\)</span>也是距离空间：非负性、对称性、三角不等式</p>
<p>（1）非负性</p>
<p><span class="math inline">\(\because \rho\)</span>是距离空间</p>
<p><span class="math inline">\(\therefore \rho(x, y) \geq 0\)</span></p>
<p><span class="math inline">\(\therefore d(x, y)=\frac{\rho(x, y)}{1+\rho(x, y)} \geq 0\)</span></p>
<p>（2）对称性</p>
<p><span class="math inline">\(\because \rho\)</span>是距离空间</p>
<p><span class="math inline">\(\therefore \rho(x, y) = \rho(y, x)\)</span></p>
<p><span class="math inline">\(\therefore d(x, y)=\frac{\rho(x, y)}{1+\rho(x, y)} = \frac{\rho(y, x)}{1+\rho(y, x)} = d(y, x)\)</span></p>
<p>（3）三角不等式</p>
<p><span class="math inline">\(\because \rho\)</span>是距离空间</p>
<p><span class="math inline">\(\therefore \rho(x, z) \leq \rho(x, y) + \rho(y, z)\)</span></p>
<p><span class="math inline">\(\therefore d(x, z)=\frac{\rho(x, z)}{1+\rho(x, z)} \leq \frac{\rho(x, y)+\rho(y, z)}{1+\rho(x, y)+\rho(y, z)}\)</span></p>
<p><span class="math inline">\(\therefore \frac{\rho(x, y)+\rho(y, z)}{1+\rho(x, y)+\rho(y, z)}=\frac{\rho(x, y)}{1+\rho(x, y)+\rho(y, z)}+\frac{\rho(y, z)}{1+\rho(x, y)+\rho(y, z)}\)</span></p>
<p><span class="math inline">\(\therefore \frac{\rho(x, y)}{1+\rho(x, y)+\rho(y, z)}+\frac{\rho(y, z)}{1+\rho(x, y)+\rho(y, z)} \leq \frac{\rho(x, y)}{1+\rho(x, y)}+\frac{\rho(y, z)}{1+\rho(y, z)}=d(x,y)+d(y,z)\)</span></p>
<p><span class="math inline">\(\therefore d(x, z) \leq d(x, y)+d(y, z)\)</span></p>
<p>因此<span class="math inline">\(\left ( X, d \right )\)</span>是距离空间</p>
<p>2、证明<span class="math inline">\(d\)</span>收敛到<span class="math inline">\(x\)</span>：随着<span class="math inline">\(x_{n}\)</span>的增加，距离接近于0</p>
<p><span class="math inline">\(\because X\)</span>中的点列<span class="math inline">\(\left\{x_{n}\right\}_{n=1}^{\infty}\)</span>按距离<span class="math inline">\(\rho\)</span>收敛到<span class="math inline">\(x\)</span></p>
<p><span class="math inline">\(\therefore \forall \varepsilon &gt; 0,\exists N\)</span>时，当<span class="math inline">\(n&gt;N\)</span>是<span class="math inline">\(\rho \left ( x_{n},x \right ) &lt; \varepsilon\)</span></p>
<p><span class="math inline">\(\therefore \forall \varepsilon &gt; 0,\exists N\)</span>时，当<span class="math inline">\(n&gt;N\)</span>是<span class="math inline">\(d \left ( x_{n},x \right ) &lt; \frac{\varepsilon}{1+\varepsilon}\)</span></p>
<p><span class="math inline">\(\therefore \frac{\varepsilon}{1+\varepsilon}\)</span>任意小，并且<span class="math inline">\(\frac{\varepsilon}{1+\varepsilon}&gt;0\)</span></p>
<p><span class="math inline">\(\therefore \left\{x_{n}\right\}_{n=1}^{\infty}\)</span>也按距离<span class="math inline">\(d\)</span>收敛到<span class="math inline">\(x\)</span></p>
<p>3、<span class="math inline">\(A\)</span>也是<span class="math inline">\(\left ( X, \rho \right )\)</span>中的闭集</p>
<p><span class="math inline">\(\because A\)</span>是<span class="math inline">\(\left ( X, d \right )\)</span>中的闭集的充要条件是<span class="math inline">\(A\)</span>中任意一个收敛点列必收敛于<span class="math inline">\(A\)</span>中的一点。</p>
<p><span class="math inline">\(\therefore X\)</span>中点列<span class="math inline">\(\left\{x_{n}\right\}_{n=1}^{\infty}\)</span>按距离<span class="math inline">\(d\)</span>收敛到<span class="math inline">\(X\)</span>中的一个点<span class="math inline">\(x_{0}\)</span>。</p>
<p>有第2问结果知道点列<span class="math inline">\(\left\{x_{n}\right\}_{n=1}^{\infty}\)</span>在距离<span class="math inline">\(\rho\)</span>也收敛于<span class="math inline">\(x_{0}\)</span>，并且<span class="math inline">\(x_{0} \in X\)</span>。</p>
<p><span class="math inline">\(\therefore A\)</span>是<span class="math inline">\(\left ( X, \rho \right )\)</span>中的闭集。</p>
<h2 id="第五题">第五题</h2>
<p>设<span class="math inline">\(V\)</span>是数域<span class="math inline">\(P\)</span>上的赋范线性空间，证明：</p>
<blockquote>
<ul>
<li>1、<span class="math inline">\(V\)</span>中的收敛点列<span class="math inline">\(\left\{x_{n}\right\}\)</span>的极限是唯一的。</li>
<li>2、<span class="math inline">\(V\)</span>中收敛序列<span class="math inline">\(\left\{x_{n}\right\}\)</span>必有界，即存在正数<span class="math inline">\(M\)</span>，使得<span class="math inline">\(\left\|x_{n}\right\| \leq M(n=1,2, \cdots)\)</span>;</li>
<li>3、如果<span class="math inline">\(V\)</span>中序列<span class="math inline">\(\left\{x_{n}\right\}\)</span>收敛到<span class="math inline">\(x \in V\)</span>，则<span class="math inline">\(\left\{x_{n}\right\}\)</span>的任意一个子序列{<span class="math inline">\(\left\{x_{n_{k}}\right\}\)</span>}也收敛到<span class="math inline">\(x\)</span>；</li>
<li>4、如果<span class="math inline">\(V\)</span>中序列<span class="math inline">\(\left\{x_{n}\right\}\)</span>，<span class="math inline">\(\left\{y_{n}\right\}\)</span>分别收敛到<span class="math inline">\(x,y \in V\)</span>，则对任意<span class="math inline">\(a,b \in P\)</span>，有</li>
</ul>
</blockquote>
<p><span class="math display">\[\begin{equation}
    \underset{n \rightarrow \infty}{lim}\left(a x_{n}+b y_{n}\right)=ax+by
\end{equation}\]</span></p>
<p>1、证明<span class="math inline">\(\left\{x_{n}\right\}\)</span>极限的唯一性</p>
<p>设<span class="math inline">\(x,\tilde{x}\)</span>都是<span class="math inline">\(x_{n}\)</span>的极限，则得到<span class="math inline">\(0\leq \|x+\tilde{x} \| \leq \|x+x_{n}\|+\|x_{n}+\tilde{x}\|\)</span></p>
<p><span class="math inline">\(\therefore\)</span>当<span class="math inline">\(n \rightarrow \infty\)</span>时，<span class="math inline">\(\|x+x_{n} \| \rightarrow 0,\|x_{n}+\tilde{x}\| \rightarrow 0\)</span></p>
<p><span class="math inline">\(\therefore \|x+\tilde{x} \|=0\)</span></p>
<p><span class="math inline">\(\therefore x=\tilde{x}\)</span></p>
<p>2、证明<span class="math inline">\(\left\{x_{n}\right\}\)</span>必有界</p>
<p>由<span class="math inline">\(x_{n} \rightarrow x \left ( n \rightarrow \infty \right )\)</span>知，存在正数<span class="math inline">\(K\)</span>，使得当<span class="math inline">\(k &gt; K\)</span>时，<span class="math inline">\(d\left ( x_{k},x \right )&lt; 1\)</span></p>
<p><span class="math display">\[\begin{equation}
    r=max\left \{ d\left ( x_{1},x \right ),\cdots ,d\left ( x_{K-1},x \right ),1 \right \}
\end{equation}\]</span></p>
<p>则<span class="math inline">\(\left\{x_{n}\right\}\subseteq U\left ( x,r \right )\)</span></p>
<p>3、证明任意一个子序列{<span class="math inline">\(\left\{x_{n_{k}}\right\}\)</span>}也收敛到<span class="math inline">\(x\)</span></p>
<p><span class="math inline">\(\because x_{n} \rightarrow x\)</span></p>
<p><span class="math inline">\(\therefore \exists N\)</span>，使得<span class="math inline">\(n &gt; N\)</span>时有<span class="math inline">\(\left \| x_{n}-x \right \| &lt; \varepsilon\)</span></p>
<p><span class="math inline">\(\because x_{n_{k}}\)</span>是<span class="math inline">\(x_{n}\)</span>的子列</p>
<p><span class="math inline">\(\therefore \exists k\)</span>，使得<span class="math inline">\(k&gt;K\)</span>时，有<span class="math inline">\(n_{k}&gt;N\)</span></p>
<p>4、证明<span class="math inline">\(\underset{n \rightarrow \infty}{lim} \left(a x_{n}+b y_{n}\right)=ax+by\)</span></p>
<p><span class="math inline">\(\because \left \|\left(a x_{n}+b y_{n}\right)-\left ( ax+by\right ) \right \|=\left \| a\left ( x_{n}-x\right )+b\left ( y_{n}-y\right )\right \|\)</span></p>
<p><span class="math inline">\(\therefore \left \| a\left ( x_{n}-x\right )+b\left ( y_{n}-y\right )\right \| \leq \left \| a\left ( x_{n}-x\right )\right \|+\left \| b\left ( y_{n}-y\right )\right \|\)</span></p>
<p><span class="math inline">\(\because \left \| a\left ( x_{n}-x\right )\right \|+\left \| b\left ( y_{n}-y\right )\right \|=\left | a\right |\left \| x_{n}-x\right \|+\left | b\right |\left \| y_{n}-y\right \|\)</span></p>
<p><span class="math inline">\(\therefore \left \|\left(a x_{n}+b y_{n}\right)-\left ( ax+by\right ) \right \| \leq \left | a\right |\left \| x_{n}-x\right \|+\left | b\right |\left \| y_{n}-y\right \|\)</span></p>
<p>又<span class="math inline">\(\because x_{n}\)</span>收敛到<span class="math inline">\(x\)</span>，并且<span class="math inline">\(y_{n}\)</span>也收敛到<span class="math inline">\(y\)</span></p>
<p><span class="math inline">\(\therefore \underset{n \rightarrow \infty}{lim} \left(a x_{n}+b y_{n}\right)=ax+by\)</span></p>
<h2 id="第六题">第六题</h2>
<p>证<span class="math inline">\(U_{n}\)</span>是循环群，<span class="math inline">\(U_{n}=span\left ( e^{i\frac{2\pi k}{n}} \right )\left ( i=0,1,\cdots,n-1 \right )\)</span></p>
<p>先证群，再证循环群</p>
<p>设<span class="math inline">\(\varepsilon ^{k}=e^{\frac{i \cdot 2\pi \cdot k}{n}}\)</span></p>
<p>当<span class="math inline">\(k=n\)</span>时，<span class="math inline">\(\varepsilon ^{n}=e^{i \cdot 2\pi}=1,\varepsilon ^{0}=e^{0}=1\)</span></p>
<p><span class="math inline">\(\forall \varepsilon ^{a},\varepsilon ^{a},0\leq a,b\leq n-1\)</span></p>
<blockquote>
<ul>
<li>证明运算封闭</li>
</ul>
</blockquote>
<p><span class="math inline">\(\because \varepsilon ^{a}\cdot \varepsilon ^{b}=e^{i\frac{2\pi a}{n}}\cdot e^{i\frac{2\pi b}{n}}=e^{i\frac{2\pi \left ( a+b \right )}{n}}\)</span></p>
<p><span class="math inline">\(\therefore \varepsilon ^{a}\cdot \varepsilon ^{b} \in U_{n}\)</span></p>
<blockquote>
<ul>
<li>证明结合律</li>
</ul>
</blockquote>
<p><span class="math inline">\(\because \left ( \varepsilon ^{a}\cdot \varepsilon ^{b} \right )\cdot \varepsilon ^{c}=e^{i\frac{2\pi \left ( a+b \right )}{n}}\cdot e^{i\frac{2\pi c}{n}}=e^{i\frac{2\pi \left ( a+b+c \right )}{n}}\)</span></p>
<p><span class="math inline">\(\because \varepsilon ^{a} \cdot \left ( \varepsilon ^{b}\cdot \varepsilon ^{c} \right )=e^{i\frac{2\pi a}{n}} \cdot e^{i\frac{2\pi \left ( b+c \right )}{n}}=e^{i\frac{2\pi \left ( a+b+c \right )}{n}}\)</span></p>
<p><span class="math inline">\(\therefore \left ( \varepsilon ^{a}\cdot \varepsilon ^{b} \right )\cdot \varepsilon ^{c}=\varepsilon ^{a} \cdot \left ( \varepsilon ^{b}\cdot \varepsilon ^{c} \right )\)</span></p>
<blockquote>
<ul>
<li>证明有单位元</li>
</ul>
</blockquote>
<p><span class="math inline">\(\because \varepsilon ^{0}=e ^{0}=1\)</span></p>
<p>并且<span class="math inline">\(\because \varepsilon ^{k}\cdot \varepsilon ^{0}=\varepsilon ^{k}\)</span></p>
<p><span class="math inline">\(\therefore \varepsilon ^{0}\)</span>为单位元</p>
<blockquote>
<ul>
<li>证明有逆元</li>
</ul>
</blockquote>
<p><span class="math inline">\(\forall \varepsilon ^{k} \in U_{n}\)</span></p>
<p>则<span class="math inline">\(\exists \varepsilon ^{n-k}\)</span>使得</p>
<p><span class="math inline">\(\varepsilon ^{k} \cdot \varepsilon ^{n-k}=\varepsilon ^{n}=1=\varepsilon ^{0}\)</span></p>
<p><span class="math inline">\(\varepsilon ^{n-k} \cdot \varepsilon ^{k}=\varepsilon ^{0}\)</span></p>
<p>综上所述<span class="math inline">\(U_{n}\)</span>是群</p>
<p><span class="math inline">\(\because \varepsilon ^{k}=e^{\frac{i \cdot 2\pi \cdot k}{n}}\)</span>，即生成元是<span class="math inline">\(e^{\frac{i \cdot 2\pi}{n}}\)</span></p>
<p><span class="math inline">\(\therefore U_{n}=span\left ( e^{i\frac{2\pi k}{n}} \right )\)</span>是循环群</p>
]]></content>
      <categories>
        <category>课程</category>
      </categories>
  </entry>
  <entry>
    <title>最优化理论和应用</title>
    <url>/optimization-theory/</url>
    <content><![CDATA[<h1 id="向量空间与矩阵">向量空间与矩阵</h1><h2 id="向量与矩阵">向量与矩阵</h2><h3 id="线性相关">线性相关</h3><p>下式中，只有当所有系数<span class="math inline">\(\alpha_{i}(i=1,2,\cdots,k)\)</span>都等于零的前提下才成立，那么就称向量机<span class="math inline">\({\mathbf{a_{1}},\mathbf{a_{2}},\cdots,\mathbf{a_{k}}}\)</span>是线性无关的，否则只要有一个<span class="math inline">\(\mathbf{a_{k}}\)</span>不等于零，那么就是线性相关。</p><a id="more"></a>



<p><span class="math display">\[\begin{equation}
    \alpha_{1} \mathbf{a_{1}}+\alpha_{2} \mathbf{a_{2}}+\cdots+\alpha_{k} \mathbf{a_{k}}=\mathbf{0}
\end{equation}\]</span></p>
<p>对于所有包含<span class="math inline">\(\mathbf{0}\)</span>向量的集合都是线性相关。</p>
<p>给定了向量<span class="math inline">\(\mathbf{a}\)</span>，如果存在标量<span class="math inline">\(a_{1}, a_{2}, \cdots, a_{k}\)</span>，使得</p>
<p><span class="math display">\[\begin{equation}
    \mathbf{a}=\alpha_{1} \mathbf{a_{1}}+\alpha_{2} \mathbf{a_{2}}+\cdots+\alpha_{k} \mathbf{a_{k}}
\end{equation}\]</span></p>
<p>那么就称向量<span class="math inline">\(\mathbf{a}\)</span>是<span class="math inline">\({\mathbf{a_{1}},\mathbf{a_{2}},\cdots,\mathbf{a_{k}}}\)</span>的线性组合。</p>
<h3 id="生成子空间">生成子空间</h3>
<p>假定<span class="math inline">\(\mathbf{a_{1}},\mathbf{a_{2}},\cdots,\mathbf{a_{k}}\)</span>是<span class="math inline">\(\mathbb{R}^n\)</span>中的任意向量，那么它们所有线性组合的集合称为生成子空间，记为:</p>
<p><span class="math display">\[\begin{equation}
    span\left [ \mathbf{a_{1}},\mathbf{a_{2}},\cdots,\mathbf{a_{k}} \right ]=\left \{ \sum_{i=1}^{k}\alpha_{i}\mathbf{a_{i}}:\alpha_{1},\cdots,\alpha_{k},\alpha_{k}\in\mathbb{R} \right \}
\end{equation}\]</span></p>
<p>给定子空间<span class="math inline">\(\mathcal{V}\)</span>，如果存在<strong>线性无关</strong>的向量集合<span class="math inline">\(\left\{\mathbf{a_{1}}, \mathbf{a_{2}}, \cdots, \mathbf{a_{k}}\right\}\subset \mathcal{V}\)</span>，使得<span class="math inline">\(\mathcal{V}=span\left [\mathbf{a_{1}}, \mathbf{a_{2}}, \cdots, \mathbf{a_{k}} \right]\)</span>，那么就称<span class="math inline">\(\left \{ \mathbf{a_{1}}, \mathbf{a_{2}}, \cdots, \mathbf{a_{k}} \right \}\)</span>是子空间<span class="math inline">\(\mathcal{V}\)</span>的一组基，并且<span class="math inline">\(\mathcal{V}\)</span>中所有基都具有相同数量的向量，该数量称为<span class="math inline">\(\mathcal{V}\)</span>的维数，记为<span class="math inline">\(dim\mathcal{V}\)</span>。</p>
<p>如果<span class="math inline">\(\left |\mathbf{a_{1}}, \mathbf{a_{2}}, \cdots, \mathbf{a_{k}}\right |\)</span>是<span class="math inline">\(\mathcal{V}\)</span>的基，那么<span class="math inline">\(\mathcal{V}\)</span>中任意的向量<span class="math inline">\(\mathbf{a}\)</span>都可以通过下式进行表示，其中<span class="math inline">\(\mathbf{a_{i}} \in \mathbb{R},\ i=1,2,\cdots,k\)</span>。</p>
<p><span class="math display">\[\begin{equation}
    \mathbf{a}=\alpha_{1}\mathbf{a_{1}}+\alpha_{2}\mathbf{a_{2}}+\cdots+\alpha_{k}\mathbf{a_{k}}
\end{equation}\]</span></p>
<h2 id="矩阵的秩">矩阵的秩</h2>
<p><span class="math display">\[\begin{equation}
    \mathbf{A} = 
        \begin{bmatrix}
            a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n}\\ 
            a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n}\\ 
            \vdots &amp; \cdots &amp; \ddots &amp; \vdots\\ 
            a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn}
        \end{bmatrix}
\end{equation}\]</span></p>
<p><span class="math inline">\(\mathbf{A}\)</span>的第<span class="math inline">\(k\)</span>列用<span class="math inline">\(\mathbf{a_{k}}\)</span>表示</p>
<p><span class="math display">\[\begin{equation}
    \mathbf{a_{k}} = 
        \begin{bmatrix}
            a_{1k}\\ 
            a_{2k}\\ 
            \vdots\\ 
            a_{mk}
        \end{bmatrix}
\end{equation}\]</span></p>
<p>矩阵<span class="math inline">\(\mathbf{A}\)</span>中线性无关列的最大数目称为<span class="math inline">\(\mathbf{A}\)</span>的<strong>秩</strong>，记为<span class="math inline">\(rank\mathbf{A}\)</span>，并且<span class="math inline">\(rank\mathbf{A}\)</span>就是<span class="math inline">\(span\left[\mathbf{a_{1}}, \mathbf{a_{2}}, \cdots, \mathbf{a_{k}}\right]\)</span>的维数。</p>
<p>给定<span class="math inline">\(m\times n\)</span>矩阵<span class="math inline">\(\mathbf{A}\)</span>，那么<span class="math inline">\(p\)</span><strong>阶子式</strong>是一个<span class="math inline">\(p\times p\)</span>矩阵的行列式，该<span class="math inline">\(p\times p\)</span>矩阵由矩阵<span class="math inline">\(\mathbf{A}\)</span>减去<span class="math inline">\(m-p\)</span>行和<span class="math inline">\(n-p\)</span>列得到，并且<span class="math inline">\(p\leq min\left \{ m,n \right \}\)</span>。同时，如果<span class="math inline">\(m\times n\left ( m \geq n \right )\)</span>矩阵<span class="math inline">\(\mathbf{A}\)</span>具有<span class="math inline">\(n\)</span>阶子式，那么<span class="math inline">\(rank\mathbf{A}=n\)</span>。</p>
<p>假定<span class="math inline">\(\mathbf{A}\)</span>是<span class="math inline">\(n \times n\)</span>的方阵，如果存在<span class="math inline">\(n \times n\)</span>方阵<span class="math inline">\(\mathbf{B}\)</span>，使得<span class="math inline">\(\mathbf{A}\mathbf{B}=\mathbf{B}\mathbf{A}=\mathbf{I}_{n}\)</span>，其中<span class="math inline">\(\mathbf{I}_{n}\)</span>为<span class="math inline">\(n \times n\)</span>的单位矩阵，<span class="math inline">\(\mathbf{B}\)</span>为<span class="math inline">\(\mathbf{A}\)</span>的逆矩阵，记为<span class="math inline">\(\mathbf{B}=\mathbf{A}^{-1}\)</span></p>
<h2 id="线性方程">线性方程</h2>
<p>假设方程写成矩阵形式</p>
<p><span class="math display">\[\begin{equation}
    \mathbf{A}\mathbf{x}=\mathbf{b}
\end{equation}\]</span></p>
<p>其中<span class="math inline">\(\mathbf{A}\)</span>为系数矩阵，<span class="math inline">\(\left [ \mathbf{A},\mathbf{b} \right ]\)</span>为增广矩阵，那么方程<span class="math inline">\(\mathbf{A}\mathbf{x}=\mathbf{b}\)</span>要有解，那么<span class="math inline">\(rank\mathbf{A}=rank\left [ \mathbf{A},\mathbf{b} \right ]\)</span>。</p>
<p>如果<span class="math inline">\(\mathbf{A} \in \mathbb{R}^{m \times n}\)</span>，且<span class="math inline">\(rank\mathbf{A}=m\)</span>，那么只要<span class="math inline">\(n-m\)</span>个未知数就可以求解<span class="math inline">\(\mathbf{A}\mathbf{x}=\mathbf{b}\)</span>。</p>
<h2 id="内积和范数">内积和范数</h2>
<p>对于<span class="math inline">\(\mathbf{x},\mathbf{y} \in \mathbb{R}^{n}\)</span>，其欧式内积为：</p>
<p><span class="math display">\[\begin{equation}
    \left \langle \mathbf{x},\mathbf{y} \right \rangle=\sum_{i=1}^{n}x_{i}y_{i}=\mathbf{x}^{\top}\mathbf{y}
\end{equation}\]</span></p>
<p>如果向量<span class="math inline">\(\mathbf{x}\)</span>和<span class="math inline">\(\mathbf{y}\)</span>，使得<span class="math inline">\(\left \langle \mathbf{x},\mathbf{y} \right \rangle=0\)</span>，那么<span class="math inline">\(\mathbf{x}\)</span>和<span class="math inline">\(\mathbf{y}\)</span><strong>正交</strong>。</p>
<p>向量<span class="math inline">\(\mathbf{x}\)</span>的欧式范数为<span class="math inline">\(\left \| \mathbf{x} \right \|=\sqrt{\left \langle \mathbf{x},\mathbf{x} \right \rangle}=\sqrt{\mathbf{x}^{\top}\mathbf{x}}\)</span>。</p>
<p><strong>柯西-施瓦茨不等式：</strong><span class="math inline">\(\left | \left \langle \mathbf{x},\mathbf{y} \right \rangle \right |=\left \| \mathbf{x} \right \|\left \| \mathbf{y} \right \|\)</span></p>
<p><span class="math inline">\(\mathbb{R}^{n}\)</span><strong>中的毕达哥拉斯定理：</strong>如果<span class="math inline">\(\mathbf{x}\)</span>和<span class="math inline">\(\mathbf{y}\)</span>正交，则<span class="math inline">\(\left \langle \mathbf{x},\mathbf{y} \right \rangle=0\)</span>，就可以得到<span class="math inline">\(\left \| \mathbf{x}+\mathbf{y} \right \|^{2}=\left \| \mathbf{x} \right \|^{2} + 2\left \langle \mathbf{x},\mathbf{y} \right \rangle + \left \| \mathbf{y} \right \|^{2}=\left \| \mathbf{x} \right \|^{2}+\left \| \mathbf{y} \right \|^{2}\)</span>。</p>
<h1 id="变换">变换</h1>
<h2 id="线性变换">线性变换</h2>
<p><strong>相似矩阵：</strong>给定两个<span class="math inline">\(n \times n\)</span>的矩阵<span class="math inline">\(\mathbf{A}\)</span>和<span class="math inline">\(\mathbf{B}\)</span>，如果存在一个非奇异矩阵（可逆）<span class="math inline">\(\mathbf{T}\)</span>，使得<span class="math inline">\(\mathbf{A}=\mathbf{T}^{-1}\mathbf{B}\mathbf{T}\)</span>，那么称阵<span class="math inline">\(\mathbf{A}\)</span>和<span class="math inline">\(\mathbf{B}\)</span>相似，在不同的基下，相似矩阵对应的线性变换相同。</p>
<h2 id="特征值与特征向量">特征值与特征向量</h2>
<p>特征值：令<span class="math inline">\(\mathbf{A}\)</span>为<span class="math inline">\(n \times n\)</span>的方阵，如果存在<span class="math inline">\(\lambda\)</span>和非零向量<span class="math inline">\(\mathbf{v}\)</span>满足等式<span class="math inline">\(\mathbf{A}\mathbf{v}=\lambda\mathbf{v}\)</span>，那么称<span class="math inline">\(\lambda\)</span>为特征值，<span class="math inline">\(\mathbf{v}\)</span>为特征向量。<span class="math inline">\(\lambda\)</span>为<span class="math inline">\(\mathbf{A}\)</span>的充要条件为矩阵<span class="math inline">\(\lambda\mathbf{I}-\mathbf{A}\)</span>是<strong>奇异矩阵，</strong>即多项式<span class="math inline">\(\textbf{det}\left [ \lambda\mathbf{I}-\mathbf{A} \right ]=0\)</span>，其中<span class="math inline">\(\mathbf{I}\)</span>是<span class="math inline">\(n \times n\)</span>的单位矩阵，多项式为特征多项式，下面的方程为特征方程。</p>
<p><span class="math display">\[\begin{equation}
    \textbf{det}[\lambda \mathbf{I}-\mathbf{A}]=\lambda^{n}+a_{n-1} \lambda^{n-1}+\cdots+a_{1} \lambda+a_{0}=0
\end{equation}\]</span></p>
<p>1、假定方程<span class="math inline">\(\textbf{det}\left [ \lambda\mathbf{I}-\mathbf{A} \right ]=0\)</span>存在<span class="math inline">\(n\)</span>个相异的根<span class="math inline">\(\lambda_{1},\lambda_{2},\cdots,\lambda_{n}\)</span>，那么就存在<span class="math inline">\(n\)</span>个线性无关的向量<span class="math inline">\(\mathbf{\mathcal{v}_{1}},\mathbf{\mathcal{v}_{2}},\cdots,\mathbf{\mathcal{v}_{n}}\)</span>。当矩阵<span class="math inline">\(\mathbf{A}=\mathbf{A}^{\top}\)</span>，则称<span class="math inline">\(\mathbf{A}\)</span>为对称矩阵。</p>
<p>2、对于<span class="math inline">\(n \times n\)</span>实数对称矩阵，其<span class="math inline">\(n\)</span>个特征向量是相互正交的。</p>
<h2 id="正投影">正投影</h2>
<h2 id="矩阵范数">矩阵范数</h2>
<p>矩阵<span class="math inline">\(\mathbf{A}\)</span>的范数记为<span class="math inline">\(\left \| \mathbf{A} \right \|\)</span>，是满足以下条件的任意函数<span class="math inline">\(\left \| \cdot \right \|\)</span></p>
<p>1、如果<span class="math inline">\(\mathbf{A} \neq \mathbf{0}\)</span>，那么就有<span class="math inline">\(\left \| \mathbf{A} \right \| &gt; 0\)</span>，<span class="math inline">\(\left \| \mathbf{0} \right \| = 0\)</span>。</p>
<p>2、对于任意<span class="math inline">\(c \in \mathbb{R}\)</span>，就有<span class="math inline">\(\left \| c\mathbf{A} \right \|=\left | c \right |\left \| \mathbf{A} \right \|\)</span>。</p>
<p>3、<span class="math inline">\(\left \| \mathbf{A}+\mathbf{B} \right \| \leq \left \| \mathbf{A} \right \|+\left \| \mathbf{B} \right \|\)</span></p>
<p>4、<span class="math inline">\(\left \| \mathbf{A} \mathbf{B} \right \| \leq \left \| \mathbf{A} \right \|\left \| \mathbf{B} \right \|\)</span></p>
<p>令</p>
<p><span class="math display">\[\begin{equation}
    \left \| \mathbf{x} \right \|=\sqrt{\left(\sum_{k=1}^{n}\left|x_{k}\right|^{2}\right)}=\sqrt{\langle\mathbf{x}, \mathbf{x}\rangle}
\end{equation}\]</span></p>
<p>则由该向量函数导出矩阵范数为</p>
<p><span class="math display">\[\begin{equation}
    \left \| \mathbf{A} \right \|=\sqrt{\lambda_{1}}
\end{equation}\]</span></p>
<p>其中，<span class="math inline">\(\lambda_{1}\)</span>是矩阵<span class="math inline">\(\mathbf{A}^{\top}\mathbf{A}\)</span>的最大特征矩阵。</p>
<p><strong>瑞利不等式：</strong>如果<span class="math inline">\(n \times n\)</span>矩阵<span class="math inline">\(\mathbf{P}\)</span>是一个实数对称正定矩阵，则有</p>
<p><span class="math display">\[\begin{equation}
    \lambda_{\min }(\mathbf{P})\|\mathbf{x}\|^{2} \leq \mathbf{x}^{\top} \mathbf{P} \mathbf{x} \leq \lambda_{\max }(\mathbf{P})\|\mathbf{x}\|^{2}
\end{equation}\]</span></p>
<p>其中，<span class="math inline">\(\lambda_{\min }(\mathbf{P})\)</span>表示<span class="math inline">\(\mathbf{P}\)</span>的最小特征值，<span class="math inline">\(\lambda_{\max }(\mathbf{P})\)</span>表示<span class="math inline">\(\mathbf{P}\)</span>的最大特征值。</p>
<h1 id="有关几何概念">有关几何概念</h1>
<h2 id="线段">线段</h2>
<p><span class="math inline">\(\mathbf{x}\)</span>和<span class="math inline">\(\mathbf{y}\)</span>是空间<span class="math inline">\(\mathbb{R}^{n}\)</span>中的两个点，<span class="math inline">\(\mathbf{z}\)</span>是两点之间连线上的点，那么就可以得到</p>
<p><span class="math display">\[\begin{equation}
    \mathbf{z}-\mathbf{y}=\alpha \left ( \mathbf{x}-\mathbf{y} \right )
\end{equation}\]</span></p>
<p>其中<span class="math inline">\(\alpha\)</span>是区间<span class="math inline">\(\left [ 0,1 \right ]\)</span>之间的实数，因此上式可写为</p>
<p><span class="math display">\[\begin{equation}
    \mathbf{z}=\alpha \mathbf{x}+\left ( 1-\alpha  \right )\mathbf{y}
\end{equation}\]</span></p>
<p>并且表示为</p>
<p><span class="math display">\[\begin{equation}
    \left \{ \alpha \mathbf{x}+\left ( 1-\alpha  \right )\mathbf{y}:\alpha \in \left [ 0,1 \right ] \right \}
\end{equation}\]</span></p>
<h2 id="超平面和线性簇">超平面和线性簇</h2>
<p>令<span class="math inline">\(u_{1}, u_{2}, \dots, u_{n}, v \in \mathbb{R}\)</span>，其中至少存在一个<span class="math inline">\(u_{i}\)</span>不为零，由所有满足线性方程</p>
<p><span class="math display">\[\begin{equation}
    u_{1} x_{1}+u_{2} x_{2}+\cdots+u_{n} x_{n}=v
\end{equation}\]</span></p>
<p>的点<span class="math inline">\(\mathbf{x}=\left[x_{1}, x_{2}, \cdots, x_{n}\right]^{\top}\)</span>组成的集合成为空间<span class="math inline">\(\mathbb{R}^{n}\)</span>的超平面，可写为</p>
<p><span class="math display">\[\begin{equation}
    \left\{\mathbf{x} \in \mathbb{R}^{n}: \mathbf{u}^{\top} \mathbf{x}=v\right\}
\end{equation}\]</span></p>
<p>当<span class="math inline">\(n\)</span>为2时，即二维空间时，超平面为一条直线方程<span class="math inline">\(u_{1} x_{1}+u_{2} x_{2}=v\)</span>，当<span class="math inline">\(n\)</span>为3时，即三维空间时，超平面为一个面。</p>
<h2 id="凸集">凸集</h2>
<p>已知两点<span class="math inline">\(\mathbf{u},\mathbf{v} \in \mathbb{R}^{n}\)</span>之间的线段可表示为集合</p>
<p><span class="math display">\[\begin{equation}
    \left \{ \mathbf{w}=\mathbb{R}^{n}: \mathbf{w}=\alpha \mathbf{u}+(1-\alpha) \mathbf{v}, \alpha \in[0,1] \right \}
\end{equation}\]</span></p>
<p>如果点<span class="math inline">\(\mathbf{w}=\alpha \mathbf{u}+(1-\alpha) \mathbf{v}\left ( \alpha \in \left [ 0,1 \right ] \right )\)</span>称为点<span class="math inline">\(\mathbf{u}\)</span>和点<span class="math inline">\(\mathbf{v}\)</span>的凸组合。如果<span class="math inline">\(\mathbf{u},\mathbf{v} \in \Theta\)</span>，并且<span class="math inline">\(\mathbf{u}\)</span>和<span class="math inline">\(\mathbf{v}\)</span>之间的线段都在<span class="math inline">\(\Theta\)</span>内，那么<span class="math inline">\(\Theta \in \mathbb{R}^{n}\)</span>为凸集。</p>
<p>凸集的性质：</p>
<p>1、如果<span class="math inline">\(\Theta\)</span>是一个凸集，且<span class="math inline">\(\beta\)</span>是一个实数，那么<span class="math inline">\(\beta \Theta=\{\mathbf{x}: \mathbf{x}=\beta \mathbf{v}, \mathbf{v} \in \Theta\}\)</span>也是凸集。</p>
<p>2、如果<span class="math inline">\(\Theta_{1}\)</span>和<span class="math inline">\(\Theta_{2}\)</span>都是凸集，那么集合<span class="math inline">\(\Theta_{1}+\Theta_{2}=\left\{\mathbf{x}: \mathbf{x}=\mathbf{v}_{1}+\mathbf{v}_{2}, \mathbf{v}_{1} \in \Theta_{1}, \mathbf{v}_{2} \in \Theta_{2}\right\}\)</span>也是凸集。</p>
<p>3、任意多个凸集的交集都是凸集。</p>
]]></content>
      <categories>
        <category>课程</category>
      </categories>
  </entry>
  <entry>
    <title>决策树</title>
    <url>/dt/</url>
    <content><![CDATA[<img src="/dt/dt.png" class title="决策树"><h1 id="原理">原理</h1><p>决策树是一种<strong>非参数监督</strong>学习方法，用于分类和回归任务。对于离散值的构建的树模型，一般为分类树，而用连续值构建的树模型，一般为回归树。</p><blockquote>
<ul>
<li>决策树的本质：从训练集中归纳出一组分类规则，或者说是由训练数据集估计条件概率模型。</li>
<li>决策树学习的损失函数：正则化的极大似然函数。</li>
<li>决策树学习的测试：最小化损失函数。</li>
</ul>
</blockquote><a id="more"></a>



<p>决策树的构造通常分为两个阶段，分别是<code>构造和剪枝</code>。</p>
<p>1、构造：<code>生成一颗完整的树</code>，即在构造的过程总选择什么属性作为节点，树中的节点有三个，分别是：</p>
<p>（1）根节点：树的顶端，即最开始的节点，如上图中的“天气”。</p>
<p>（2）内部节点：树内部的分叉节点，如上图中的“温度”、“湿度”等</p>
<p>（3）叶节点：树最底部的节点，即决策树最终的结果，并且叶节点没有子节点。</p>
<p>在进行树构造时，需要解决三个主要问题：</p>
<p>（1）选择哪个属性作为根节点？</p>
<p>（2）选择哪个属性作为子节点？</p>
<p>（3）停止并获得目标状态的条件？即产生一个叶节点</p>
<p>2、剪枝：<code>为防止过拟合</code>，就需要给决策树瘦身（并不需要精确的判断所有属性）。</p>
<img src="/dt/overfitting.png" class title="过拟合">
<p>上图中的1表示欠拟合，2表示具有较好泛化性，3表示过拟合。</p>
<p><strong>泛化能力：</strong>指分类器是通过训练集获得的分类能力。如果对训练集分类的过于精准，即过度依赖于训练数据，那么得到的决策树其容错率就会比较低，泛化能力差。因为训练集只是全部数据的抽样，并不能体现全部数据的特点。</p>
<p><strong>预剪枝：</strong>在决策树构造时就进行剪枝。即在构造的过程中对节点进行评估，如果对某个节点进行划分，在验证集中不能带来准确性的提升，那么对这个节点进行划分就没有意义，这时就会把当前节点作为<code>叶节点</code>，不对其进行划分。预剪枝降低了过拟合的风险，显著减少了决策树的训练时间开销和测试时间开销，但可能带来欠拟合的风险 。</p>
<p><strong>后剪枝：</strong>在生成决策树之后再进行剪枝。通常会从决策树的叶节点开始，逐层向上对每个节点进行评估。如果剪掉这个节点子树，与保留该节点子树在分类准确性上差别不大，或者剪掉该节点子树，能在验证集中带来准确性的提升，那么就可以把该节点子树进行剪枝。<code>方法是：用该节点子树中使用最频繁的哪个节点来代替该节点。</code>后剪枝的欠拟合风险很小，泛化性能往往优于预剪枝决策树，但其训练时间开销比未剪枝和预剪枝都要大得多。</p>
<h2 id="特点">特点</h2>
<blockquote>
<ul>
<li>优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。</li>
<li>缺点：可能会产生过度匹配的问题。</li>
<li>适用数据类型：数值型和标称型（结果只在有限目标集中取值，如列表值）。</li>
</ul>
</blockquote>
<p>创建分支的伪代码createBranch()如下所示：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">If so <span class="keyword">return</span> 类标签：</span><br><span class="line">Else</span><br><span class="line">    寻找划分数据集的最好特征</span><br><span class="line">    划分数据集</span><br><span class="line">    创建分支节点</span><br><span class="line">        <span class="keyword">for</span> 每个划分的子集</span><br><span class="line">            调用函数createBranch()并增加返回结果到分支节点中</span><br><span class="line">        <span class="keyword">return</span> 分支节点</span><br></pre></td></tr></table></figure>
<h2 id="示例">示例</h2>
<img src="/dt/example1.png" class title="示例">
<p>该示例是用于构造是否去打篮球的决策树，其中的属性有天气、温度、湿度、刮风。</p>
<h1 id="纯度与信息熵">纯度与信息熵</h1>
<h2 id="纯度">纯度</h2>
<p>决策树的构造过程可以理解为寻找纯净划分的过程，纯度则是让目标变量的分歧最小。</p>
<p>例如下面的三个集合：</p>
<blockquote>
<ul>
<li>集合 1：6 次都去打篮球；</li>
<li>集合 2：4 次去打篮球，2 次不去打篮球；</li>
<li>集合 3：3 次去打篮球，3 次不去打篮球。</li>
</ul>
</blockquote>
<p>按照纯度来分，集合 1 &gt; 集合 2 &gt; 集合 3。因为集合 1的分歧最小，集合 3的分歧最大。</p>
<h2 id="信息熵">信息熵</h2>
<p>设<span class="math inline">\(X\)</span>是一个取有限个值的离散随机变量，其概率分布为：</p>
<p>\begin{matrix} P(X=x_{i})=p_{i} &amp; i=1,2,,n \end{matrix}</p>
<p><strong>信息熵：</strong>表示信息的不确定度。由于随机离散事件的出现概率存在着不确定性，因此为了衡量这种信息的不确定性，就是用信息熵来表示。同时，<strong>信息熵也用于在构建树的每个步骤决定要拆分的特征</strong>。</p>
<p>1、经验熵：熵中的概率由数据估计(特别是最大似然估计)得到。</p>
<p><span class="math display">\[\begin{equation}
    Entropy(D)=-\sum_{i=1}^{n}p_{i}\log_{2}p_{i}=-\sum_{i=1}^{n}\frac{\left | c_{k} \right |}{\left | D \right |}\log_{2}\frac{\left | c_{k} \right |}{\left | D \right |}
\end{equation}\]</span></p>
<p>其中<span class="math inline">\(\frac{\left | c_{k} \right |}{\left | D \right |}\)</span>为<span class="math inline">\(\left | c_{k} \right |\)</span>样本数量和总样本数量<span class="math inline">\(\left | D \right |\)</span>的比，当<span class="math inline">\(\log\)</span>底为2时，成为比特熵，以<span class="math inline">\(e\)</span>为底时称为纳特熵。</p>
<p>2、条件熵：已知在随机事件<span class="math inline">\(X\)</span>的条件下随机事件<span class="math inline">\(Y\)</span>的不确定性。</p>
<p><span class="math display">\[\begin{equation}
    Entropy(Y | X)=-\sum_{i=1}^{n}p_{i}H\left ( Y | X = x_{i} \right )
\end{equation}\]</span></p>
<p>其中<span class="math inline">\(p_{i} = P\left (X = x_{i} \right )\)</span>表示条件<span class="math inline">\(x_{i}\)</span>出现的概率，<span class="math inline">\(H\left ( Y | X = x_{i} \right )\)</span>表示事件<span class="math inline">\(x_{i}\)</span>发生时，发生事件<span class="math inline">\(Y\)</span>的概率。</p>
<p><strong>当不确定性越大时，它所包含的信息量也就越大，信息熵也就越高。</strong>以示例中的集合为例：</p>
<blockquote>
<ul>
<li>集合1：<span class="math inline">\(Entropy(t)=-\frac{1}{6}\log_{2}(\frac{6}{6})=0\)</span></li>
<li>集合2：<span class="math inline">\(Entropy(t)=-\frac{4}{6}\log_{2}(\frac{4}{6})-\frac{2}{6}\log_{2}(\frac{2}{6})=0.9\)</span></li>
<li>集合3：<span class="math inline">\(Entropy(t)=-\frac{3}{6}\log_{2}(\frac{3}{6})-\frac{3}{6}\log_{2}(\frac{3}{6})=1\)</span></li>
</ul>
</blockquote>
<p>从结果可以看出，<strong>信息熵越大，纯度越低。当集合中的所有样本均匀混合时，信息熵最大，纯度最低。</strong></p>
<p>因此，在构建决策树时，会基于纯度来进行构建，典型的“不纯度”指标有三种，分别是<strong>信息增益（ID3 算法）、信息增益率（C4.5 算法）以及基尼指数（Cart 算法）。</strong></p>
<h1 id="信息增益id3-算法">信息增益（ID3 算法）</h1>
<h2 id="原理-1">原理</h2>
<p>父亲节点的信息熵减去所有子节点的信息熵。在计算的过程中会计算每个子节点的归一化信息熵，<strong>即按照每个子节点在父节点中出现的概率</strong>来计算这些子节点的信息熵。</p>
<p><span class="math display">\[\begin{equation}
    Gain(D, A)=H(D)-H(D|A)=Entropy(D)-\sum_{i=1}^{k} \frac{\left|D_{i}\right|}{|D|} Entropy\left(D_{i}\right)
\end{equation}\]</span></p>
<p>其中<span class="math inline">\(D\)</span>表示父节点，<span class="math inline">\(D_{i}\)</span>是子节点，<span class="math inline">\(A\)</span>是在父节点<span class="math inline">\(D\)</span>中选择的属性，<span class="math inline">\(\frac{\left|D_{i}\right|}{|D|}\)</span>表示以父节点的分叉属性<span class="math inline">\(A\)</span>为例，子节点在父节点出现的概率。<strong>该式可以理解为由于特征<span class="math inline">\(A\)</span>而使得对数据集<span class="math inline">\(D\)</span>的分类的不确定减少的程度。</strong></p>
<img src="/dt/infogain.png" class title="信息增益">
<p><span class="math display">\[\begin{equation}
    Gain(D, A)=Entropy(D)-\left(\frac{3}{10} Entropy\left(D_{1}\right)+\frac{7}{10} Entropy\left(D_{2}\right)\right)
\end{equation}\]</span></p>
<h2 id="实现步骤">实现步骤</h2>
<blockquote>
<ul>
<li>Step1：根据要分类目标计算其信息熵。</li>
<li>Step2：根据分类目标，为每个属性计算信归一化信息熵。</li>
<li>Step3：根据分类目标和式子（2）计算每个属性的信息增益，并把<strong>信息增益最大的作为根节点</strong>。</li>
<li>Step4：根据根节点的属性值进行分类，计算每一类中信息增益最大的属性，并把该属性作为该节点的分割属性，如下图。</li>
<li>Step5：循环Step4从而得到整棵树。</li>
</ul>
</blockquote>
<img src="/dt/dt-split.png" class title="节点分裂">
<h2 id="缺陷">缺陷</h2>
<p>1、ID3没有剪枝策略，容易过拟合。</p>
<p>2、ID3算法中倾向于选择出现概率比较多的属性。例如，“编号”属性容易将会被选为最优属性。</p>
<p>3、只能用于处理离散分布的特征。</p>
<p>4、没有考虑缺失值。</p>
<p>所以，ID3的缺陷为<strong>当有些属性可能对分类任务没有太大作用，但是他们仍然可能会被选为最优属性</strong>，即对噪声敏感，并且当训练数据如果有少量错误，可能会产生决策树分类错误。</p>
<h1 id="信息增益率c4.5算法">信息增益率（C4.5算法）</h1>
<h2 id="改进">改进</h2>
<p>1、引入悲观剪枝策略进行后剪枝。</p>
<p>2、引入信息增益率作为划分标准。</p>
<p>3、离散化处理连续值。</p>
<p>4、可以处理缺省数据。</p>
<h2 id="原理-2">原理</h2>
<p>因为 ID3 在计算的时候，倾向于选择取值多的属性。为了避免这个问题，C4.5 采用信息增益率的方式来选择属性。</p>
<p><span class="math display">\[\begin{equation}
    \text { 信息增益率 }=\frac{\text { 信息增益 }}{\text { 父节点熵 }}=\frac{H(D)-H(D|A)}{\text { H(D) }}
\end{equation}\]</span></p>
<p>当属性有很多值的时候，相当于被划分成了许多份，<strong>虽然信息增益变大了，但是对于C4.5来说，属性熵也会变大，</strong>所以整体的信息增益率并不大。</p>
<h2 id="剪枝">剪枝</h2>
<p>ID3构造决策树时，容易产生过拟合。而在C4.5中，会在决策树构造之后采用悲观剪枝（PEP），这样可以提升决策树的泛化能力。</p>
<p>悲观剪枝是后剪枝技术中的一种，通过递归估算每个内部节点的损失函数，比较剪枝前后这个节点的损失函数来决定是否对其进行剪枝。<strong>这种剪枝方法的优势是不再需要一个单独的测试数据集。</strong></p>
<p>决策树的损失函数为：</p>
<p><span class="math display">\[\begin{equation}
    C_{\alpha}(T)=\sum_{t=1}^{|T|} N_{t} H_{t}(T)+\alpha|T|=\sum_{t=1}^{|T|} \sum_{k=1}^{K} N_{tk} \log\frac{N_{tk}}{N_{t}}+\alpha|T|=C(T)+\alpha|T|
\end{equation}\]</span></p>
<p>其中<span class="math inline">\(T\)</span>表示子树的叶节点；<span class="math inline">\(H_{t}(T)\)</span>表示第<span class="math inline">\(t\)</span>个叶子的熵；<span class="math inline">\(N_{t}\)</span>表示该叶子所含的训练样例的个数；<span class="math inline">\(\alpha\)</span>表示惩罚系数，并控制模型和训练数据之间拟合程度，当<span class="math inline">\(\alpha\)</span>较大时，树模型较为简单，反之则树模型较为复杂，为0时只考虑模型与训练数据的拟合程度，不考虑模型的复杂度；C(T)表示模型对训练数据的预测误差，即模型与训练数据的拟合程度。</p>
<h3 id="实现步骤-1">实现步骤</h3>
<blockquote>
<ul>
<li>Step1：计算每个结点的经验熵。</li>
<li>Step2：递归的从树的叶结点向上回缩。</li>
</ul>
</blockquote>
<p>设一组叶结点回缩到其父结点之前与之后的整体树分别为<span class="math inline">\(T_{B}\)</span>和<span class="math inline">\(T_{A}\)</span>，其对应的损失函数值分别是<span class="math inline">\(C_{\alpha}(T_{B})\)</span>和<span class="math inline">\(C_{\alpha}(T_{A})\)</span>，如果<span class="math inline">\(C_{\alpha}(T_{A}) \leq C_{\alpha}(T_{B})\)</span>，则进行剪枝，即将父结点变为新的叶结点。</p>
<blockquote>
<ul>
<li>Step3：返回Step3，直到不能继续为止，从而得到损失函数最小的子树<span class="math inline">\(T\)</span>。</li>
</ul>
</blockquote>
<h2 id="离散化处理连续值">离散化处理连续值</h2>
<p>对于处理连续值时，C4.5选择具有最高信息增益属性的属性值作为阈值。</p>
<h2 id="处理缺省数据">处理缺省数据</h2>
<p>当某个属性由于数据缺失，使得只有<span class="math inline">\(B\)</span>条样本，而总样本数为<span class="math inline">\(A\)</span>，那么最后算出的信息增益率为：</p>
<p><span class="math display">\[\begin{equation}
    \text { 某属性的信息增益率 }=\frac{B}{A}*\text { 某属性的实际信息增益率 }
\end{equation}\]</span></p>
<h2 id="缺陷-1">缺陷</h2>
<p>1、C4.5只能用于分类。</p>
<p>2、熵模型中的对数运算、连续值、排序运算都消耗大量运算。</p>
<p>C4.5在ID3的基础上，用信息增益率代替了信息增益，解决了噪声敏感的问题，并且可以对所构造树进行剪枝，同时还能处理连续数值以及数值缺失等情况。<strong>但由于C4.5需要对数据集进行多次扫描，算法效率相对较低。</strong></p>
<h1 id="基尼指数cart算法">基尼指数（CART算法）</h1>
<h2 id="原理-3">原理</h2>
<p>CART的全称是分类与回归树，即该算法既可以用于分类问题，也可以用于回归问题，并且使用CART生成的树只能是二叉树。</p>
<p>1、回归树：使用<strong>平方误差最小化准则</strong>来选择特征并进行划分。每一个叶节点给出的预测值是划分到该叶子节点的所有样本目标值的均值，这样只是在给定划分的情况下最小化了平方误差。</p>
<p>2、分类树：使用<strong>基尼指数（GINI）最小化准则</strong>来选择特征并进行划分。基尼指数表示集合的不确定性，或者是不纯度。基尼指数越大，集合不确定性越高，不纯度也越大。</p>
<h3 id="算法步骤">算法步骤</h3>
<p><strong>1、决策树生成：</strong>基于训练数据集生成决策树，生成的决策树要尽量大。</p>
<p><strong>2、决策树剪枝：</strong>用于验证数据集对已生成的数进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准。</p>
<h2 id="回归树">回归树</h2>
<p>假设一组训练数据为</p>
<p><span class="math display">\[\begin{equation}
    D=\left \{ \left ( x_{1},y_{1} \right ), \left ( x_{2},y_{2} \right ),\cdots,\left ( x_{N},y_{N} \right ) \right \}
\end{equation}\]</span></p>
<p>所谓回归树就把输入空间进行划分，并且在每个划分的单元上进行输出。如果把空间划分为<span class="math inline">\(M\)</span>个单元<span class="math inline">\(R_{1},R_{2},\cdots,R_{M}\)</span>，并且在每个单元<span class="math inline">\(R_{m}\)</span>上有一个固定的输出<span class="math inline">\(c_{m}\)</span>，那么回归树就可以表示为：</p>
<p><span class="math display">\[\begin{equation}
    f\left ( x \right )=\sum_{m=1}^{M}c_{m}I\left ( x \in R_{m} \right )
\end{equation}\]</span></p>
<p>并且回归树使用平方误差最小准则求每个单元<span class="math inline">\(R_{m}\)</span>上的最有输出。由平方误差最小准则就可以知道单元<span class="math inline">\(R_{m}\)</span>上的<span class="math inline">\(c_{m}\)</span>最优值<span class="math inline">\(\hat{c_{m}}\)</span>为所有输入实例输出的<span class="math inline">\(y_{i}\)</span>的均值，即：</p>
<p><span class="math display">\[\begin{equation}
    \hat{c}_{m}=ave\left(y_{i} | x_{i} \in R_{m}\right)=\frac{1}{N_{m}}\sum_{x_{i} \in R_{m}(j,s)}y_{i},\ x \in R_{m},\ m=1,2
\end{equation}\]</span></p>
<p>在CART回归树中，输入空间的划分采用启发式方法，即选择第<span class="math inline">\(j\)</span>个变量<span class="math inline">\(x^{(j)}\)</span>和空间取值<span class="math inline">\(s\)</span>作为分割变量和分割点，并通过分割点定义两个区域：</p>
<p><span class="math display">\[\begin{equation}
    R_{1}\left ( j,s \right )=\left \{ x|x^{\left ( j \right )} \leq s \right \}\ R_{2}\left ( j,s \right )=\left \{ x|x^{\left ( j \right )} &gt; s \right \}
\end{equation}\]</span></p>
<p>因此，最优的分割变量<span class="math inline">\(j\)</span>和分割点<span class="math inline">\(s\)</span>选择就是求解下面的式子：</p>
<p><span class="math display">\[\begin{equation}
    \min_{j, s}\left[\min_{c_{1}} \sum_{x_{i} \in R_{1}(j,s)}\left(y_{i}-c_{1}\right)^{2}+\min_{c_{2}} \sum_{x_{i} \in R_{2}(j,s)}\left(y_{i}-c_{2}\right)^{2}\right]
\end{equation}\]</span></p>
<p>并且对于固定输入<span class="math inline">\(j\)</span>，可以球的最优切点<span class="math inline">\(s\)</span>，使得<span class="math inline">\(\hat{c}_{1}\)</span>和<span class="math inline">\(\hat{c}_{2}\)</span>最优。</p>
<p><span class="math display">\[\begin{equation}
    \hat{c}_{1}=ave\left(y_{i} | x_{i} \in R_{1}(j,s)\right)\ \hat{c}_{2}=ave\left(y_{i} | x_{i} \in R_{2}(j,s)\right)
\end{equation}\]</span></p>
<h3 id="损失函数">损失函数</h3>
<p>回归树使用平方误差来表示回归树对训练数据的预测误差。</p>
<p><span class="math display">\[\begin{equation}
    \sum_{x_{i} \in R_{m}}\left(y_{i}-f\left(x_{i}\right)\right)^{2}
\end{equation}\]</span></p>
<h2 id="分类树">分类树</h2>
<p><span class="math display">\[\begin{equation}
    Gini(D)=1-\sum_{k=1}^{K}\left ( \frac{\left | C_{k} \right |}{\left | D \right |} \right )^2
\end{equation}\]</span></p>
<p>其中<span class="math inline">\(\left | C_{k} \right |\)</span>表示<span class="math inline">\(D\)</span>中属于第<span class="math inline">\(k\)</span>类样本的个数，<span class="math inline">\(\left | D \right |\)</span>数据集的个数，<span class="math inline">\(K\)</span>是类的个数。</p>
<p>如果样本集合<span class="math inline">\(D\)</span>中根据特征<span class="math inline">\(A\)</span>的某个取值<span class="math inline">\(a\)</span>来把<span class="math inline">\(D\)</span>分割为<span class="math inline">\(D_{1}\)</span>和<span class="math inline">\(D_{2}\)</span>，那么在特征A的条件下，集合D的基尼指数为：</p>
<p><span class="math display">\[\begin{equation}
    Gini(D, A)=\frac{\left|D_{1}\right|}{D} Gini\left(D_{1}\right)+\frac{\left|D_{2}\right|}{D} Gini\left(D_{2}\right)
\end{equation}\]</span></p>
<p>其中<span class="math inline">\(Gini(D, A)\)</span>表示经<span class="math inline">\(A=a\)</span>分割后集合<span class="math inline">\(D\)</span>的不确定性。基尼指数值越大，样本集合的不确定性就越大。<strong>在树生成时选择基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点。</strong></p>
<h2 id="剪枝-1">剪枝</h2>
<p>决策树的损失函数为：</p>
<p><span class="math display">\[\begin{equation}
    C_{\alpha}(T)=C(T)+\alpha|T|
\end{equation}\]</span></p>
<p>其中<span class="math inline">\(T\)</span>为任意子树；<span class="math inline">\(C(T)\)</span>为预测误差，用来衡量模型与训练数据的拟合程度；<span class="math inline">\(|T|\)</span>为子树<span class="math inline">\(T\)</span>的叶子节点个数，即树的复杂度；<span class="math inline">\(\alpha\)</span>为惩罚系数，用来控制模型和训练数据之间拟合程度，当<span class="math inline">\(\alpha\)</span>较大时，树模型较为简单，反之则树模型较为复杂。并且当<span class="math inline">\(\alpha\)</span>从0开始不断增加到<span class="math inline">\(+\infty\)</span>时，会产生一系列的子树序列：<span class="math inline">\(T_{0},T_{1},\cdots,T_{n}\)</span>，并且每一个<span class="math inline">\(T_{i+1}\)</span>子树都是由前一个子树<span class="math inline">\(T_{i}\)</span>剪掉某一个内部节点来生成的。</p>
<p>对于任意一个内部节点<span class="math inline">\(t\)</span>，剪枝前有<span class="math inline">\(|T_{t}|\)</span>个叶子节点，并且预测误差为<span class="math inline">\(C(T_{t})\)</span>，剪枝后因为只有一个叶节点<span class="math inline">\(t\)</span>，因此预测误差为<span class="math inline">\(C(t)\)</span>，，因此损失函数的变化为：</p>
<p>剪枝前：</p>
<p><span class="math display">\[\begin{equation}
    C_{\alpha}(T)=C(T)+\alpha|T|
\end{equation}\]</span></p>
<p>剪枝后：</p>
<p><span class="math display">\[\begin{equation}
    C_{\alpha}(t)=C(t)+\alpha \times 1=C(t)+\alpha
\end{equation}\]</span></p>
<p>要是剪枝后的损失函数与剪枝前的损失函数相同<span class="math inline">\(C(T)+\alpha|T|=C(t)+\alpha\)</span>，就可以得到：</p>
<p><span class="math display">\[\begin{equation}
    \alpha=\frac{C(t)-C\left(T_{t}\right)}{\left|T_{t}\right|-1}
\end{equation}\]</span></p>
<p>其中，<span class="math inline">\(\alpha\)</span>的意义在于在<span class="math inline">\(\left [ \alpha_{i}, \alpha_{i+1} \right )\)</span>中惩罚系数的临界值，如果比该<span class="math inline">\(\alpha\)</span>大，那么一定有<span class="math inline">\(C_{\alpha}(T) &gt; C_{\alpha}(t)\)</span>，即剪掉这个节点后都比不剪掉要更优。</p>
<p>对于分类树来说采用叶子节点里概率最大的类别作为当前对象的预测类别。而在回归树中，则是采用最终叶子的均值或者中位数作为预测结果输出。</p>
<h2 id="优点">优点</h2>
<blockquote>
<ul>
<li>基尼指数的计算不需要对数运算，更加高效。</li>
<li>基尼指数更偏向于连续属性，熵更偏向于离散属性。</li>
</ul>
</blockquote>
<h1 id="参考资料">参考资料</h1>
<p>[1] <a href="https://www.cnblogs.com/molieren/articles/10664954.html" target="_blank" rel="noopener">决策树</a></p>
<p>[2] <a href="https://blog.csdn.net/jiaoyangwm/article/details/79525237#3__680" target="_blank" rel="noopener">机器学习实战（三）——决策树</a></p>
<p>[3] <a href="https://zhuanlan.zhihu.com/p/85731206" target="_blank" rel="noopener">【机器学习】决策树</a></p>
]]></content>
      <categories>
        <category>基础</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>决策树</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习资料</title>
    <url>/mlresource/</url>
    <content><![CDATA[<h1 id="国际顶级会议">国际顶级会议</h1>
<h2 id="aaai"><a href="http://www.aaai.org/Conferences/AAAI/aaai.php" target="_blank" rel="noopener">AAAI</a></h2>
<h2 id="cikm-2010"><a href="http://www.yorku.ca/cikm10/papers.php" target="_blank" rel="noopener">CIKM 2010</a></h2>
<h2 id="cikm-2011"><a href="http://www.cikm2011.org/" target="_blank" rel="noopener">CIKM 2011</a></h2>
<a id="more"></a>
<h2 id="colt-2010"><a href="http://www.colt2010.org/" target="_blank" rel="noopener">COLT 2010</a></h2>
<h2 id="colt-2011"><a href="http://colt2011.sztaki.hu/" target="_blank" rel="noopener">COLT 2011</a></h2>
<h2 id="computer-vision-resource"><a href="http://www.cvpapers.com/index.html" target="_blank" rel="noopener">Computer Vision Resource</a></h2>
<h2 id="icjia"><a href="http://ijcai.org/" target="_blank" rel="noopener">ICJIA</a></h2>
<h2 id="icml"><a href="http://www.machinelearning.org/icml.html" target="_blank" rel="noopener">ICML</a></h2>
<h2 id="nips"><a href="http://books.nips.cc/" target="_blank" rel="noopener">NIPS</a></h2>
<h2 id="sigir-2010"><a href="http://www.sigir2010.org/doku.php?id=program:sessions" target="_blank" rel="noopener">SIGIR 2010</a></h2>
<h2 id="sigir-2011"><a href="http://www.sigir2011.org/" target="_blank" rel="noopener">SIGIR 2011</a></h2>
<h2 id="sigkdd"><a href="http://www.kdd.org/kdd2011/" target="_blank" rel="noopener">SIGKDD</a></h2>
<h2 id="sigkdd2010"><a href="http://www.kdd.org/kdd2010/papers.shtml" target="_blank" rel="noopener">SIGKDD2010</a></h2>
<h1 id="论文搜索">论文搜索</h1>
<h2 id="cv顶级会议论文下载"><a href="http://cvpapers.com/" target="_blank" rel="noopener">CV顶级会议论文下载</a></h2>
<h2 id="google-学术搜索"><a href="http://scholar.google.com.hk/schhp?hl=zh-CN&amp;tab=ws" target="_blank" rel="noopener">google 学术搜索</a></h2>
<h2 id="超全计算机视觉资源汇总"><a href="http://www.visionbib.com/bibliography/contents.html" target="_blank" rel="noopener">超全计算机视觉资源汇总</a></h2>
<h2 id="联合参考文献"><a href="http://www.ucdrs.net/admin/union/index.do" target="_blank" rel="noopener">联合参考文献</a></h2>
<h1 id="学术牛人主页">学术牛人主页</h1>
<h2 id="feifei-li--computer-vision"><a href="http://vision.stanford.edu/" target="_blank" rel="noopener">feifei li -computer vision</a></h2>
<h2 id="googlers-in-machine-learning"><a href="http://research.google.com/pubs/MachineLearning.html" target="_blank" rel="noopener">Googlers in Machine Learning</a></h2>
<h2 id="michael-i.-jordan"><a href="http://www.cs.berkeley.edu/~jordan/" target="_blank" rel="noopener">Michael I. Jordan</a></h2>
<h2 id="microsoft-research"><a href="http://research.microsoft.com/apps/dp/areas.aspx" target="_blank" rel="noopener">Microsoft Research</a></h2>
<h2 id="mit-leozhu-cv"><a href="http://people.csail.mit.edu/leozhu/" target="_blank" rel="noopener">mit leozhu cv</a></h2>
<h2 id="pff-cv"><a href="http://www.cs.brown.edu/~pff/" target="_blank" rel="noopener">pff cv</a></h2>
<h2 id="yahoo-research"><a href="http://research.yahoo.com/publication" target="_blank" rel="noopener">Yahoo! Research</a></h2>
<h2 id="zhangzhang-si"><a href="http://www.stat.ucla.edu/~zzsi/" target="_blank" rel="noopener">zhangzhang si</a></h2>
<h2 id="国外人工智能界牛人主页"><a href="http://caiqi1123.blog.163.com/blog/static/5736178120080213836366/" target="_blank" rel="noopener">国外人工智能界牛人主页</a></h2>
<h2 id="计算机视觉相关资源"><a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/" target="_blank" rel="noopener">计算机视觉相关资源</a></h2>
<h2 id="牛人周志华推荐的人工智能网站"><a href="http://blog.csdn.net/huzhyi21/archive/2009/09/20/4573476.aspx" target="_blank" rel="noopener">牛人（周志华）推荐的人工智能网站</a></h2>
<h2 id="数据挖掘牛人-一览"><a href="http://blog.csdn.net/dnnyyq/archive/2010/01/24/5250935.aspx" target="_blank" rel="noopener">数据挖掘牛人 一览</a></h2>
<h2 id="谈机器学习machine-learning大牛"><a href="http://blog.sina.com.cn/s/blog_591e979d0100kds5.html" target="_blank" rel="noopener">谈机器学习(Machine Learning)大牛</a></h2>
<h2 id="周志华"><a href="http://cs.nju.edu.cn/zhouzh/" target="_blank" rel="noopener">周志华</a></h2>
<h1 id="学术期刊">学术期刊</h1>
<h2 id="ieee-transactions-pattern-analysis-and-machine-intelligence"><a href="http://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34" target="_blank" rel="noopener">IEEE Transactions Pattern Analysis and Machine Intelligence</a></h2>
<h2 id="acm-transactions-on-knowledge-discovery-from-data"><a href="http://portal.acm.org/citation.cfm?id=J1054&amp;picked=prox&amp;cfid=16966702&amp;cftoken=61328876" target="_blank" rel="noopener">ACM Transactions on Knowledge Discovery from Data</a></h2>
<h2 id="american-statistical-association---journal-of-the-american-statistical-association"><a href="http://pubs.amstat.org/loi/jasa" target="_blank" rel="noopener">American Statistical Association - Journal of the American Statistical Association</a></h2>
<h2 id="annals-of-statistics"><a href="http://www.imstat.org/aos/" target="_blank" rel="noopener">Annals of Statistics</a></h2>
<h2 id="artificial-intelligent"><a href="http://www.sciencedirect.com/science/journal/00043702" target="_blank" rel="noopener">Artificial Intelligent</a></h2>
<h2 id="computer-vision-and-image-understanding"><a href="http://www.sciencedirect.com/science/journal/10773142/116" target="_blank" rel="noopener">Computer Vision and Image Understanding</a></h2>
<h2 id="data-mining-and-knowledge-discovery"><a href="http://springer.lib.tsinghua.edu.cn/content/1384-5810/" target="_blank" rel="noopener">Data Mining and Knowledge Discovery</a></h2>
<h2 id="ieee-knowledge-and-data-engineering"><a href="http://ieeexplore.ieee.org/xpl/tocresult.jsp?isnumber=4358933" target="_blank" rel="noopener">IEEE Knowledge and Data Engineering</a></h2>
<h2 id="ieee-t.-on-information-theory"><a href="http://ieeexplore.ieee.org/xpl/tocresult.jsp?isnumber=6126885" target="_blank" rel="noopener">IEEE T. on Information Theory</a></h2>
<h2 id="ieee-t.-on-neural-networks"><a href="http://ieeexplore.ieee.org/xpl/tocresult.jsp?isnumber=6099846" target="_blank" rel="noopener">IEEE T. on Neural Networks</a></h2>
<h2 id="ieee-t.-on-systems-machine-and-cybernetics"><a href="http://ieeexplore.ieee.org/xpl/tocresult.jsp?isnumber=10230" target="_blank" rel="noopener">IEEE T. on Systems Machine and Cybernetics</a></h2>
<h2 id="image-processing-ieee-transactions-on"><a href="http://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83" target="_blank" rel="noopener">Image Processing, IEEE Transactions on</a></h2>
<h2 id="image-vision-computing"><a href="http://www.sciencedirect.com/science/journal/02628856" target="_blank" rel="noopener">Image Vision Computing</a></h2>
<h2 id="international-journal-of-computer-vision"><a href="http://springer.lib.tsinghua.edu.cn/content/0920-5691/" target="_blank" rel="noopener">International Journal of Computer Vision</a></h2>
<h2 id="journal-of-machine-learning-research"><a href="http://jmlr.csail.mit.edu/" target="_blank" rel="noopener">Journal of Machine Learning Research</a></h2>
<h2 id="journal-of-the-royal-statistical-society-series-b-statistical-methodology"><a href="http://onlinelibrary.wiley.com/journal/10.1111/(ISSN)1467-9868" target="_blank" rel="noopener">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</a></h2>
<h2 id="machine-learning"><a href="http://springer.lib.tsinghua.edu.cn/content/0885-6125/" target="_blank" rel="noopener">Machine Learning</a></h2>
<h2 id="neural-computation"><a href="http://www.mitpressjournals.org/toc/neco/24/3" target="_blank" rel="noopener">Neural Computation</a></h2>
<h2 id="pattern-analysis-and-applications"><a href="http://www.springerlink.com/content/1433-7541" target="_blank" rel="noopener">Pattern Analysis and Applications</a></h2>
<h2 id="pattern-recognition"><a href="http://www.sciencedirect.com/science/journal/00313203" target="_blank" rel="noopener">Pattern Recognition</a></h2>
<h1 id="牛人主页主页有很多论文代码">牛人主页（主页有很多论文代码）</h1>
<h2 id="serge-belongie-at-uc-san-diego"><a href="http://cseweb.ucsd.edu/~sjb/" target="_blank" rel="noopener">Serge Belongie at UC San Diego</a></h2>
<h2 id="antonio-torralba-at-mit"><a href="http://web.mit.edu/torralba/www/" target="_blank" rel="noopener">Antonio Torralba at MIT</a></h2>
<h2 id="alexei-ffros-at-cmu"><a href="http://www.cs.cmu.edu/~efros/" target="_blank" rel="noopener">Alexei Ffros at CMU</a></h2>
<h2 id="ce-liu-at-microsoft-research-new-england"><a href="http://people.csail.mit.edu/celiu/" target="_blank" rel="noopener">Ce Liu at Microsoft Research New England</a></h2>
<h2 id="vittorio-ferrari-at-univ.of-edinburgh"><a href="http://www.vision.ee.ethz.ch/~calvin/" target="_blank" rel="noopener">Vittorio Ferrari at Univ.of Edinburgh</a></h2>
<h2 id="kristen-grauman-at-ut-austin"><a href="http://www.cs.utexas.edu/~grauman/" target="_blank" rel="noopener">Kristen Grauman at UT Austin</a></h2>
<h2 id="devi-parikh-at-tti-chicago-marr-prize-at-iccv2011"><a href="http://ttic.uchicago.edu/~dparikh/index.html" target="_blank" rel="noopener">Devi Parikh at TTI-Chicago (Marr Prize at ICCV2011)</a></h2>
<h2 id="john-wright-at-columbia-univ."><a href="http://www.columbia.edu/~jw2966/" target="_blank" rel="noopener">John Wright at Columbia Univ.</a></h2>
<h2 id="piotr-dollar-at-caltech"><a href="http://vision.ucsd.edu/~pdollar/" target="_blank" rel="noopener">Piotr Dollar at CalTech</a></h2>
<h2 id="boris-babenko-at-uc-san-diego"><a href="http://vision.ucsd.edu/~bbabenko/" target="_blank" rel="noopener">Boris Babenko at UC San Diego</a></h2>
<h2 id="david-ross-at-googleyoutube"><a href="http://www.cs.toronto.edu/~dross/" target="_blank" rel="noopener">David Ross at Google/Youtube</a></h2>
<h2 id="david-donoho-at-stanford-univ."><a href="http://www-stat.stanford.edu/~donoho/index.html" target="_blank" rel="noopener">David Donoho at Stanford Univ.</a></h2>
]]></content>
      <categories>
        <category>资源</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>【2013】A Review on Multi-Label Learning Algorithms</title>
    <url>/review-multilabel-alg/</url>
    <content><![CDATA[<h1 id="介绍">介绍</h1><p>在传统的单标签监督学习中，<span class="math inline">\(x\)</span>表示一个实体空间，<span class="math inline">\(y\)</span>表示一个标签空间，而传统的监督学习就是通过训练集</p><p><span class="math display">\[\begin{align}
    \left \{ \left ( x_{i} | y_{i} \right ) | 1 \leq i \leq m \right \}
\end{align}\]</span></p><a id="more"></a>


<p>练一个方法使得<span class="math inline">\(f:\mathcal{X} \rightarrow \mathcal{Y}\)</span>，并且<span class="math inline">\(x_{i} \in \mathcal{X}\)</span>表示一个特征属性的对象，而<span class="math inline">\(y_{i} \in \mathcal{Y}\)</span>这事该特征对应的语义标签。</p>
<p>与传统的单标签的监督学习相比，多标签学习中每一个对象都是单独的一个实体，并且这个对象具有一组标签集，而不是一个标签。因此，多标签学习的任务就是构造能够预测实体所具有的标签集的方法。早期的多标签学习主要应用于文本分类，然而在过去十年间，多标签学习已经被广泛的应用于图像、生物信息等领域。</p>
<p>这篇文章对多标签学习进行了回顾，主要分为三个部分：</p>
<p>1、多标签学习的原理（包括学习框架、关键挑战、阈值校准）和评价指标（包括基于实例的评价、基于标签的评价、理论结果）</p>
<p>2、对八种最具代表性的算法进行总结和讨论</p>
<p>3、简要总结了几种相关的学习设定</p>
<h1 id="范式">范式</h1>
<h2 id="形式定义">形式定义</h2>
<p>变量及意义</p>
<table>
<colgroup>
<col style="width: 16%">
<col style="width: 83%">
</colgroup>
<thead>
<tr class="header">
<th>符号</th>
<th style="text-align: left;">数学意义</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\mathcal{X}\)</span></td>
<td style="text-align: left;">表示<span class="math inline">\(d\)</span>维的实体空间<span class="math inline">\(\mathbb{R}^{d}\)</span>（或<span class="math inline">\(\mathbb{Z}^{d}\)</span>）</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\mathcal{Y}\)</span></td>
<td style="text-align: left;">表示带有<span class="math inline">\(q\)</span>个可能标签的标签空间<span class="math inline">\(\left \{ y_{1}, y_{2}, \cdots , y_{q} \right \}\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(x\)</span></td>
<td style="text-align: left;">表示具有<span class="math inline">\(d\)</span>维的特征向量实体<span class="math inline">\(x\)</span>，<span class="math inline">\(\left ( x_{1}, x_{2}, \cdots , x_{d} \right )^{T} \left ( x \in \mathcal{X} \right )\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(Y\)</span></td>
<td style="text-align: left;">表示与某个实体<span class="math inline">\(x\)</span>的相关标签集<span class="math inline">\(\left ( Y \in \mathcal{Y} \right )\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\bar{Y}\)</span></td>
<td style="text-align: left;">表示在<span class="math inline">\(\mathcal{Y}\)</span>中<span class="math inline">\(Y\)</span>的补集</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\mathcal{D}\)</span></td>
<td style="text-align: left;">表示多标签训练集<span class="math inline">\(\left \{ \left ( x_{i}, Y_{i} \right ) \mid 1 \leq i \leq m \right \}\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\mathcal{S}\)</span></td>
<td style="text-align: left;">表示多标签测试集<span class="math inline">\(\left \{ \left ( x_{i}, Y_{i} \right ) \mid 1 \leq i \leq p \right \}\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(f\left ( \cdot , \cdot \right )\)</span></td>
<td style="text-align: left;">表示一个实值函数<span class="math inline">\(f: \mathcal{X} \times \mathcal{Y} \rightarrow \mathbb{R}\)</span>, <span class="math inline">\(f\left ( x, y \right )\)</span>返回<span class="math inline">\(x\)</span>每个可能标签<span class="math inline">\(y\)</span>的置信度。简单说，当与<span class="math inline">\(x\)</span>相关的标签<span class="math inline">\({y}&#39; \in Y\)</span>时，那么置信度函数<span class="math inline">\(f\)</span>返回最大值，当与<span class="math inline">\(x\)</span>不相关的标签<span class="math inline">\({y}&#39;&#39; \notin Y\)</span>时，那么置信度函数<span class="math inline">\(f\)</span>返回最小值，即<span class="math inline">\(f\left ( x,{y}&#39; \right ) &gt; f\left ( x, {y}&#39;&#39; \right )\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(t\left ( \cdot \right )\)</span></td>
<td style="text-align: left;">表示阈值函数<span class="math inline">\(t: \mathcal{X} \rightarrow \mathbb{R}\)</span>, 即该函数能够把<span class="math inline">\(\mathcal{X}\)</span>映射到<span class="math inline">\(\mathbb{R}\)</span>上，并且对于分类器<span class="math inline">\(h\left ( \cdot \right )\)</span>来说只有当实值函数<span class="math inline">\(f\left ( \cdot , \cdot \right )\)</span>的值大于阈值函数<span class="math inline">\(t\left ( \cdot \right )\)</span>时，分类器<span class="math inline">\(h\left ( \cdot \right )\)</span>才能返回可能的标签集，数学表达为： <span class="math inline">\(h\left ( x \right ) = \left \{ y \mid f \left ( x, y \right ) &gt; t \left ( x \right ) , y \in \mathcal{Y} \right \}\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(h\left ( \cdot \right )\)</span></td>
<td style="text-align: left;">表示一个多标签分类器函数<span class="math inline">\(h: \mathcal{X} \rightarrow 2^{\mathcal{Y}}\)</span>, <span class="math inline">\(h\left ( x \right )\)</span>返回<span class="math inline">\(x\)</span>可能的标签集，该函数由置信度函数<span class="math inline">\(f\left ( \cdot , \cdot \right )\)</span>和阈值函数<span class="math inline">\(t\left ( \cdot \right )\)</span>组合得到<span class="math inline">\(h\left ( x \right ) = \left \{ y \mid f \left ( x, y \right ) &gt; t \left ( x \right ) , y \in \mathcal{Y} \right \}\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(rank_{f}\left ( \cdot , \cdot \right )\)</span></td>
<td style="text-align: left;">表示通过<span class="math inline">\(f\left ( x , \cdot \right )\)</span>函数获得每个在相关标签集<span class="math inline">\(\mathcal{Y}\)</span>中的标签<span class="math inline">\(y\)</span>的置信度，并对这些置信度进行降序排名</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\left | \cdot \right |\)</span></td>
<td style="text-align: left;">表示以集合<span class="math inline">\(\mathcal{A}\)</span>为例，<span class="math inline">\(\left | \mathcal{A} \right |\)</span>返回集合<span class="math inline">\(\mathcal{A}\)</span>的基数</td>
</tr>
<tr class="odd">
<td><span class="math inline">\([\![ \cdot ]\!]\)</span></td>
<td style="text-align: left;">表示当<span class="math inline">\(\pi\)</span>成立时<span class="math inline">\([\![ \pi ]\!]\)</span>返回数字1，否则返回0</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\phi \left ( \cdot , \cdot \right )\)</span></td>
<td style="text-align: left;">表示当<span class="math inline">\(y \in Y\)</span>时，<span class="math inline">\(\phi \left ( Y , y \right )\)</span>返回+1，否则返回-1</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\mathcal{D}_{j}\)</span></td>
<td style="text-align: left;">表示从多标签训练集<span class="math inline">\(\mathcal{D}\)</span>中的第<span class="math inline">\(j\)</span>类标签<span class="math inline">\(y_{j}\)</span>得到的二元训练集<span class="math inline">\(\left \{ \left ( x_{i}, \phi \left ( Y_{i}, y_{i} \right ) \right ) \mid 1 \leq i \leq m \right \}\)</span>，即当遍历所有的<span class="math inline">\(x_{i}\)</span>所对应的相关标签集<span class="math inline">\(Y_{i}\)</span>，当<span class="math inline">\(y_{j} \in Y_{i}\)</span>时，那么上式<span class="math inline">\(\left ( x_{i}, \phi \left ( Y_{i}, y_{i} \right ) \right )\)</span>就为1，否则为0</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\psi \left ( \cdot ,\cdot ,\cdot \right )\)</span></td>
<td style="text-align: left;">表示假如<span class="math inline">\(y_{j} \in Y\)</span>，并且<span class="math inline">\(y_{k} \notin Y\)</span>，那么<span class="math inline">\(\psi \left ( Y, y_{j}, y_{k} \right )\)</span>返回+1，而假如<span class="math inline">\(y_{j} \notin Y\)</span>，并且<span class="math inline">\(y_{k} \in Y\)</span>，那么<span class="math inline">\(\psi \left ( Y, y_{j}, y_{k} \right )\)</span>返回-1</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\mathcal{D}_{jk}\)</span></td>
<td style="text-align: left;">表示从多标签训练集<span class="math inline">\(\mathcal{D}\)</span>中的标签对<span class="math inline">\(\left(y_{j}, y_{k}\right)\)</span>得到的二元训练集<span class="math inline">\(\left \{ \left ( x_{i}, \psi \left ( Y_{i}, y_{j}, y_{k} \right ) \right ) \mid \phi \left ( Y_{i},y_{j} \neq \phi \left ( Y_{i}, y_{k} \right ), 1 \leq i \leq m \right ) \right \}\)</span>，即当遍历所有的<span class="math inline">\(x_{i}\)</span>所对应的相关标签集<span class="math inline">\(Y_{i}\)</span>时，<span class="math inline">\(y_{j}\)</span>和<span class="math inline">\(y_{k}\)</span>有且只有一个属于<span class="math inline">\(Y_{i}\)</span>时，那么上式<span class="math inline">\(\left ( x_{i}, \psi \left ( Y_{i}, y_{j}, y_{k} \right ) \right )\)</span>就为1，否则为-1</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\sigma_{\mathcal{Y}}\left ( \cdot \right )\)</span></td>
<td style="text-align: left;">表示一个把<span class="math inline">\(\mathcal{Y}\)</span>的幂集映射到正整数的映射函数<span class="math inline">\(\sigma_{\mathcal{Y}}:2^{\mathcal{Y}} \rightarrow \mathbb{N}\)</span>（<span class="math inline">\(\sigma_{\mathcal{Y}}^{-1}\)</span>表示逆函数）</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\mathcal{D}_{\mathcal{Y}}^{\dagger}\)</span></td>
<td style="text-align: left;">表示从训练集<span class="math inline">\(\mathcal{D}\)</span>获得多类单标签训练集<span class="math inline">\(\left \{ \left ( x_{i}, \sigma_{\mathcal{Y}}\left ( Y_{i} \right ) \right ) \mid 1 \leq i \leq m \right \}\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\mathcal{B}\)</span></td>
<td style="text-align: left;">表示二元学习算法【复杂度：<span class="math inline">\(\mathcal{F}_{\mathcal{B}}\left ( m,d \right )\)</span>为训练集的复杂度，<span class="math inline">\(\mathcal{F}_{\mathcal{B}}^{&#39;}\left ( d \right )\)</span>为每个实例的测试集复杂度】</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\mathcal{M}\)</span></td>
<td style="text-align: left;">表示多类学习算法【复杂度：<span class="math inline">\(\mathcal{F}_{\mathcal{M}}\left ( m,d,q \right )\)</span>为训练集的复杂度，<span class="math inline">\(\mathcal{F}_{\mathcal{M}}^{&#39;}\left ( d,q \right )\)</span>为每个实例的测试集复杂度】</td>
</tr>
</tbody>
</table>
<h3 id="学习框架">学习框架</h3>
<p>有多种有效的多标签指标可以用于描述多标签数据集的特征。其中最直接的方法就是首先测量多标签数据集的方法是平均标签基数：<span class="math inline">\(LCard\left ( \mathcal{D} \right ) = \frac{1}{m} \sum_{i = 1}^{m} \left | Y_{i} \right |\)</span>，即遍历所有<span class="math inline">\(x_{i}\)</span>所对应的相关标签集<span class="math inline">\(Y_{i}\)</span>的基数，并且求和在求平均，从而得到标签集的平均标签基数。然后通过标签基数归一化得到标签密度<span class="math inline">\(LDen\left ( \mathcal{D} \right ) = \frac{1}{\left | \mathcal{Y} \right |} \cdot LCard \left ( \mathcal{D} \right )\)</span>。另一个广泛用于测量多标签数据集多样性的方法是：<span class="math inline">\(LDiv \left ( \mathcal{D} \right ) = \left | \left \{ Y \mid \exists x : \left ( x, Y \right ) \in \mathcal{D} \right \} \right |\)</span>，即在某个多标签数据集中出现不同标签集的数量。同样，标签的多样性可以通过实例的数量来进行归一化，以表示不同标签集的比例：<span class="math inline">\(PLDiv \left ( \mathcal{D} \right ) = \frac{1}{\left | \mathcal{D} \right |} \cdot LDiv\left ( \mathcal{D} \right )\)</span>。</p>
<h3 id="主要挑战">主要挑战</h3>
<p>多标签学习的主要挑战是随着标签种类的增加，标签集的增加呈指数级增长。假如有20种标签（<span class="math inline">\(q = 20\)</span>），那么可能的标签集就有<span class="math inline">\(2^{20}\)</span>个。</p>
<p>为了应对标签集的指数增长，就需要在学习过程中增加标签与标签之间的关系或者依赖。因此，利用标签之间的关系就是多标签学习成功的关键。目前增加标签和标签之间的关联按照所使用方法的顺序，主要包含三种：</p>
<p>1、一阶策略：一阶策略把多标签学习转化为多个不相关的二元分类问题，即对每个标签进行分类。该方法的优点就是简单，当时缺点是由于忽略了标签之间的关系，因此效果次之。</p>
<p>2、二阶策略：二阶策略考虑了标签对内的关系，即某个标签和其他所有标签之间关联度的排序。因此，使用二阶策略具有良好的泛化性。但在实际应用中标签直接的关系往往超过两两之间的二阶关系。</p>
<p>3、高阶策略：高阶策略考虑了一个标签和所有标签之间的关系或者多个随机标签子集的关系。因此，使用高阶策略与采用一阶和二阶策略相比具有更好的泛化性，但计算要求更高，可扩展性更低。</p>
<h3 id="阈值标定">阈值标定</h3>
<p>在多标签学习中通常采用置信度函数<span class="math inline">\(f\left ( \cdot , \cdot \right )\)</span>的返回值作为学习的模型。因此为了获得实例<span class="math inline">\(x\)</span>的正确标签集，即<span class="math inline">\(h\left ( x \right )\)</span>，那么每个标签的置信度函数<span class="math inline">\(f\left ( x , y \right )\)</span>都需要通过阈值函数<span class="math inline">\(t\left ( x \right )\)</span>进行校准。通常阈值标定有两种方法，分别是使用固定的阈值函数<span class="math inline">\(t\left ( \cdot \right )\)</span>和通过训练集中获取动态阈值函数<span class="math inline">\(t\left ( \cdot \right )\)</span>。</p>
<p>1、固定阈值函数：采用这种方法最直接的就是使用0作为标定常量。另一个做法是使用0.5作为标定常量。此外，对于在测试集中未知的实例<span class="math inline">\(x\)</span>，设置标定常量可以最小化训练集和测试集在某个多标签指标上的差异，特别是标签基数。</p>
<p>2、动态阈值函数：采用这种方法通常使用stacking处理（即将训练集拆分为N个部分，当某个模型随机训练拆分后的某个数据集，并把训练后的结果给到下一个新的训练集，以此类推）来获取动态阈值函数。一个常用的获取动态阈值函数<span class="math inline">\(t\left ( \cdot \right )\)</span>的方法是使用线性模型，即当<span class="math inline">\(f^{*}\left ( x \right ) = \left ( f\left ( x, y_{1} \right ), \cdots , f\left ( x, y_{q} \right ) \right )^{T} \in \mathbb{R}^{q}\)</span>时（保存了实例<span class="math inline">\(x\)</span>对于<span class="math inline">\(q\)</span>个标签的置信度），阈值函数模型为<span class="math inline">\(t\left ( x \right ) = \left \langle \omega ^{*}, f^{*}\left ( x \right ) \right \rangle + b^{*}\)</span>，就是说只要能计算出<span class="math inline">\(q\)</span>维向量的权重<span class="math inline">\(w^{*}\)</span>和偏差<span class="math inline">\(b^{*}\)</span>，就可以解决基于训练集<span class="math inline">\(\mathcal{D}\)</span>的线性最小二乘问题<span class="math inline">\(\underset{\left \{ \omega^{*}, b^{*} \right \}}{min}\sum_{i=1}^{m} \left ( \left \langle w^{*}, f^{*}\left ( x_{i} \right ) \right \rangle + b^{*} - s\left ( x_{i} \right ) \right )^{2}\)</span>。其中，<span class="math inline">\(s\left ( x_{i} \right ) = argmin_{a \in \mathbb{R}}\left ( \left | \left \{ y_{j} \mid y_{j} \in Y_{i}, f\left ( x_{i}, y_{i} \right ) \leq a \right \} \right | + \left | \left \{ y_{k} \mid y_{k} \in \bar{Y_{i}}, f\left ( x_{i}, y_{k} \right ) \geq a \right \} \right | \right )\)</span>表示stacking模型处理输出，该模型把每个训练集中可能与<span class="math inline">\(x_{i}\)</span>相关的标签集<span class="math inline">\(\mathcal{Y}\)</span>分为相关和不相关两个部分，从而达到最小的错误分类。 \end{enumerate}</p>
<blockquote>
<p>最小二乘问题：简单理解就是让总误差的平方最小<span class="math inline">\(\varepsilon\)</span>的<span class="math inline">\(y\)</span>就是真值。通过对下式求导就可以得到最小二乘的一个特例算术平均数，即算术平均数可以让误差最小。其中的“二乘”就是指平方。</p>
</blockquote>

<blockquote>
<p>把最小二乘进行推广，就可以到如线性方程<span class="math inline">\(f\left ( x \right ) = ax + b\)</span>，又可以得到总误差<span class="math inline">\(\varepsilon = \sum \left ( f\left ( x_{i} \right ) - y_{i} \right )^{2} = \sum \left ( ax_{i} + b - y_{i} \right )^{2}\)</span>。</p>
</blockquote>
<img src="/review-multilabel-alg/least-squares.png" class title="最小二乘">
<img src="/review-multilabel-alg/linear-least-squares.png" class title="线性最小二乘">
<h2 id="评价指标">评价指标</h2>
<h3 id="评价分类">评价分类</h3>
<p>传统监督学习的泛化性评价指标通常采用精度、F值（F-Measure）、ROC曲线的下面积（AUC）等来表示。由于多标签学习中一个实例会和多个标签相关联，而不是像单标签学习中一个实例和一个标签相关联。因此与传统的但标签学习相比，多标签学习的评价指标要复杂的多，大致可以分为两大类，分别是基于实例的评价和基于标签的评价。</p>
<img src="/review-multilabel-alg/roc.png" class title="ROC和AUC">
<blockquote>
<ul>
<li>1、准确率（Accuracy）：预测准确率的高低，即<span class="math inline">\(\frac{TP+TN}{TP+TN+FP+FN}\)</span>。</li>
<li>2、查准率（Precision，P指标，【宁愿漏掉，不可错杀】）：预测为正例中实际为正例的占比，即<span class="math inline">\(\frac{TP}{TP+FP}\)</span>。</li>
<li>3、查全率（Recall，R指标，【宁愿错杀，不可漏掉】）：，正例预测正确占所有正例预测的比例，即<span class="math inline">\(\frac{TP}{TP+FN}\)</span>。</li>
<li>4、F值：查准率和查全率在非负权重<span class="math inline">\(\beta\)</span>下的加权调和平均值，即<span class="math inline">\(\frac{\left ( 1 + \beta^{2} \right ) \times P \times R}{\beta ^{2}\left ( P+R \right )}\)</span>，<span class="math inline">\(\beta\)</span>一般取0.3。当<span class="math inline">\(\beta\)</span>取1时，那么F值就为P指标和R指标的调和平均，即F1值。</li>
<li>5、ROC曲线：该曲线是由X轴的假正率（FPR，<span class="math inline">\(\frac{FP}{FP+TN}\)</span>）和Y轴的真正率（TPR，<span class="math inline">\(\frac{TP}{TP+FN}\)</span>）组成一个<span class="math inline">\(1 \times 1\)</span>正方形，如图1所示。</li>
<li>6、AUC面积：即ROC曲线的下半部分面积，当<span class="math inline">\(AUC &lt; 0.5\)</span>表示预测没有意义，当<span class="math inline">\(0.5 &lt; AUC &lt; 0.7\)</span>表示预测价值较低，当<span class="math inline">\(0.5 &lt; AUC &lt; 0.9\)</span>表示具有一定的预测价值，当<span class="math inline">\(AUC &gt; 0.9\)</span>表示预测准确度较高。</li>
</ul>
</blockquote>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">项目</th>
<th style="text-align: center;">预测正类（Positive）</th>
<th style="text-align: center;">预测负类（Negative）</th>
<th style="text-align: center;">实际总计</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">实际正类（True）</td>
<td style="text-align: center;">将正类预测为正类（TP）</td>
<td style="text-align: center;">将正类预测为负类（FN）</td>
<td style="text-align: center;">实际为正类（<span class="math inline">\(TP + FN = P\)</span>）</td>
</tr>
<tr class="even">
<td style="text-align: center;">实际负类（False）</td>
<td style="text-align: center;">将负类预测为正类（FP）</td>
<td style="text-align: center;">将负类预测为负类（TN）</td>
<td style="text-align: center;">实际为负类（<span class="math inline">\(FP + TN = N\)</span>）</td>
</tr>
<tr class="odd">
<td style="text-align: center;">预测总计</td>
<td style="text-align: center;">预测为正类（<span class="math inline">\(TP + FP = P^{&#39;}\)</span>）</td>
<td style="text-align: center;">预测为负类（<span class="math inline">\(TN + FN = N^{&#39;}\)</span>）</td>
<td style="text-align: center;"><span class="math inline">\(P+N\)</span></td>
</tr>
</tbody>
</table>
<p>1、基于实例的评价：通过获取每个实例的性能评价，然后取平均。</p>
<p>2、基于标签的评价：通过对每个标签的性能进行评价从而获取整个学习系统的性能，并且返回所有类别标签的宏平均或微平均。</p>
<blockquote>
<ul>
<li>1、宏平均：即在多标签（多类）分类中把每一类指标做算术平均，然后在根据平均后的结果计算F值。</li>
<li>2、微平均：即在多标签（多类）分类中的查准率<span class="math inline">\(\frac{\sum TP}{\sum TP + \sum FP}\)</span>。</li>
</ul>
</blockquote>
<img src="/review-multilabel-alg/evaluation-metrics.png" class title="主要的多标签评价指标">
<p>需要注意的是，除了从分类角度考虑多标签分类系统<span class="math inline">\(h\left ( \cdot \right )\)</span>的泛化性外，也可以通过置信度<span class="math inline">\(f\left ( \cdot , \cdot \right )\)</span>的排序来考虑多标签分类系统的泛化性</p>
<h3 id="基于实例的评价指标">基于实例的评价指标</h3>
<p>1、子集精度（Subset Accuracy）：即分类正确的比例，如预测标签集<span class="math inline">\(h\left ( x_{i} \right )\)</span>与基准标签集<span class="math inline">\(Y_{i}\)</span>相同时，则<span class="math inline">\([\![h\left ( x_{i} \right )=Y_{i}]\!]\)</span>返回1，否则返回0，其中<span class="math inline">\(p\)</span>为实例的个数。但当标签集空间<span class="math inline">\(q\)</span>很大时，由于判定的条件是全等，因此过于严格，造成评价解决并不合理。</p>
<p><span class="math display">\[\begin{align}
    subsetacc(h)=\frac{1}{p}\sum_{i=1}^{p}[\![h\left ( x_{i} \right )=Y_{i}]\!]
\end{align}\]</span></p>
<p>2、汉明损失（Hamming Loss）：即计算每个实例预测得到的标签集与基准标签集的差值的数量，包含没有被预测到的或者被误分类的标签数量，其中<span class="math inline">\(h\left ( x_{i} \right )\)</span>表示实际预测得到的标签集，<span class="math inline">\(\Delta Y_{i}\)</span>表示预测标签集和标准标签集的差值。当属于测试集<span class="math inline">\(\mathcal{S}\)</span>的实例只与一个标签相关，则<span class="math inline">\(hloss\left ( h \right )\)</span>是传统分类误差率的<span class="math inline">\(2 / q\)</span>倍。</p>
<p><span class="math display">\[\begin{align}
    hloss\left ( h \right )=\frac{1}{p}\sum_{i=1}^{p}\left | h\left ( x_{i} \right )\Delta Y_{i} \right |
\end{align}\]</span></p>
<p>3、准确率（Accuracy）、查准率（Precision，P指标）、查全率（Recall，R指标）、F值。</p>
<p><span class="math display">\[\begin{align}
    \begin{matrix}
        Accuracy_{exam}\left ( h \right )=\frac{1}{p}\sum_{i=1}^{p}\frac{\left | Y_{i}\bigcap h\left ( x_{i} \right ) \right |}{\left | Y_{i}\bigcup h\left ( x_{i} \right ) \right |} &amp; Precision_{exam}\left ( h \right )=\frac{1}{p}\sum_{i=1}^{p}\frac{\left | Y_{i}\bigcap h\left ( x_{i} \right ) \right |}{\left | h\left ( x_{i} \right ) \right |} \\ 
        Recall_{exam}\left ( h \right )=\frac{1}{p}\sum_{i=1}^{p}\frac{\left | Y_{i}\bigcap h\left ( x_{i} \right ) \right |}{\left | Y_{i} \right |} &amp; F_{\beta }^{exam}\left ( h \right )=\frac{\left ( 1+\beta ^{2} \right )\cdot Precision_{exam}\left ( h \right )\cdot Recall_{exam}\left ( h \right )}{\beta ^{2}\cdot Precision_{exam}\left ( h \right )+Recall_{exam}\left ( h \right )}
    \end{matrix}
\end{align}\]</span></p>
<p>通过置信度函数<span class="math inline">\(f\left ( \cdot ,\cdot \right )\)</span>可以有四个基于实例的排序指标对多标签实例进行度量。</p>
<p>1、1-错误率（One-error）：即输出结果中排序第一的标签一定不属于当前实例。</p>
<p><span class="math display">\[\begin{align}
    one-error\left ( f \right ) = \frac{1}{p}\sum_{i=1}^{p} [\![\left [ argmax_{y\in \mathcal{Y}}f\left ( x_{i}, y \right ) \right ] \notin Y_{i}]\!]
\end{align}\]</span></p>
<p>2、覆盖率（Coverage）：对获取的每个与实例<span class="math inline">\(x_{i}\)</span>相关标签<span class="math inline">\(y\)</span>的置信度按照降序进行排名后，获得最后一个被正确预测的标签排名与第一个正确预测的标签排名的差值（或者说是距离），即距离越小越好，说明正确预测的标签即排名越高。</p>
<p><span class="math display">\[\begin{align}
    coverage\left ( f \right )=\frac{1}{p}\sum_{i=1}^{p}max_{y\in Y_{i}}rank_{f}\left ( x_{i},y \right )-1
\end{align}\]</span></p>
<img src="/review-multilabel-alg/coverage.png" class title="覆盖率的例子">
<p>3、损失排序（Ranking Loss）：把与<span class="math inline">\(x_{i}\)</span>相关的标签集<span class="math inline">\({y}&#39;\)</span>的置信度与不相关的标签集<span class="math inline">\({y}&#39;&#39;\)</span>的置信度进行两两排序比较，获得不相关标签集出现在相关标签集中的概率，该损失值越小，预测结果越好。</p>
<p><span class="math display">\[\begin{align}
    rloss\left ( f \right )=\frac{1}{p}\sum_{i=1}^{p}\left | \left \{ \left ( {y}&#39;,{y}&#39;&#39; \right ) \mid f\left ( x_{i},{y}&#39; \right ) \leq f\left ( x_{i},{y}&#39;&#39; \right ),\left ( {y}&#39;,{y}&#39;&#39; \right ) \in Y_{i}\times \bar{Y_{i}} \right \} \right |
\end{align}\]</span></p>
<p>4、平均精度（Average Precision）：分母是标签<span class="math inline">\(x_{i}\)</span>的相关标签<span class="math inline">\(y\)</span>的排名，分子表示属于<span class="math inline">\(x_{i}\)</span>相关标签集<span class="math inline">\(Y_{i}\)</span>且排名小于等于相关标签<span class="math inline">\(y\)</span>的个数。该值越大，预测效果越好。</p>
<p><span class="math display">\[\begin{align}
    avgprec\left ( f \right )=\frac{1}{p}\sum_{i=1}^{p}\frac{1}{\left | Y_{i} \right |}\underset{y \in Y_{i}}{\sum} \frac{\left | \left \{ {y}&#39;\mid rank_{f}\left ( x,{y}&#39; \right ) \leq rank_{f}\left ( x_{i},y \right ),{y}&#39;\in Y_{i} \right \} \right |}{rank_{f}\left ( x_{i},y \right )}
\end{align}\]</span></p>
<p>对于1-错误率（One-error）、覆盖率（Coverage）、损失排序（Ranking Loss）而言都是值越小说明学习系统的性能越好，当1-错误率（One-error）和损失排序（Ranking Loss）为0时，覆盖率（Coverage）为<span class="math inline">\(\frac{1}{p}\sum_{i=1}^{p}\left | Y_{i} \right |-1\)</span>时，系统性能最好。而对于其他评价系统来说，值越大说明系统性能越好，1为最佳性能。</p>
<h3 id="基于标签的评价指标">基于标签的评价指标</h3>
<p>对于基于多标签分类器<span class="math inline">\(h\left ( \cdot \right )\)</span>的第<span class="math inline">\(j\)</span>类标签<span class="math inline">\(y_{j}\)</span>有四个基本特征量，这四个基本特征量都是在二元分类上某个标签基本特征量，分别是</p>
<p><span class="math display">\[\begin{align}
    \begin{matrix}
        TP_{j}=\left | \left \{ x_{i} \mid y_{i} \in Y_{i} \wedge y_{j} \in h\left ( x_{i} \right ), 1 \leq i \leq p \right \} \right | &amp; FP_{j}=\left | \left \{ x_{i} \mid y_{i} \notin Y_{i} \wedge y_{j} \in h\left ( x_{i} \right ), 1 \leq i \leq p \right \} \right | \\ 
        TN_{j}=\left | \left \{ x_{i} \mid y_{i} \notin Y_{i} \wedge y_{j} \notin h\left ( x_{i} \right ), 1 \leq i \leq p \right \} \right | &amp; FN_{j}=\left | \left \{ x_{i} \mid y_{i} \in Y_{i} \wedge y_{j} \notin h\left ( x_{i} \right ), 1 \leq i \leq p \right \} \right |
    \end{matrix}
\end{align}\]</span></p>
<p>其中<span class="math inline">\(TP_{j}\)</span>表示正类预测为正类，<span class="math inline">\(FP_{j}\)</span>表示负类预测为正类，<span class="math inline">\(TN_{j}\)</span>表示正类预测为负类，<span class="math inline">\(FN_{j}\)</span>表示负类预测为负类。并且基于上面四个基本特征量可以得到大部分的二元分类指标。假设<span class="math inline">\(B \left(TP_{j}, FP_{j}, TN_{j}, FN_{j}\right)\)</span>表示一个二元分类的评价指标（<span class="math inline">\(B \in \left [ 准确率、查准率、查全率、F^{\beta} \right ]^{4}\)</span>），那么基于标签的分类指标就可以从宏平均或微平均得到。</p>
<p>1、宏平均（Macro-averaging）</p>
<p><span class="math display">\[\begin{align}
    B_{macro}\left ( h \right )=\frac{1}{q}\sum_{j=1}^{q}B\left ( TP_{j},FP_{j},TN_{j},FN_{j} \right )
\end{align}\]</span></p>
<p>2、微平均（Micro-averaging）</p>
<p><span class="math display">\[\begin{align}
    B_{micro}\left ( h \right )=B\left ( \sum_{j=1}^{q}TP_{j},\sum_{j=1}^{q}FP_{j},\sum_{j=1}^{q}TN_{j},\sum_{j=1}^{q}FN_{j} \right )
\end{align}\]</span></p>
<p>同时，还可以得到<span class="math inline">\(Accuracy_{macro}\left ( h \right )=Accuracy_{micro}\left ( h \right )\)</span>，以及<span class="math inline">\(Accuracy_{micro}\left ( h \right )+hloss\left ( h \right )=1\)</span>。</p>
<p>当置信度函数<span class="math inline">\(f\left(\cdot , \cdot \right)\)</span>可用时，可以获得基于标签的指标排序，如宏平均AUC，并且下式也遵守AUC和Wilcoxon-Mann-Whitney统计量之间的关系。</p>
<p><span class="math display">\[\begin{align}
    AUC_{macro}=\frac{1}{q}\sum_{j=1}^{q}AUC_{j}=\frac{1}{q}\sum_{j=1}^{q}\frac{\left | \left \{ \left ( {x}&#39;,{x}&#39;&#39; \right ) \mid f\left ( {x}&#39;,y_{j} \geq f\left ( {x}&#39;&#39;,y_{j} \right ),\left ( {x}&#39;,{x}&#39;&#39; \right ) \in \mathcal{Z}_{j} \times \bar{\mathcal{Z}_{j}} \right ) \right \} \right |}{\left | \mathcal{Z}_{j} \right |\left | \bar{\mathcal{Z}_{j}} \right |}
\end{align}\]</span></p>
<p>其中，<span class="math inline">\(\mathcal{Z}_{j} = \left \{ x_{i} \mid y_{j} \in Y_{i}, 1 \leq i \leq p \right \}\left ( \bar{\mathcal{Z}_{j}} = \left \{ x_{i} \mid y_{j} \notin Y_{i}, 1 \leq i \leq p \right \} \right )\)</span></p>
<p>此外，还可以获得微平均AUC。</p>
<p><span class="math display">\[\begin{align}
    AUC_{micro}=\frac{\left | \left \{ \left ( {x}&#39;,{x}&#39;&#39;,{y}&#39;,{y}&#39;&#39; \right ) \mid f\left ( {x}&#39;,{y}&#39; \right ) \geq f\left ( {x}&#39;&#39;,{y}&#39;&#39; \right ),\left ( {x}&#39;,{y}&#39; \right )\in \mathcal{S}^{+},\left ( {x}&#39;&#39;,{y}&#39;&#39; \right ) \in \mathcal{S}^{-} \right \} \right |}{\left | \mathcal{S}^{+} \right |\left | \mathcal{S}^{-} \right |}
\end{align}\]</span></p>
<p>不论是微平均AUC还是宏平均AUC，都是值越大，系统的性能越好，最优值为1。</p>
<h3 id="理论结果">理论结果</h3>
<p>目前的多标签学习算法都在一个方面进行了优化，并且有研究表明如果只是单纯的提高子集的精度（Subset Accuracy），并且使用汉明损失（Hamming Loss）的话，系统的性能反而更差，因此需要多标签学习算法的性能应该在更为广泛的度量范围内进行测试，而不是仅在优化的算法上进行测试。</p>
<p>由于多标签指标通常是非凸的和不连续的，因此在实践中，大多数学习算法都会使用一些替代方法来优化或者替代多标签指标。最近，研究了多标签学习的一致性，即一个分类器的损失函数是否是否会随着训练集大小的增加而收敛到贝叶斯损失，并且还提出了一种在<span class="math inline">\(\mathcal{X} \times 2^{\mathcal{Y}}\)</span>固定分布上，基于代理损失函数的多标签学习一致性的充分必要条件，即具有最优代理损失函数的分类器集必须落入能够产生最优原始多标签损失函数的分类器集中。</p>
<blockquote>
<p>代理损失函数：即当目标函数非凸、不连续时，数学性质不好，优化起来比较复杂，这时候需要使用其他的性能较好的函数进行替换</p>
</blockquote>
<blockquote>
<p>贝叶斯损失（Bayes loss）：</p>
</blockquote>
<p>通过关注排序损失（ranking loss）可以发现标签对所定义的非双向凸代替损失与排序损失（ranking loss）一致，并且一些近期的多标签学习方法与传统确定的多标签学习方法得到的结果不一致。对于这个负面结果，在最小化排序损失（ranking loss）中得到了与多标签学习具有一致性的补充结果。即通过简化双向排序问题，使得在单个标签上定义具有简单变量的凸代理函数，从而使排序损失（ranking loss）的遗憾界和收敛速度具有一致性。</p>
<blockquote>
<p>遗憾界（regret bounds）：</p>
</blockquote>
<h1 id="学习算法">学习算法</h1>
<h2 id="算法分类">算法分类</h2>
<p>本论文选择八个主要的算法，这八个算法首先具有普适性，即每个算法都有其独特的特点；其次具有原始的影响，是大多数后续算法的基础；最后是具有较高的影响力，这些算法都在多标签学习领域中被大量引用。多标签学习领域中可以把算法大致分为两大类，分别是：</p>
<p>1、问题转换的方法：即核心是把数据拟合到某一个算法上。这类方法是把多标签学习的问题转化为已有的学习方案上。其中代表算法首先是一阶的二元关联方法，其次是把多标签学习转化为标签排序的二阶标签排序校准方法，最后是把多标签学习转化为二元分类问题的高阶分类链方法和把多标签学习转化为多类分类任务的高阶随机k-labelsets方法。</p>
<p>2、算法适配的方法：即核心是把现有的某一个算法拟合到数据上。该类方法是使用目前流行的学习技术来适配多标签学习的问题，从而处理多标签数据。其中代表算法首先是通过适配惰性学习技术的一阶ML-KNN方法和适配了决策树的一阶ML-DT方法，其次是适配了核技术的二阶Rank-SVM方法和适配了信息论技术的二阶CML方法。</p>
<img src="/review-multilabel-alg/multi-label-learning-algorithms.png" class title="多标签学习算法">
<h2 id="问题转换的方法">问题转换的方法</h2>
<h3 id="二元关联">二元关联</h3>
<p>该算法的基本思想是把多标签学习问题转化为<span class="math inline">\(q\)</span>个不相关的二元分类问题，即每个标签的二元分类问题都可能是实例<span class="math inline">\(x_{i}\)</span>上的标签。例如对于第<span class="math inline">\(j\)</span>个标签<span class="math inline">\(y_{j}\)</span>，那么本算法首先考虑标签<span class="math inline">\(y_{j}\)</span>是否属于实例<span class="math inline">\(x_{i}\)</span>的相关标签集<span class="math inline">\(Y_{i}\)</span>来构建相应的二元训练集<span class="math inline">\(\mathcal{D}_{j}\)</span>。</p>
<p><span class="math display">\[\begin{align}
    \mathcal{D}_{j}=\left \{ \left ( x_{i},\phi \left ( Y_{i}, y_{j} \right ) \right ) \mid 1 \leq i \leq m \right \} \nonumber \\
    where \ \phi \left ( Y_{i}, y_{i} \right )=
    \left\{\begin{matrix}
    +1, &amp; y_{j} \in Y_{i} \\ 
    -1, &amp; otherwise
    \end{matrix}\right.
\end{align}\]</span></p>
<p>然后通过二元学习算法<span class="math inline">\(\mathcal{B}\)</span>构建二元分类器<span class="math inline">\(g_{j} : \mathcal{X} \rightarrow \mathbb{R}\)</span>，即<span class="math inline">\(g_{j} \leftarrow \mathcal{B}\left ( \mathcal{D}_{j} \right )\)</span>。因此，对于任意的多标签训练实例<span class="math inline">\(\left( x_{i}, Y_{i} \right)\)</span>，实例<span class="math inline">\(x_{i}\)</span>都将参与<span class="math inline">\(q\)</span>个二元分类器的学习过程。对于相关标签<span class="math inline">\(y_{j} \in Y_{i}\)</span>来说，<span class="math inline">\(x_{i}\)</span>是作为分类器<span class="math inline">\(g_{j}\left ( \cdot \right )\)</span>的一个正例；此外，对于不相关标签<span class="math inline">\(y_{k} \in \bar{Y_{i}}\)</span>来说，<span class="math inline">\(x_{i}\)</span>是作为分类器<span class="math inline">\(g_{j}\left ( \cdot \right )\)</span>的一个负例。这种训练策略被称为交叉训练。</p>
<p>对于未知实例<span class="math inline">\(x\)</span>来说，二元关联预测是通过查询每个独立二元分类器，并结合其他相关标签来构建实例<span class="math inline">\(x\)</span>的相关标签集<span class="math inline">\(Y\)</span>。</p>
<p><span class="math display">\[\begin{align}
    Y = \left \{ y_{j} \mid g_{j}\left ( x \right ) &gt; 0, 1 \leq j \leq q \right \}
\end{align}\]</span></p>
<p>注意，当所有的二元分类器都输出负例时，那么预测得到的标签集<span class="math inline">\(Y\)</span>将为空。因此，为了避免产生预测标签集<span class="math inline">\(Y\)</span>为空，就需要引入T-Criterion规则。该准则通过包含具有最大输出的标签实例或者具有最少负例的实例来对上式进行补充。</p>
<p><span class="math display">\[\begin{align}
    Y = \left \{ y_{j} \mid g_{j}\left ( x \right ) &gt; 0, 1 \leq j \leq q \right \} \bigcup \left \{ y_{j^{*}} = j^{*} = argmax_{1 \leq j \leq q} g_{j}\left ( x \right ) \right \}
\end{align}\]</span></p>
<p>评论：二元关联是许多最先进的多标签学习技术的基础模块。此外，由于二元关联忽略了标签之间的潜在管理，并且当标签数量<span class="math inline">\(q\)</span>很大但标签密度（<span class="math inline">\(LDen\left ( \mathcal{D} \right )\)</span>）较低时，二元分类器就会在每个标签上出现类失衡的问题。</p>
<img src="/review-multilabel-alg/binary-relevance.png" class title="二元关联算法">
<h3 id="分类器链">分类器链</h3>
<p>该算法的基本思想是把多标签学习问题转化为一个二元分类问题链，即在分类器链中的子二元分类器建立在上一个子二元分类器预测的基础上。</p>
<p>假如有<span class="math inline">\(q\)</span>个可能的标签类<span class="math inline">\(\left\{ y_{1}, y_{2}, y_{3}, \cdots , y_{q}\right\}\)</span>，并且有一个全排列函数<span class="math inline">\(\tau : \left ( 1, \cdots ,q \right ) \rightarrow \left ( 1, \cdots ,q \right )\)</span>可以对所有标签进行排序，即<span class="math inline">\(y_{\tau \left ( 1 \right )} \succ y_{\tau \left ( 2 \right )} \succ \cdots \succ y_{\tau \left ( q \right )}\)</span>，其中<span class="math inline">\(\tau \left ( j \right )\left(1 \leq j \leq q\right)\)</span>返回排列好的标签号。对于在一个已排列列表的第<span class="math inline">\(j\)</span>个标签<span class="math inline">\(y_{\tau \left ( j \right )} \left(1 \leq j \leq q\right)\)</span>，通过添加与<span class="math inline">\(y_{\tau \left ( j \right )}\)</span>之前所有标签相关的实例来构建一个二元训练集。</p>
<p><span class="math display">\[\begin{align}
    \mathcal{D}_{\tau \left ( j \right )} = \left \{ \left ( \left [ x_{i}, pre_{\tau \left ( j \right )}^{i} \right ], \phi \left ( Y_{i}, y_{\tau \left ( j \right )} \right ) \right ) \mid 1 \leq i \leq m \right \} \nonumber \\
    where \ pre_{\tau \left ( j \right )}^{i} = \left ( \phi \left ( Y_{i}, y_{\tau \left ( 1 \right )} \right ), \cdots , \phi \left ( Y_{i}, y_{\tau \left ( j-1 \right )} \right ) \right )^{T}
\end{align}\]</span></p>
<p>其中，<span class="math inline">\(\left [ x_{i}, pre_{\tau \left ( j \right )}^{i} \right ]\)</span>由<span class="math inline">\(x_{i}\)</span>和<span class="math inline">\(pre_{\tau \left ( j \right )}^{i}\)</span>两个向量组成。其中<span class="math inline">\(pre_{\tau \left ( j \right )}^{i}\)</span>表示已排序标签<span class="math inline">\(y_{\tau \left ( j \right )}\)</span>之前的所有标签与实例<span class="math inline">\(x_{i}\)</span>的二元相关性赋值（<span class="math inline">\(pre_{\tau \left ( 1 \right )}^{i} = \varnothing\)</span>，即<span class="math inline">\(pre_{\tau \left ( 1 \right )}^{i}\)</span>为空集）。</p>
<p>然后，通过一些二元学习算法<span class="math inline">\(\mathcal{B}\)</span>构建标签<span class="math inline">\(y_{\tau \left ( j \right )}\)</span>的二元分类器<span class="math inline">\(g_{\tau \left ( j \right )} : \mathcal{X} \times \left \{ -1, +1 \right \}^{j-1} \rightarrow \mathbb{R}\)</span>，并且来决定<span class="math inline">\(y_{\tau \left ( j \right )}\)</span>是否是<span class="math inline">\(x_{i}\)</span>的相关标签，即<span class="math inline">\(g_{\tau \left ( j \right )} \leftarrow \mathcal{B}\left ( \mathcal{D}_{\tau \left ( j \right )} \right )\)</span>。</p>
<p>对于一个未知的实例<span class="math inline">\(x\)</span>，通过遍历整个分类器链来预测<span class="math inline">\(x\)</span>的相关标签集<span class="math inline">\(Y\)</span>。假设<span class="math inline">\(\lambda_{\tau \left ( j \right )}^{x} \in \left \{ -1, +1 \right \}\)</span>是标签<span class="math inline">\(y_{\tau \left ( j \right )}\)</span>在实例<span class="math inline">\(x_{i}\)</span>上的预测结果，那么器递归推导如下：</p>
<p><span class="math display">\[\begin{align}
    \begin{matrix}
        \lambda_{\tau \left ( 1 \right )}^{x} = sign\left [ g_{\tau \left ( 1 \right )}\left ( x \right ) \right ] \\ 
        \lambda_{\tau \left ( j \right )}^{x} = sign\left [ g_{\tau \left ( 1 \right )}\left ( \left [ x, \lambda_{\tau \left ( 1 \right )}^{x}, \cdots , \lambda_{\tau \left ( j-1 \right )}^{x} \right ] \right ) \right ]\left ( 2 \leq j \leq q \right )
    \end{matrix}
\end{align}\]</span></p>
<p>其中，<span class="math inline">\(sign\left [ \cdot \right ]\)</span>是一个带符号的函数。因此，实例<span class="math inline">\(x\)</span>相应的预测标签集为：</p>
<p><span class="math display">\[\begin{align}
    Y = \left \{ y_{\tau \left ( j \right )} \mid \lambda_{\tau \left ( j \right )}^{x} = +1, 1 \leq j \leq q \right \}
\end{align}\]</span></p>
<p>对于采用分类器链来解决多标签学习的问题来说，全排列函数<span class="math inline">\(\tau\)</span>的顺序对结果有很大影响。由于排序的影响，因此可以在标签集上通过<span class="math inline">\(n\)</span>个随机排列<span class="math inline">\(\tau\)</span>来构建一个分类器链的集合，即<span class="math inline">\(\tau ^{\left ( 1 \right )}, \tau ^{\left ( 2 \right )}, \cdots , \tau ^{\left ( n \right )}\)</span>。对于每一个排列<span class="math inline">\(\tau ^{\left ( r \right )}\left ( 1 \leq r \leq n \right )\)</span>来说，并不是直接通过原始的训练集<span class="math inline">\(\mathcal{D}\)</span>产生一个分类器链，而是通过对训练集<span class="math inline">\(\mathcal{D}\)</span>进行采样（<span class="math inline">\(\left | \mathcal{D}^{\left( r \right)} \right | = 0.67 \cdot \left | \mathcal{D} \right |\)</span>）或者替换（<span class="math inline">\(\left | \mathcal{D}^{\left( r \right)} \right | = \left | \mathcal{D} \right |\)</span>）的方式来得到一个修改后的训练集<span class="math inline">\(\mathcal{D}^{\left( r \right)}\)</span>。</p>
<p>评论：分类器链以随机方式考虑标签之间的相关性。与二进关联相比，分类器链具有利用标签之间相关性的优点，但是同时由于分类器链自身的特性，因此就无法实现并行处理。在训练阶段，分类器链从参考标记拓展了实例空间的特性，即<span class="math inline">\(pre_{\tau \left ( j \right )}^{i}\)</span>。另一种替代扩展特征值为二元值的可行方案是，当算法<span class="math inline">\(\mathcal{B}\)</span>（如朴素贝叶斯）产生的模型能够返回后验概率时，那么扩展特征值就会输出概率，而不是二元值。</p>
<img src="/review-multilabel-alg/classifier-chains.png" class title="分类器链">
<h3 id="校准标签排序">校准标签排序</h3>
<p>该算法的基本思想是把多标签学习问题转化为标签排序问题，即利用成熟的两两标签比较技术实现标签集中标签的排序。</p>
<p>假如有<span class="math inline">\(q\)</span>个可能的标签类<span class="math inline">\(\left\{ y_{1}, y_{2}, y_{3}, \cdots , y_{q}\right\}\)</span>，那么按照两两成对比较就会有总共<span class="math inline">\(\frac{q\left ( q-1 \right )}{2}\)</span>个二元分类器，其中每一对标签为<span class="math inline">\(\left ( y_{j}, y_{k} \right )\left ( 1\leq j\leq k\leq q \right )\)</span>。要对标签<span class="math inline">\(\left ( y_{j}, y_{k} \right )\)</span>进行两两比较，首先通过考虑每个训练实例<span class="math inline">\(x\)</span>与标签对<span class="math inline">\(y_{j}\)</span>和<span class="math inline">\(y_{k}\)</span>之间的相对相关性才能构建一个基于成对标签的二元训练集。</p>
<p><span class="math display">\[\begin{align}
    \mathcal{D}_{jk}=\left \{ \left ( x_{i},\varphi \left ( Y_{i},y_{j},y_{k} \right ) \right ) \mid \phi \left ( Y_{i},y_{j} \right ) \neq \phi \left ( Y_{i},y_{k} \right ),1\leq i\leq m \right \} \nonumber \\
    where \ \varphi \left ( Y_{i}, y_{j}, y_{k} \right ) = \left\{\begin{matrix}
    +1, &amp; if \ \phi \left ( Y_{i}, y_{j} \right ) = +1 \ and \ \phi \left ( Y_{i}, y_{k} \right ) = -1 \\ 
    -1, &amp; if \ \phi \left ( Y_{i}, y_{j} \right ) = -1 \ and \ \phi \left ( Y_{i}, y_{k} \right ) = +1
    \end{matrix}\right.
\end{align}\]</span></p>
<p>也就是说只有当实例<span class="math inline">\(x_{i}\)</span>与标签<span class="math inline">\(y_{j}\)</span>和标签<span class="math inline">\(y_{k}\)</span>具有上面的显著关系时才会被包含在训练集<span class="math inline">\(\mathcal{D}_{jk}\)</span>中。然后通过一些二元分类算法<span class="math inline">\(\mathcal{B}\)</span>构建一个标签<span class="math inline">\(\left ( y_{j}, y_{k} \right )\)</span>的二元分类器<span class="math inline">\(g_{jk} : \mathcal{X} \rightarrow \mathbb{R}\)</span>，即<span class="math inline">\(g_{jk} \leftarrow \mathcal{B}\left ( \mathcal{D}_{jk} \right )\)</span>。因此对于任意的多标签训练实例<span class="math inline">\(\left ( x_{i}, Y_{i} \right )\)</span>来说，实例<span class="math inline">\(x_{i}\)</span>都将参与到<span class="math inline">\(\left | Y_{i} \right |\left | \bar{Y_{i}} \right |\)</span>二元分类器的学习过程中。对于任意<span class="math inline">\(x \in \mathcal{X}\)</span>的实例，当<span class="math inline">\(g_{jk}\left(x\right) &gt; 0\)</span>时，那么学习系统会投票给<span class="math inline">\(y_{j}\)</span>，否则就投票给<span class="math inline">\(y_{k}\)</span>。</p>
<p>对于一个未知的实例<span class="math inline">\(x\)</span>，校准标签排序算法首先把实例<span class="math inline">\(x\)</span>反馈到<span class="math inline">\(\frac{q\left ( q-1 \right )}{2}\)</span>个已经训练好的二元分类器上，以此来获得所有可能标签的投票结果。</p>
<p><span class="math display">\[\begin{align}
    \zeta \left ( x,y_{j} \right )=\sum_{k=1}^{j-1}[\![ g_{kj}\left ( x \right ) \leq 0 ]\!] + \sum_{k=j+1}^{q}[\![ g_{kj}\left ( x \right ) &gt; 0 ]\!] \ \left ( 1 \leq j \leq q \right )
\end{align}\]</span></p>
<p>根据上面的定义，可以得到<span class="math inline">\(\sum_{j=1}^{q}\zeta \left ( x, y_{i} \right ) = \frac{q\left ( q-1 \right )}{2}\)</span>，并且<span class="math inline">\(\mathcal{Y}\)</span>中的所有标签都可以根据各自的投票进行重新排序（即关系被任意打破）。</p>
<p>因此，还需要使用阈值函数来将排好序的标签列表进行重新分类，即分为相关标签和不相关标签。而对于使用标签对方式的校准标签排序算法而言，需要在每个多标签训练实例<span class="math inline">\(\left ( x_{i},Y_{i} \right )\)</span>中引入了一个虚拟标签<span class="math inline">\(y_{V}\)</span>。因此虚拟标签<span class="math inline">\(y_{V}\)</span>就是区分<span class="math inline">\(x_{i}\)</span>的相关标签和不相关标签的一个分割点。换句话说，比<span class="math inline">\(y_{V}\)</span>排名高的就是相关标签<span class="math inline">\(y_{j} \in Y_{i}\)</span>，而比<span class="math inline">\(y_{V}\)</span>排名低的就是不相关标签<span class="math inline">\(y_{k} \in \bar{Y_{i}}\)</span>。</p>
<p>除了原始的<span class="math inline">\(\frac{q\left ( q-1 \right )}{2}\)</span>个二元分类器外，对于每个新标签对<span class="math inline">\(\left ( y_{j},y_{V} \right )\left ( 1 \leq j \leq q \right )\)</span>来言，又会有<span class="math inline">\(q\)</span>个辅助二元分类器。这个二元训练集将会以以下方式进行构建：</p>
<p><span class="math display">\[\begin{align}
    \mathcal{D}_{jV}=\left \{ \left ( x_{i}, \varphi \left ( Y_{i}, y_{j}, y_{V} \right ) \right ) \mid 1 \leq i \leq m\right \} \nonumber \\
    where \ \varphi \left ( Y_{i}, y_{j}, y_{V} \right ) = \left\{\begin{matrix}
        +1, &amp; if \ y_{j} \in Y_{i} \\ 
        -1, &amp; otherwise
        \end{matrix}\right.
\end{align}\]</span></p>
<p>基于新添加二元训练集，通过一个二元分类算法<span class="math inline">\(\mathcal{B}\)</span>构建了一个虚拟标签的二元分类器<span class="math inline">\(g_{jV} : \mathcal{X} \rightarrow \mathbb{R}\)</span>，即<span class="math inline">\(g_{jV} \leftarrow \mathcal{B}\left ( \mathcal{D}_{jV} \right )\)</span>。然后通过引入新的分类器来更新投票函数<span class="math inline">\(\zeta \left ( x,y_{j} \right )\)</span>，具体如下：</p>
<p><span class="math display">\[\begin{align}
    \zeta^{*} \left ( x,y_{j} \right ) = \zeta \left ( x,y_{j} \right ) + [\![ g_{jV}\left ( x \right ) &gt; 0 ]\!] \left ( 1 \leq j \leq q \right )
\end{align}\]</span></p>
<p>此外，虚拟标签的总投票数可以计算为：</p>
<p><span class="math display">\[\begin{align}
    \zeta^{*} \left ( x,y_{j} \right ) = \sum_{j=1}^{q}[\![ g_{jV}\left ( x \right ) \leq 0 ]\!]
\end{align}\]</span></p>
<p>因此，对于未知实例<span class="math inline">\(x\)</span>的预测标签集可以表示为：</p>
<p><span class="math display">\[\begin{align}
    Y = \left \{ y_{j} \mid \zeta^{*} \left ( x,y_{j} \right ) &gt; \zeta^{*} \left ( x,y_{V} \right ) , 1 \leq j \leq q \right \}
\end{align}\]</span></p>
<p>比较二元关联算法和校准标签排序算法中的训练集创建，可以看出两者的训练集产生相同。因此，校准标签排序算法可以被认为是标签之间两两比较算法的扩展版，即是为了便于学习而把<span class="math inline">\(q\)</span>个二元关联分类器扩展得到<span class="math inline">\(\frac{q\left ( q-1 \right )}{2}\)</span>个二元分类器。</p>
<p>评论：校准标签排序算法是通过两两标签构建分类器的二阶方法。相较于以往一对多算法构建的二元分类器相比，校准标签排序以一对一方式构建二元分类器（除了虚拟标签），具有减轻因标签类不平衡造成的问题的优点。此外，通过校准标签排序算法生成的分类器的数量从原先的随着标签<span class="math inline">\(q\)</span>数量成线性增长变为了随着标签<span class="math inline">\(q\)</span>数量成二次增长。因此，校准标签排序的优化和改进主要集中在通过精确裁剪或近似裁剪从而减少测试阶段的分类器数量，例如通过底层二元学习算法<span class="math inline">\(\mathcal{B}\)</span>的特性（感知机的对偶形式等）可以在训练阶段更为高效的获取二次数量的感知机。</p>
<blockquote>
<p>感知机的对偶形式：</p>
</blockquote>
<img src="/review-multilabel-alg/calibrated-label-ranking.png" class title="校准标签排序">
<h3 id="random-k-labelsets">Random k-Labelsets</h3>
<p>该算法的基本思想是把多标签学习问题转换为多类分类问题的集合，即集合中的每个学习器组件都会针对标签空间<span class="math inline">\(\mathcal{Y}\)</span>中的某个随机子集使用LP（Label Powerset）技术实现一个多类分类器。LP技术是把多标签学习问题转化为多类（单标签）分类问题最直接的方法。假设<span class="math inline">\(\sigma_{\mathcal{Y}}:2^{\mathcal{Y}} \rightarrow \mathbb{N}\)</span>是<span class="math inline">\(\mathcal{Y}\)</span>的幂集映射到自然数的映射函数，而<span class="math inline">\(\sigma_{\mathcal{Y}}^{-1}\)</span>是逆函数。在训练阶段，LP首先把原始的多标签训练集<span class="math inline">\(\mathcal{D}\)</span>中的每个不同标签作为一个新类，从而把在<span class="math inline">\(\mathcal{D}\)</span>转化为多类训练集。</p>
<p><span class="math display">\[\begin{align}
    \mathcal{D}_{\mathcal{Y}}^{\dagger}=\left \{ \left ( x_{i}, \sigma_{\mathcal{Y}}\left ( Y_{i} \right ) \right ) \mid 1\leq i \leq m \right \}
\end{align}\]</span></p>
<p>因此，<span class="math inline">\(\mathcal{D}_{\mathcal{Y}}^{\dagger}\)</span>的新的多类标签集合为：</p>
<p><span class="math display">\[\begin{align}
    \Gamma \left ( \mathcal{D}_{\mathcal{Y}}^{\dagger} \right )=\left \{ \sigma_{\mathcal{Y}} \left ( Y_{i} \right ) \mid 1 \leq i \leq m \right \}
\end{align}\]</span></p>
<p>显然，<span class="math inline">\(\left | \Gamma \left ( \mathcal{D}_{\mathcal{Y}}^{\dagger} \right ) \right | \leq min\left ( m, 2^{\left | \mathcal{Y} \right |} \right )\)</span>，即新的多类标签集一定小于等于训练集中实例的数量或者标签集全排列后的数量中最小的那个。然后，通过一些多类学习算法<span class="math inline">\(\mathcal{M}\)</span>构建一个多类分类器<span class="math inline">\(g_{\mathcal{Y}}^{\dagger }:\mathcal{X}\rightarrow \Gamma \left ( \mathcal{D}_{\dagger }^{\mathcal{Y}} \right )\)</span>，即<span class="math inline">\(g_{\mathcal{Y}}^{\dagger } \leftarrow \mathcal{M} \left ( \mathcal{D}_{\dagger }^{\mathcal{Y}} \right )\)</span>。因此，对于任意多标签训练实例<span class="math inline">\(\left ( x_{i}, Y_{i} \right )\)</span>，实例<span class="math inline">\(x_{i}\)</span>将会被重新分配一个已经映射好的新的单标签<span class="math inline">\(\sigma_{\mathcal{Y}}\left ( Y_{i} \right )\)</span>，并参与到后续的多类分类器中。</p>
<p>对于未知的实例<span class="math inline">\(x\)</span>，LP要预测其相关标签集<span class="math inline">\(Y\)</span>，那么首先就要查询多类分类器的预测，然后把结果通过逆函数得到相关标签集<span class="math inline">\(Y\)</span>的幂集。</p>
<p><span class="math display">\[\begin{align}
    Y=\sigma_{\mathcal{Y}}^{-1}\left ( g_{\mathcal{Y}}^{\dagger} \left ( x \right ) \right )
\end{align}\]</span></p>
<p>不幸的是，LP在可行性方面有两个主要的缺点，分别是：</p>
<p>1、不完备性：根据上式可以知道，LP预测的标签集只能是训练集中的标签集，即训练集外的标签集就不能进行泛化<span class="math inline">\(\left \{ Y_{i} \mid 1 \leq i \leq m\right \}\)</span>。</p>
<p>2、效率较低：当<span class="math inline">\(\mathcal{Y}\)</span>非常大时，就会在<span class="math inline">\(\Gamma \left ( \mathcal{D}_{\mathcal{Y}}^{\dagger} \right )\)</span>有非常多的新的标签类，导致在训练分类器<span class="math inline">\(g_{\mathcal{Y}}^{\dagger } \left(\cdot \right)\)</span>是非常的复杂，并且一些新产生的标签类其对应的实例有非常少。</p>
<p>因此，为了保持LP的简单性，以及克服LP的两个主要缺点，Random k-Labelsets选择在多标签数据的学习中把集成学习与LP相结合。这个方法的核心是仅在随机k-Labelsets（在<span class="math inline">\(\mathcal{Y}\)</span>标签集中大小为<span class="math inline">\(k\)</span>的子集）中使用LP，从而保证计算的效率，然后把多个LP分类器进行组合，从而保证了预测的完备性。</p>
<p>假设<span class="math inline">\(\mathcal{Y}^{k}\)</span>表示在标签集<span class="math inline">\(\mathcal{Y}\)</span>中所有可能的k-Labelsets，例如第<span class="math inline">\(l\)</span>个k-Labelsets表示为<span class="math inline">\(\mathcal{Y}^{k}\left(l\right)\)</span>，即<span class="math inline">\(\mathcal{Y}^{k}\left(l\right) \subseteq \mathcal{Y},\left | \mathcal{Y}^{k}\left(l\right) \right | = k, 1 \leq l \leq \binom{q}{k}\)</span>，类似于<span class="math inline">\(\mathcal{D}_{\mathcal{Y}}^{\dagger}\)</span>，一个多类训练集能够通过把原始标签空间<span class="math inline">\(\mathcal{Y}\)</span>缩小为<span class="math inline">\(\mathcal{Y}^{k}\left(l\right)\)</span>进行构建。</p>
<p><span class="math display">\[\begin{align}
    \mathcal{D}_{\mathcal{Y}^{k}\left ( l \right )}^{\dagger}=\left \{ \left ( x_{i}, \sigma_{\mathcal{Y}^{k}\left ( l \right )}\left ( Y_{i} \cap \mathcal{Y}^{k}\left ( l \right ) \right ) \right ) \mid 1 \leq i \leq m \right \}
\end{align}\]</span></p>
<p>因此，新的多类标签集可以通过<span class="math inline">\(\mathcal{D}_{\mathcal{Y}^{k}\left ( l \right )}^{\dagger}\)</span>表示为：</p>
<p><span class="math display">\[\begin{align}
    \Gamma \left ( \mathcal{D}_{\mathcal{Y}^{k}\left ( l \right )}^{\dagger} \right )=\left \{ \sigma_{\mathcal{Y}^{k}\left ( l \right )}\left ( Y_{i} \cap \mathcal{Y}^{k}\left ( l \right ) \right ) \mid 1 \leq i \leq m \right \}
\end{align}\]</span></p>
<p>然后，通过多类学习算法<span class="math inline">\(\mathcal{M}\)</span>构建一个多类分类器<span class="math inline">\(g_{\mathcal{Y}^{k}\left ( l \right )}^{\dagger}:\mathcal{X} \rightarrow \Gamma \left ( \mathcal{D}_{\mathcal{Y}^{k}\left ( l \right )}^{\dagger} \right )\)</span>，即<span class="math inline">\(g_{\mathcal{Y}^{k}\left ( l \right )}^{\dagger}: \mathcal{M} \left ( \mathcal{D}_{\mathcal{Y}^{k}\left ( l \right )}^{\dagger} \right )\)</span>。</p>
<p>为了创建了由<span class="math inline">\(n\)</span>个分类组件的集合，Random k-Labelsets会在<span class="math inline">\(n\)</span>个子标签集<span class="math inline">\(\mathcal{Y}^{k}\left ( l_{r} \right )\left ( 1 \leq r \leq n \right )\)</span>上使用LP，使得每个子标签集都会产生一个多类分类器<span class="math inline">\(g_{\mathcal{Y}^{k}\left ( l \right )}^{\dagger}\left ( \cdot \right )\)</span>。对于未知的实例<span class="math inline">\(x\)</span>，每类标签都需要计算以下两个数量。</p>
<p><span class="math display">\[\begin{align}
    \begin{matrix}
        \tau \left ( x, y_{j} \right ) = \sum_{r=1}^{n}[\![ y_{j} \in \mathcal{Y}^{k}\left ( l_{r} \right ) ]\!] \ \left ( 1 \leq j \leq q \right ) \\ 
        \mu \left ( x, y_{j} \right ) = \sum_{r=1}^{n} [\![ y_{j} \in \sigma_{\mathcal{Y}^{k}\left ( l_{r} \right )}^{-1} \left ( g_{\mathcal{Y}^{k}\left ( l_{r} \right )}^{\dagger } \left ( x \right )\right ) ) ]\!] \ \left ( 1 \leq j \leq q \right )
    \end{matrix}
\end{align}\]</span></p>
<p>其中，<span class="math inline">\(\tau \left ( x, y_{j} \right )\)</span>表示在集合中标签<span class="math inline">\(y_{j}\)</span>可以获取的最大预期投票数，而<span class="math inline">\(\mu \left ( x, y_{j} \right )\)</span>则表示在集合中<span class="math inline">\(y_{j}\)</span>获取的实际投票数。因此，对于标签集的预测就可以等价为：</p>
<p><span class="math display">\[\begin{align}
    Y=\left \{ y_{j} \mid \frac{\mu \left ( x, y_{j} \right )}{\tau \left ( x, y_{j} \right )} &gt; 0.5, 1 \leq j \leq q \right \}
\end{align}\]</span></p>
<p>换句话说，当实际投标数超过了最大投票数的一半时，那么就认为<span class="math inline">\(y_{j}\)</span>标签与实例<span class="math inline">\(x\)</span>相关。对于有<span class="math inline">\(n\)</span>个k-labelsets来说，其每个标签的最大平均投票数为<span class="math inline">\(\frac{nk}{q}\)</span>。在Random k-Labelsets算法的经验，一般<span class="math inline">\(k = 3，n = 2q\)</span>。</p>
<p>评论：Random k-Labelsets算法是一个高阶方法，即标签相关性的维度，由子标签集的大小决定。此外，在使用Random k-Labelsets算法时，可以通过预设数量阈值从而把在标签数量低于阈值的训练集<span class="math inline">\(\mathcal{D}\)</span>裁剪掉。虽然Random k-Labelsets算法通过将集成学习作为其固有的一部分来修正LP的缺点，但集成学习也可以作为一种元级策略，通过包含同质或异质的多标签学习器来促进多标签学习。</p>
<img src="/review-multilabel-alg/random-k-Labelsets.png" class title="Random k-Labelsets">
<h2 id="算法适配的方法">算法适配的方法</h2>
<h3 id="多标签的k最邻近分类算法ml-knn">多标签的K最邻近分类算法（ML-KNN）</h3>
<p>该算法的基本思想是通过适配KNN技术来处理多标签数据，即利用最大后验概率（MAP）准则对邻近标签内的信息进行推理，从而实现对多标签数据的预测。</p>
<p>对于未知实例<span class="math inline">\(x\)</span>，假设<span class="math inline">\(\mathcal{N}\left(x\right)\)</span>表示实例<span class="math inline">\(x\)</span>在训练集<span class="math inline">\(\mathcal{D}\)</span>中的最邻近数据集。通常，实例之间的相似性使用欧氏距离进行度量。对于第<span class="math inline">\(j\)</span>类标签，在ML-KNN选择计算下面的统计信息：</p>
<p><span class="math display">\[\begin{align}
    C_{j}=\sum_{\left ( x^{*},Y^{*} \right ) \in \mathcal{N}\left ( x \right )}[\![ y_{j} \in Y^{*} ]\!]
\end{align}\]</span></p>
<p>其中<span class="math inline">\(C_{j}\)</span>表示与标签<span class="math inline">\(y_{j}\)</span>相关的实例<span class="math inline">\(x\)</span>的领域数量。</p>
<p>假设<span class="math inline">\(H_{j}\)</span>是实例<span class="math inline">\(x\)</span>有标签<span class="math inline">\(y_{j}\)</span>的事件，并且<span class="math inline">\(\mathbb{P}\left ( H_{j} \mid C_{j} \right )\)</span>表示实例<span class="math inline">\(x\)</span>有标签<span class="math inline">\(y_{j}\)</span>，且正好有<span class="math inline">\(C_{j}\)</span>个领域的情况下的后验概率。而<span class="math inline">\(\mathbb{P}\left (\neg H_{j} \mid C_{j} \right )\)</span>则表示实例<span class="math inline">\(x\)</span>没有标签<span class="math inline">\(y_{j}\)</span>，且正好有<span class="math inline">\(C_{j}\)</span>个领域的情况下的后验概率。因此，按照MAP准则，标签集的预测通过判断<span class="math inline">\(\mathbb{P}\left ( H_{j} \mid C_{j} \right )\)</span>是否大于<span class="math inline">\(\mathbb{P}\left (\neg H_{j} \mid C_{j} \right )\)</span>或者不大于。</p>
<p><span class="math display">\[\begin{align}
    Y = \left \{ y_{j} \mid \frac{\mathbb{P}\left (H_{j} \mid C_{j} \right )}{\mathbb{P}\left (\neg H_{j} \mid C_{j} \right )} &gt; 1, 1 \leq j \leq q\right \}
\end{align}\]</span></p>
<blockquote>
<ul>
<li>1、KNN的说明：即计算实例<span class="math inline">\(x\)</span>和所有训练实例的距离，然后进行排序，取前k个实例，这k个实例的多数属于某个类，实例<span class="math inline">\(x\)</span>就归属于哪个类。虽然KNN简单，但由于要和所有的训练实例计算距离，因此当训练集特别大的时候，这种方式非常耗时，可以采用KD树（K-Dimensional Tree）的方式来减少输入实例和训练实例的计算次数从而优化性能。</li>
<li>2、KD树（K-Dimensional Tree）：以树型方式对<span class="math inline">\(N\)</span>维空间的实例进行存储，以便对其进行快速检索。KD树的创建相当于用不断垂直于坐标轴的超平面将<span class="math inline">\(N\)</span>维空间进行划分，构成一系列的<span class="math inline">\(N\)</span>维超矩阵区域。</li>
<li>3、KD树的生成：首先从<span class="math inline">\(m\)</span>个样本的<span class="math inline">\(N\)</span>维特征中分别计算<span class="math inline">\(N\)</span>个特征取值的方差，用方差最大的第<span class="math inline">\(k\)</span>维特征<span class="math inline">\(n_{k}\)</span>来作为根节点，然后选择特征<span class="math inline">\(n_{k}\)</span>的中间值<span class="math inline">\(n_{kv}\)</span>作为样本的划分点，对于所有第<span class="math inline">\(k\)</span>维特征的实例来说，其值小于<span class="math inline">\(n_{kv}\)</span>时，则该实例划入左分支，对取值大于等于<span class="math inline">\(n_{kv}\)</span>的样本，则划入右分支。然后再对左子树和右子树，采用相同的方法在分支中找到方差最大的特征作为当前分支的根节点，最后递归的生成KD树。</li>
<li>4、KD树的搜索：首先找到包含预测目标点的子节点，以该节点为中心，以目标前到该节点的距离为半径，得到一个超球体，从KNN的原理可以知道最近邻的点一定在该超球体内部。其次，返回该节点的父节点，检查另一个子节点所包含的超矩形是否与球体相交，如果有相交，那么就需要在该节点查询是否有更加近的近邻，如果有就更新最近邻和超球体，如果没有相交就继续返回父节点，在另一个子节点中查找最近邻。最后直到回到了根节点，算法结束，此时保存的最近领就是最终的最近邻。</li>
</ul>
</blockquote>
<img src="/review-multilabel-alg/kd-create.png" class title="KD创建">
<img src="/review-multilabel-alg/kd-structure.png" class title="Random KD结构">
<img src="/review-multilabel-alg/kd-search.png" class title="Random KD搜索">
<blockquote>
<p>欧式距离的说明：即常见的两点之间的距离，对于二维坐标系其距离为：<span class="math inline">\(E\left ( d \right ) = \sqrt{\left ( x_{1} - x_{2} \right )^{2} + \left ( y_{1} - y_{2} \right )^{2}}\)</span>。如果扩展到N维数据，那么两点之间的距离就是各维度数据分别求差后平方，然后再后求和，最后再开平方，即<span class="math inline">\(E\left ( d \right ) = \sqrt{\left ( x_{1} - x_{2} \right )^{2} + \left ( y_{1} - y_{2} \right )^{2} + \left ( z_{1} - z_{2} \right )^{2} + \cdots }\)</span>。</p>
</blockquote>
<p>根据贝叶斯定理，就可以得到：</p>
<p><span class="math display">\[\begin{align}
    \frac{\mathbb{P}\left (H_{j} \mid C_{j} \right )}{\mathbb{P}\left (\neg H_{j} \mid C_{j} \right )} = \frac{\mathbb{P}\left ( H_{j} \right ) \cdot \mathbb{P}\left ( C_{j} \mid H_{j} \right )}{\mathbb{P}\left ( \neg H_{j} \right ) \cdot \mathbb{P}\left ( C_{j} \mid \neg H_{j} \right )}
\end{align}\]</span></p>
<p>这里的<span class="math inline">\(\mathbb{P}\left ( H_{j} \right )\)</span>（<span class="math inline">\(\mathbb{P}\left ( \neg H_{j} \right )\)</span>）表示实例<span class="math inline">\(x\)</span>与标签<span class="math inline">\(y_{j}\)</span>具有相关性的事件<span class="math inline">\(H_{j}\)</span>（或<span class="math inline">\(x\)</span>与标签<span class="math inline">\(y_{j}\)</span>不相关）的先验概率。此外，<span class="math inline">\(\mathbb{P}\left (C_{j} \mid H_{j} \right )\)</span>（<span class="math inline">\(\mathbb{P}\left (C_{j} \mid \neg H_{j} \right )\)</span>）表示当事件<span class="math inline">\(H_{j}\)</span>成立（不成立）的情况下，实例<span class="math inline">\(x\)</span>在标签<span class="math inline">\(y_{j}\)</span>上有<span class="math inline">\(C_{j}\)</span>个邻近的概率。通过上式就可以实现先验概率的估计和执行预测。</p>
<p>为了完成上面的任务，ML-KNN采用频率计数的策略。首先，通过计算与每个标签关联的训练样本的数量来估计先验概率。</p>
<p><span class="math display">\[\begin{align}
    \mathbb{P}\left ( H_{j} \right )=\frac{s + \sum_{i=1}^{m}[\![ y_{j} \in Y_{i} ]\!]}{s \times 2 + m}; \mathbb{P}\left ( \neg H_{j} \right ) = 1 -  \mathbb{P}\left ( H_{j} \right )\ \left ( 1 \leq j \leq q \right )
\end{align}\]</span></p>
<p>其中，<span class="math inline">\(s\)</span>是用于控制平均先验概率对估计影响的平滑参数。在拉普拉斯平滑中，该值一般取值为1。</p>
<blockquote>
<p>拉普拉斯平滑的说明：通过增加平滑参数<span class="math inline">\(s\)</span>解决0概率问题。</p>
</blockquote>
<p>其次，就是对似然性的估计。对于第<span class="math inline">\(j\)</span>个标签<span class="math inline">\(y_{j}\)</span>，ML-KNN维护了两个频率数组<span class="math inline">\(\mathcal{K}_{j}\)</span>和<span class="math inline">\(\tilde{\mathcal{K}}_{j}\)</span>，每个数组都包含了<span class="math inline">\(k+1\)</span>个元素：</p>
<blockquote>
<p>最大似然的说明：似然和概率的区别在于，概率是知道事件发生的概率推算结果，而似然是根据结果推算事件发生的概率，即概率是参数推断结果的过程，而似然是结果推断参数分布的过程。延生到机器学习，就可以知道机器学习也是通过结果进行学习，然后求最大似然，最后得到未知变量的预测结果</p>
</blockquote>
<p><span class="math display">\[\begin{align}
    \begin{matrix}
        \mathcal{K}_{j}\left [ r \right ] = \sum_{i=1}^{m}[\![ y_{i} \in Y_{i} ]\!] \cdot [\![ \delta_{j}\left ( x_{i} \right ) = r ]\!] \ \left ( 0 \leq r \leq k \right ) \\ 
        \tilde{\mathcal{K}_{j}}\left [ r \right ] = \sum_{i=1}^{m}[\![ y_{i} \notin Y_{i} ]\!] \cdot [\![ \delta_{j}\left ( x_{i} \right ) = r ]\!] \ \left ( 0 \leq r \leq k \right ) \\ 
        where \ \delta_{j}\left ( x_{i} \right ) = \sum_{\left ( x^{*}, Y^{*} \right ) \in \mathcal{N}\left ( x_{i} \right )}[\![ y_{j} \in Y^{*} ]\!]
    \end{matrix}
\end{align}\]</span></p>
<p>其中，<span class="math inline">\(\delta_{j}\left ( x_{i} \right )\)</span>记录了与标签<span class="math inline">\(y_{j}\)</span>相关的实例<span class="math inline">\(x_{i}\)</span>邻域的数量。因此，<span class="math inline">\(\mathcal{K}_{j}\left [ r \right ]\)</span>保存了与标签<span class="math inline">\(y_{j}\)</span>相关，并且有<span class="math inline">\(r\)</span>个邻域的训练实例的数量。同样，<span class="math inline">\(\tilde{\mathcal{K}_{j}}\left [ r \right ]\)</span>保存了与标签<span class="math inline">\(y_{j}\)</span>不相关，但有<span class="math inline">\(r\)</span>个邻域的训练实例的数量。随后，似然性就可以通过<span class="math inline">\(\mathcal{K}_{j}\left [ r \right ]\)</span>和<span class="math inline">\(\tilde{\mathcal{K}_{j}}\left [ r \right ]\)</span>这两个数组进行估算。</p>
<p><span class="math display">\[\begin{align}
    \begin{matrix}
        \mathbb{P}\left ( C_{j} \mid H_{j} \right ) = \frac{s + \mathcal{K}_{j}\left [ C_{j} \right ]}{s \times \left ( k+1 \right ) + \sum_{r=0}^{k}\mathcal{K}_{j}\left [ r \right ] } \ \left ( 1 \leq j \leq q,0 \leq C_{j} \leq k \right )\\ 
        \mathbb{P}\left ( C_{j} \mid \neg H_{j} \right ) = \frac{s + \tilde{\mathcal{K}_{j}}\left [ C_{j} \right ]}{s \times \left ( k+1 \right ) + \sum_{r=0}^{k}\tilde{\mathcal{K}_{j}}\left [ r \right ] } \ \left ( 1 \leq j \leq q,0 \leq C_{j} \leq k \right )
    \end{matrix}
\end{align}\]</span></p>
<p>因此，有了先验概率和似然性就可以得到对标签集进行预测。</p>
<p>评价：ML-KNN算法是一个通过单独推理每个标签相关性的一阶算法。ML-KNN继承了惰性学习和贝叶斯推理的优点，具体如下：</p>
<p>1、决策边界可以自适应地调整，用于应对识别每个未知实例的不同领域。</p>
<p>2、由于估算了每类标签的先验概率，因此大大减轻了类别失衡问题。</p>
<p>其实还有其他使用惰性学习来处理多标签数据，如集合了KNN和聚合排名，通过识别一个特定标签来把KNN扩展到整个训练集。由于ML-KNN忽略了标签之间的相关性，因此已经提出了一些方法把标签之间的关系加入到ML-KNN中。</p>
<img src="/review-multilabel-alg/ml-knn.png" class title="ML-kNN">
<h3 id="多标签决策树ml-dt">多标签决策树（ML-DT）}</h3>
<p>该算法的基本思想是通过适配决策树技术来处理多标签数据，即利用多标签熵的信息增益准则来递归构建决策树。</p>
<blockquote>
<ul>
<li>1、决策树的说明：决策树是一种基本的分类与回归方法。决策树的产生通常分为两个部分，分别是构造和剪枝。</li>
<li>2、决策树的构造：即就是对更节点、分支和叶片的选择，并且节点和节点之间存在父子关系。其中要特别注意的是叶片是最底部的节点，是决策的结果。</li>
<li>3、决策树的剪枝：解决过拟合的问题。其中过拟合表示模型训练的太好，使得模型过于死板，分类条件过于严格。欠拟合表示模型训练的效果不理想。剪枝的方法分为两种，分别是预剪枝和后剪枝。</li>
<li>4、预剪枝：在决策树构造时就进行剪枝。即在构造的过程中对节点进行评估，如果对某个分支节点在验证集中不能带来准确性的提升，那么对这个分支节点的划分就没有意义，这时就会把当前分支节点作为叶节点。</li>
<li>5、后剪枝：在生成决策树之后再进行剪枝。通常会从决策树的叶节点开始，逐层向上对每个节点进行评估。如果该节点存在与否对节点子树在分类准确性上差别影响不大，或者剪掉该节点子树，能在验证集中带来准确性的提升，那么就可以把该节点子树进行剪枝。使用这个节点子树的叶片节点来替代，类标记为这个节点子树中最频繁的那个类。</li>
<li>6、在决策树中根节点的选择由两个指标组成：纯度和信息熵。</li>
<li>7、纯度：让目标变量的分歧最小。</li>
<li>8、信息熵：表示信息的不确定性<span class="math inline">\(Entropy\left ( t \right ) = -\sum_{i=0}^{c-1}p\left ( i \mid t \right )\log_{2}p\left ( i \mid t \right )\)</span>。其中<span class="math inline">\(p\left ( i \mid t \right )\)</span>表示节点<span class="math inline">\(t\)</span>为分类<span class="math inline">\(i\)</span>的概率。当不确定性越大时，它所包含的信息量也就越大，信息熵也就越高。并且当信息熵越大，纯度越低，而如果集合中的所有样本均匀混合时，信息熵最大，纯度最低。</li>
</ul>
</blockquote>
<blockquote>
<p>构建决策树时通常会采用纯度来构建，而常用的“不纯度”指标有三种，也对应了三种算法，分别是信息增益（ID3 算法）、信息增益率（C4.5算法）以及基尼指数（Cart 算法）。</p>
</blockquote>
<blockquote>
<ul>
<li>1、信息增益（ID3 算法）：即将信息熵最大的作为每个分支的父节点的属性，该方法可以提高分支的纯度，并降低信息熵。计算方法是父节点的信息熵减去所有子节点信息熵和。在具体计算时计算每个子节点的归一化信息熵，即按照每个子节点在父节点中出现的概率。</li>
</ul>
</blockquote>
<p><span class="math display">\[\begin{align}
    Gain\left (D, a \right )=Entropy\left ( D \right ) - \sum_{i=1}^{k}\frac{\left | D_{i} \right |}{\left | D \right |}Entropy\left ( D_{i} \right )
\end{align}\]</span></p>
<blockquote>
<p>其中<span class="math inline">\(D\)</span>表示父节点，<span class="math inline">\(D_{i}\)</span>表示子节点，<span class="math inline">\(Gain\left ( D, a \right )\)</span>中的<span class="math inline">\(a\)</span>表示选择的属性。ID3算法的缺陷是该算法倾向于选择取值种类比较多的属性，即有些属性可能对分类任务没有太大作用，但是他们仍然可能会被选为最优属性。例如极端情况是如果把“编号”用做一个可选属性，那么ID3就会把编号作为最有属性，但是实际上该属性是无关属性。</p>
</blockquote>
<blockquote>
<ul>
<li>2、信息增益率（C4.5算法）：为了避免ID3算法的问题，C4.5算法使用信息增益率作为属性原则的依据，即当属性有很多值的时候，相当于被划分成了许多份，虽然信息增益变大了，但是对于 C4.5 来说，属性熵也会变大，所以整体的信息增益率并不大。</li>
<li>3、悲观剪枝（PEP）：为了解决ID3容易造成过拟合的问题，在构建完成决策树后采用悲观剪枝的方法来提升决策树的泛化能力。该方法是后剪枝技术中的一种，通过递归估算每个内部节点的分类错误率，比较剪枝前后这个节点的分类错误率来决定是否对其进行剪枝，并且这种剪枝方法不需要单独的测试数据集。</li>
<li>4、离散化处理连续属性：C4.5还可以对连续属性进行离散化处理。其方法是选择具有最高信息增益所对应的阈值。</li>
</ul>
</blockquote>
<img src="/review-multilabel-alg/decision-tree.png" class title="决策树">
<p>假设有<span class="math inline">\(n\)</span>个实例的多标签数据集<span class="math inline">\(\mathcal{T} = \left \{ \left ( x_{i}, Y_{i} \right ) \mid 1 \leq i \leq n \right \}\)</span>，通过沿着分割值<span class="math inline">\(\vartheta\)</span>的第<span class="math inline">\(l\)</span>个特征对数据集<span class="math inline">\(\mathcal{T}\)</span>进行划分，可以得到信息增益为：</p>
<p><span class="math display">\[\begin{align}
    \begin{matrix}
        IG\left ( \mathcal{T} , l, \vartheta \right ) = MLEnt\left ( \mathcal{T}  \right ) - \sum \frac{\left | \mathcal{T}  \right |}{\left | \mathcal{T} ^{\rho } \right |}\cdot MLEnt\left ( \mathcal{T} ^{\rho } \right )\left ( \rho \in \left \{ -,+ \right \} \right )\\ 
        where\ \mathcal{T} ^{-}=\left \{ \left ( x_{i},Y_{i} \right ) \mid x_{il} \leq \vartheta , 1 \leq i \leq n \right \}, \ \mathcal{T} ^{+}=\left \{ \left ( x_{i},Y_{i} \right ) \mid x_{il} &gt; \vartheta , 1 \leq i \leq n \right \}
    \end{matrix}
\end{align}\]</span></p>
<p>其中，<span class="math inline">\(\mathcal{T} ^{-}\left(\mathcal{T} ^{+}\right)\)</span>表示在第<span class="math inline">\(l\)</span>个特征上值小于（大于）<span class="math inline">\(\vartheta\)</span>的所有实例。</p>
<p>从根节点（即<span class="math inline">\(\mathcal{T} = \mathcal{D}\)</span>）开始，ML-DT通过识别特征以及根据上式获取的最大信息增益来确认相应的分割点，然后生成关于<span class="math inline">\(\mathcal{T} ^{-}\)</span>和<span class="math inline">\(\mathcal{T} ^{+}\)</span>的两个子节点。通过将<span class="math inline">\(\mathcal{T} ^{-}\)</span>或<span class="math inline">\(\mathcal{T} ^{+}\)</span>作为新的根节点，递归地调用上述过程，直到满足一些停止准则<span class="math inline">\(\mathcal{C}\)</span>才不进行上述过程（即子节点的大小小于预设的阈值）。</p>
<p>要实例化ML-DT，那么就需要有计算多标签熵（即<span class="math inline">\(MLEnt\left ( \cdot \right )\)</span>）的方法。一个简单的方法是把每个相关子标签集<span class="math inline">\(Y \in \mathcal{Y}\)</span>作为一个新类，然后使用传统的单标签熵来计算多标签熵：</p>
<p><span class="math display">\[\begin{align}
    \begin{matrix}
        \widehat{MLEnt\left ( \mathcal{T} \right )}=-\sum \mathbb{P}\left ( Y \right )\cdot \log_{2}\left ( \mathbb{P}\left ( Y \right ) \right )\left ( Y \subseteq \mathcal{Y} \right )\\ 
        where \ \mathbb{P}\left ( Y \right )=\frac{\sum_{i=1}^{n}\left \| Y_{i} = Y \right \|}{n}
    \end{matrix}
\end{align}\]</span></p>
<p>然而，由于新类数量的增长与<span class="math inline">\(\left | \mathcal{Y} \right |\)</span>的空间大小相关，并且呈指数增长，但其中许多新类甚至不会出现在<span class="math inline">\(\mathcal{T}\)</span>中，因此这些新类就具有可以被忽略的估计概率（即<span class="math inline">\(\mathbb{P}\left ( Y \right ) = 0\)</span>）。为了避免这个问题，ML-DT假设标签之间是相互独立的，并且使用可分解的方式计算多标签的熵：</p>
<p><span class="math display">\[\begin{align}
    \begin{matrix}
        MLEnt\left ( \mathcal{T} \right ) = \sum_{j=1}^{q} - p_{j}\log_{2}p_{j}-\left ( 1-p_{j} \right )\log_{2}\left ( 1 - p_{j} \right )\\ 
        where \ p_{j} = \frac{\sum_{i=1}^{n}\left \| y_{i} \in Y_{i} \right \|}{n}
    \end{matrix}
\end{align}\]</span></p>
<p>其中，<span class="math inline">\(p_{j}\)</span>表示在<span class="math inline">\(\mathcal{T}\)</span>中与标签<span class="math inline">\(y_{j}\)</span>相关的实例数量的占比。需要注意的是<span class="math inline">\(MLEnt\left ( \mathcal{T} \right )\)</span>可以被认为是在标签相互独立假设下的<span class="math inline">\(\widehat{MLEnt\left ( \mathcal{T} \right )}\)</span>简化版，并且<span class="math inline">\(MLEnt\left ( \mathcal{T} \right ) &gt; \widehat{MLEnt\left ( \mathcal{T} \right )}\)</span>。</p>
<p>对于未知的实例<span class="math inline">\(x\)</span>，它验证对应路径遍历学习到的决策树，直到到达与多个训练实例<span class="math inline">\(\mathcal{T} \subseteq \mathcal{D}\)</span>相关的叶节点。相应的预测标签集为：</p>
<p><span class="math display">\[\begin{align}
    Y=\left \{ y_{j} \mid p_{j} &gt; 0.5, 1 \leq j \leq q \right \}
\end{align}\]</span></p>
<p>换句话说，假如进入某个叶节点的训练实例都与标签<span class="math inline">\(y_{j}\)</span>相关，就认为该叶节点与标签<span class="math inline">\(y_{j}\)</span>相关，那么在该叶节点中的所有测试实例就都与标签<span class="math inline">\(y_{j}\)</span>相关。</p>
<p>评论：ML-DT是在计算多标签熵时，假设标签之间相互独立情况下的一阶方法。ML-DT的一个显著特点是它能够高效的从多标签数据中获得决策树模型。未来，ML-DT可以从剪枝策略或集成学习技术方面进行改进。</p>
<img src="/review-multilabel-alg/ml-dt.png" class title="多标签决策树">
<h3 id="rank-support-vector-machinerank-svm">Rank Support Vector Machine（Rank-SVM）</h3>
<p>该算法的基本思想是使用最大边缘策略来处理多标签数据，即其中一组线性分类器被优化为最小化经验排序损失，并且使用核技巧处理非线性问题。</p>
<blockquote>
<p>SVM的说明：</p>
</blockquote>
<p>假设学习系统由<span class="math inline">\(q\)</span>条线性分类器<span class="math inline">\(\mathcal{W}=\left \{ \left ( \omega_{j}, b_{j} \right ) \mid 1 \leq j \leq q \right \}\)</span>组成，其中<span class="math inline">\(\omega_{j} \in \mathbb{R}^{d}\)</span>和<span class="math inline">\(b_{j} \in \mathbb{R}\)</span>是第<span class="math inline">\(j\)</span>类标签<span class="math inline">\(y_{j}\)</span>的权重向量和偏差。因此，Rank-SVM通过考虑实例在相关和不相关标签上的排序能力来定义学习系统在实例<span class="math inline">\(x_{i}\)</span>的相关标签<span class="math inline">\(Y_{i}\)</span>（即<span class="math inline">\(\left ( x_{i},Y_{i} \right )\)</span>）上的边界。</p>
<p><span class="math display">\[\begin{align}
    \underset{\left ( y_{j}, y_{k} \right ) \in Y_{i} \times \bar{Y_{i}}}{min}\frac{\left \langle \omega_{j}-\omega_{k},x_{i} \right \rangle + b_{j} - b_{k}}{\left \| \omega_{j}-\omega_{k} \right \|}
\end{align}\]</span></p>
<p>其中，<span class="math inline">\(\left \langle u, \upsilon \right \rangle\)</span>返回内积<span class="math inline">\(u^{\top}\upsilon\)</span>。从几何的角度说，每一个相关与不相关的标签对<span class="math inline">\(\left ( y_{j}, y_{k} \right ) \in Y_{i} \times \bar{Y_{i}}\)</span>，它们的判别边界都对应于一个超平面<span class="math inline">\(\left \langle \omega_{j}-\omega_{k},x \right \rangle + b_{j} - b_{k} = 0\)</span>。因此，上式考虑<span class="math inline">\(x_{i}\)</span>到每个相关、不相关标签对超平面距离的<span class="math inline">\(L_{2}\)</span>，并且返回在实例<span class="math inline">\(x_{i}\)</span>的相关标签<span class="math inline">\(Y_{i}\)</span>上的最小边界。因此，在整个训练集<span class="math inline">\(\mathcal{D}\)</span>上的学习系统边界就为：</p>
<p><span class="math display">\[\begin{align}
    \underset{\left ( x_{i}, Y_{i} \right ) \in \mathcal{D}}{min} \ \underset{\left ( y_{j}, y_{k} \right ) \in Y_{i} \times \bar{Y_{i}}}{min}\frac{\left \langle \omega_{j}-\omega_{k},x_{i} \right \rangle + b_{j} - b_{k}}{\left \| \omega_{j}-\omega_{k} \right \|}
\end{align}\]</span></p>
<blockquote>
<p>超平面的说明：一种忽略空间维数的线性函数。</p>
</blockquote>
<p>当学习系统能够为训练集中实例的每个相关和不相关的标签进行正确排序时，上式就会返回正边界。在这种理想状态下，就可以对线性分类器进行重新缩放：</p>
<p>1、<span class="math inline">\(\forall 1 \leq i \leq m \ and \ \left ( y_{i},y_{k} \right ) \in Y_{i} \times \bar{Y_{i}}, \ \left \langle \omega_{j} - \omega_{k}, x_{i} \right \rangle + b_{j} - b_{k} &gt; 1\)</span></p>
<p>2、<span class="math inline">\(\exists i^{*} \in \left \{ 1, \cdots , m \right \} \ and \ \left ( y_{j^{*}},y_{k^{*}} \right ) \in Y_{i^{*}} \times \bar{Y_{i^{*}}}, \ \left \langle \omega_{j^{*}} - \omega_{k^{*}}, x_{i^{*}} \right \rangle + b_{j^{*}} - b_{k^{*}} &gt; 1\)</span></p>
<p>因此，在整个训练集<span class="math inline">\(\mathcal{D}\)</span>上的学习系统的最大边界就可以表示为：</p>
<p><span class="math display">\[\begin{align}
    \begin{matrix}
        \underset{\mathcal{W}}{max} \ \underset{\left ( x_{i}, Y_{i} \right ) \in \mathcal{D}}{min} \ \underset{\left ( y_{j}, y_{k} \right ) \in Y_{i} \times \bar{Y_{i}}}{min} \ \frac{1}{\left \| \omega_{j}-\omega_{k} \right \|^{2}}\\ 
        subject \ to : \left \langle \omega_{j} - \omega_{k}, x_{i} \right \rangle + b_{j} - b_{k} \geq 1 \ \left ( 1 \leq i \leq m, \left ( y_{j}, y_{k} \right ) \in Y_{i} \times \bar{Y_{i}} \right )
    \end{matrix}
\end{align}\]</span></p>
<p>假设有足够的训练样本，因此对于每个标签对<span class="math inline">\(\left ( y_{j}, y_{k} \right )\left ( j \neq k \right )\)</span>，都存在<span class="math inline">\(\left ( x, Y \right ) \in \mathcal{D}\)</span>满足<span class="math inline">\(\left ( y_{j}, y_{k} \right ) \in Y_{i} \times \bar{Y_{i}}\)</span>。因此，上式就等价于<span class="math inline">\(max_{\mathcal{W}} \ min_{1 \leq j \leq k \leq q}\frac{1}{\left \| \omega_{j}-\omega_{k} \right \|^{2}}\)</span>，并且可以把问题优化后写为：</p>
<p><span class="math display">\[\begin{align}
    \begin{matrix}
        \underset{\mathcal{W}}{min} \ \underset{1 \leq j \leq k \leq q}{max} \ \left \| \omega_{j} - \omega_{k} \right \|^{2}\\ 
        subject \ to : \left \langle \omega_{j} - \omega_{k}, x_{i} \right \rangle + b_{j} - b_{k} \geq 1 \ \left ( 1 \leq i \leq m, \left ( y_{j}, y_{k} \right ) \in Y_{i} \times \bar{Y_{i}} \right )
    \end{matrix}
\end{align}\]</span></p>
<p>为了克服<span class="math inline">\(max\)</span>操作带来的困难，Rank-SVM选择<span class="math inline">\(sum\)</span>操作才逼近<span class="math inline">\(max\)</span>来简化上式：</p>
<p><span class="math display">\[\begin{align}
    \begin{matrix}
        \underset{\mathcal{W}}{min} \sum_{i=1}^{q} \left \| \omega_{j} \right \|^{2}\\ 
        subject \ to : \left \langle \omega_{j} - \omega_{k}, x_{i} \right \rangle + b_{j} - b_{k} \geq 1 \ \left ( 1 \leq i \leq m, \left ( y_{j}, y_{k} \right ) \in Y_{i} \times \bar{Y_{i}} \right )
    \end{matrix}
\end{align}\]</span></p>
<p>为了解决上式中的约束在实际情况下不能得到满足，因此在上式中引入了松弛变量：</p>
<p><span class="math display">\[\begin{align}
    \begin{matrix}
        \underset{\left \{ \mathcal{W}, \Xi \right \}}{min}\sum_{j=1}^{q}\left \| \mathcal{W}_{j} \right \|^{2} + C\sum_{i=1}^{m}\frac{1}{\left | Y_{i} \right |\left | \bar{Y_{i}} \right |}\underset{\left ( y_{j},y_{k} \right ) \in Y_{i} \times \bar{Y_{i}}}{\sum }\xi_{ijk}\\ 
        subject \ to : \left \langle \omega_{j} - \omega_{k}, x_{i} \right \rangle + b_{j} - b_{k} \geq 1 - \xi_{ijk},\ \xi_{ijk} \geq 0 \left ( 1 \leq i \leq m, \left ( y_{j}, y_{k} \right ) \in Y_{i} \times \bar{Y_{i}} \right )
    \end{matrix}
\end{align}\]</span></p>
<p>其中，<span class="math inline">\(\Xi = \left \{ \xi_{ijk} \mid 1 \leq i \leq m, \left ( y_{j}, y_{k} \right ) \in Y_{i} \times \bar{Y_{i}} \right \}\)</span>是一个松弛变量集。上式中的目标由权衡参数<span class="math inline">\(C\)</span>平衡的两部分组成。具体而言，第一部分对应于学习系统的边界，而第二部分对应于以铰链形式实现的学习系统替代了以排序损失实现的学习系统。注意，替代排序损失也可以用其他方式来实现，例如神经网络的全局误差函数的指数形式。</p>
<blockquote>
<p>上面一段都没动的说明：</p>
</blockquote>
<p>需要注意的是，上式是一个具有凸目标和线性约束的标准二次规划（QP）问题，可以用现成的QP求解器进行求解。此外，为了使Rank-SVM具有非线性分类能力，一种常用的方法是通过核技巧求解上式的对偶形式。此外，Rank-SVM还可以使用stacking来设置阈值函数：</p>
<p><span class="math display">\[\begin{align}
    \begin{matrix}
        t\left ( x  \right ) = \left \langle \omega^{*}, f^{*}\left ( x \right ) \right \rangle + b^{*}\\ 
        where \ \ f^{*}\left ( x \right ) = \left ( f\left ( x, y_{1} \right ), \cdots , f\left ( x, y_{q} \right ) \right )^{T} \ and \ f\left ( x, y_{j} \right ) = \left \langle \omega_{j}, x \right \rangle + b_{j}
    \end{matrix}
\end{align}\]</span></p>
<p>然后对于未知实例<span class="math inline">\(x\)</span>，预测的标签集为：</p>
<p><span class="math display">\[\begin{align}
    Y = \left \{ y_{j} \mid \left \langle \omega_{j}, x \right \rangle + b_{j} &gt; \left \langle \omega^{*}, f^{*}\left ( x \right ) \right \rangle + b^{*}, 1 \leq j \leq q \right \}
\end{align}\]</span></p>
<p>评论：Rank-SVM是一个在相关和不相关标签对上定义了超平面边界的二阶方法。该算法得益于核技术在处理非线性分类上的优势，并且基于此可以进一步的实现相关变体。首先，上式中的经验排序损失可以用其他损失结构来代替，如汉明损失。它可以被归类为结构化输出分类的一般形式。其次，阈值策略可以用stacking以外的技术来实现。第三，为了避免核技术在选择的问题，因此在多标签数据中可以使用多核技术来进行学习。</p>
<img src="/review-multilabel-alg/rank-svm.png" class title="Rank-SVM">
<h3 id="collective-multi-label-classifiercml">Collective Multi-Label Classifier（CML）</h3>
<p>该算法的基本思想是适应最大熵原理处理多标签数据，其中标签之间的相关性被编码为所得到的结果，其分布必须满足的约束。</p>
<p>对于任意的对标签实例<span class="math inline">\(\left ( x, Y \right )\)</span>，让<span class="math inline">\(\left(x, y\right)\)</span>使用二元标签向量<span class="math inline">\(y = \left ( y_{1}, y_{2}, \cdots , y_{q} \right )^{T} \in \left \{ -1, +1 \right \}^{q}\)</span>来表示相应的随机变量，其中第<span class="math inline">\(j\)</span>个分量表示<span class="math inline">\(Y\)</span>是否包含第<span class="math inline">\(j\)</span>个标签，即<span class="math inline">\(\left(y_{j} = +1\right)\)</span>表示包含，<span class="math inline">\(\left(y_{j} = -1\right)\)</span>表示不包含。从统计上讲，多标签学习任务可以等价于学习联合概率分布<span class="math inline">\(p\left(x, y\right)\)</span>。</p>
<p>假设给定他们的分布<span class="math inline">\(p\left ( \cdot ,\cdot \right )\)</span>，那么<span class="math inline">\(\mathcal{H}_{p}\left(x, y\right)\)</span>就表示实例与标签<span class="math inline">\(\left(x, y\right)\)</span>的熵。最大熵的原理是假定当前知识状态的最佳分布模型是一个服从与给定事实集合<span class="math inline">\(\mathcal{K}\)</span>的最大<span class="math inline">\(\mathcal{H}_{p}\left(x, y\right)\)</span>。</p>
<p><span class="math display">\[\begin{align}
    \begin{matrix}
        \underset{p}{max} \ \mathcal{H}_{p}\left ( x, y \right )\\ 
        subject to: \ \mathbb{E}_{p}\left [ f_{k}\left ( x, y \right ) \right ] = F_{k} \ \left ( k \in \mathcal{K} \right )
    \end{matrix}
\end{align}\]</span></p>
<p>一般来说，事实被表达为在实例与标签<span class="math inline">\(\left(x, y\right)\)</span>某个函数期望的约束，即约束为<span class="math inline">\(\mathbb{E}_{p}\left [ f_{k}\left ( x, y \right ) \right ] = F_{k}\)</span>。这里的<span class="math inline">\(\mathbb{E}_{p}\left(\cdot\right)\)</span>表示<span class="math inline">\(p\left( \cdot, \cdot\right)\)</span>的期望算子，而<span class="math inline">\(F_{k}\)</span>则表示训练集估计的期望值，如<span class="math inline">\(\frac{1}{m} \sum_{\left ( x, y \right ) \in \mathcal{D}}f_{k}\left ( x, y \right )\)</span>。</p>
<p>利用标准拉格朗日乘数法，结合<span class="math inline">\(p\left( \cdot, \cdot\right)\)</span>上的归一化约束（即<span class="math inline">\(\mathbb{E}_{p}\left[1\right] = 1\)</span>），可以求解方程（51）的约束优化问题。因此，最优分布落在了Gibbs分布族上。</p>
<p><span class="math display">\[\begin{align}
    p\left ( y \mid x \right ) = \frac{1}{Z_{\Lambda}\left ( x \right )}exp\left ( \sum_{k \in \mathcal{K}} \lambda_{k} \cdot f_{k}\left ( x, y \right ) \right )
\end{align}\]</span></p>
<blockquote>
<p>Gibbs的说明：</p>
</blockquote>
<p>这里的<span class="math inline">\(\Lambda = \left \{ \lambda_{k} \mid k \in \mathcal{K} \right \}\)</span>是一组待确定的参数集合，并且<span class="math inline">\(Z_{\Lambda}\left ( x \right )\)</span>是归一化因子的分区函数，即：</p>
<p><span class="math display">\[\begin{align}
    Z_{\Lambda}\left ( x \right ) = \sum_{x}exp\left ( \sum_{k \in \mathcal{K}} \lambda_{k} \cdot f_{k}\left ( x, y \right ) \right )
\end{align}\]</span></p>
<p>通过假设高斯先验（即<span class="math inline">\(\lambda_{k} \sim \mathcal{N}\left ( 0, \varepsilon^{2} \right )\)</span>）。通过最大化对数后验概率函数可以找到<span class="math inline">\(\Lambda\)</span>中的参数。</p>
<p><span class="math display">\[\begin{align}
    \begin{matrix}
        l\left ( \Lambda \mid \mathcal{D} \right ) = log\left ( \prod_{\left ( x, y \right ) \in \mathcal{D}} p\left ( y \mid x \right ) \right ) - \sum_{k \in \mathcal{K}}\frac{\lambda_{2}^{l}}{2\varepsilon^{2}}\\ 
        =\sum_{\left ( x, y \right ) \in \mathcal{D}}\left ( \sum_{k \in \mathcal{K}} \lambda_{k} \cdot f_{k}\left ( x, y \right ) - logZ_{\Lambda}\left ( x \right ) \right ) - \sum_{k \in \mathcal{K}}\frac{\lambda_{2}^{k}}{2\varepsilon^{2}}
    \end{matrix}
\end{align}\]</span></p>
<p>需要注意的是，等式（54）是<span class="math inline">\(\Lambda\)</span>上的一个凸函数，其全局最大值（虽然不是封闭形式）可以由任何现成的无约束优化方法如BFGS找到。一般来说，大多数数值分析都需要<span class="math inline">\(l\left ( \Lambda \mid \mathcal{D} \right )\)</span>的梯度。</p>
<p><span class="math display">\[\begin{align}
    \frac{\partial l\left ( \Lambda \mid \mathcal{D} \right )}{\partial \lambda_{k}}=\sum_{\left ( x,y \right ) \in \mathcal{D}}\left ( f_{k}\left ( x, y \right ) - \sum_{u}f_{k}\left ( x, y \right )p\left ( y \mid x \right ) \right )-\frac{\lambda_{k}}{\varepsilon^{2}} \ \left ( k \in \mathcal{K} \right )
\end{align}\]</span></p>
<p>对于CML，约束集通常包含两部分<span class="math inline">\(\mathcal{K} = \mathcal{K}_{1} \cup \mathcal{K}_{2}\)</span>。具体来说，<span class="math inline">\(\mathcal{K}_{1} = \left \{ \left ( l, j \right ) \mid 1 \leq l \leq d, 1 \leq j \leq q \right \}\)</span>使用<span class="math inline">\(f_{k}\left ( x, y \right ) = x_{l} \cdot \left \| y_{j} = 1 \right \|\left ( k = \left ( l, j \right ) \in \mathcal{K}_{1} \right )\)</span>指定了全部<span class="math inline">\(d \cdot q\)</span>个约束。此外，<span class="math inline">\(\mathcal{K}_{2} = \left \{ \left ( j_{1}, j_{2}, b_{1}, b_{2} \right ) \mid 1 \leq j_{1} \leq j_{2} \leq q , b_{1}, b_{2} \in \left \{ -1, +1 \right \}\right \}\)</span>使用<span class="math inline">\(f_{k}\left ( x, y \right ) = \left \| y_{j_{1}} = b_{1} \right \| \cdot \left \| y_{j_{2}} = b_{2} \right \| \left ( k = \left ( j_{1}, j_{2}, b_{1}, b_{2} \right ) \in \mathcal{K}_{2} \right )\)</span>指定了全部<span class="math inline">\(4 \cdot \binom{q}{2}\)</span>个约束。实际上，<span class="math inline">\(CML\)</span>的<span class="math inline">\(\mathcal{K}\)</span>中的约束可以用其他变体方式指定。</p>
<p>对于位置的实例<span class="math inline">\(x\)</span>，预测的标签集为：</p>
<p><span class="math display">\[\begin{align}
    Y = arg \ \underset{y}{max} \left ( y \mid x \right )
\end{align}\]</span></p>
<p>需要注意的是，使用<span class="math inline">\(arg \ max\)</span>进行精确推论只适用于较小的标签集。否则需要使用剪枝技术来减少<span class="math inline">\(arg \ max\)</span>的搜索空间，例如只考虑训练集中出现的标签集。</p>
<p>评论：CML是一个在每个标签对的相关性上增加了<span class="math inline">\(\mathcal{K}_{2}\)</span>约束的二阶方法。CML所研究的二阶相关性比Rank-SVM更具一般性，因为Rank-SVM只考虑相关与不相关的标签对。作为条件随机场模型（CRF），CML倾向使用在等式（52）中使用条件概率分布<span class="math inline">\(p\left ( y \mid x \right )\)</span>来进行分类。有趣的是，<span class="math inline">\(p\left ( y \mid x \right )\)</span>可以使用多种方式进行分解，例如<span class="math inline">\(p\left ( y \mid x \right ) = \prod_{j=1}^{q}p\left ( y_{j} \mid x, y_{1}, \cdots , y_{j-1} \right )\)</span>，其中的每一项可以由分类器链中的分类器来进行建模，或者<span class="math inline">\(p\left ( y \mid x \right ) = \prod_{j=1}^{q}p\left ( y_{j} \mid x, pa_{j} \right )\)</span>，其中的每一项可以由有向图中的节点<span class="math inline">\(y_{j}\)</span>和它的父节点<span class="math inline">\(pa_{j}\)</span>进行建模，并且当有向图具有有限拓扑结构的多位贝叶斯网络时，该算法成立。有向图也可用于建模多故障诊断，其中<span class="math inline">\(y_{j}\)</span>表示某一个设备部件的良好或故障状态。另一方面，已经有了一些多标签生成模型，其目的是模拟联合概率分布<span class="math inline">\(p\left ( y \mid x \right )\)</span>。</p>
<img src="/review-multilabel-alg/cml.png" class title="CML">
<blockquote>
<p>CRF的说明：</p>
</blockquote>
<h1 id="总结">总结</h1>
<img src="/review-multilabel-alg/algorithms-reviewed.png" class title="多标签学习算法综述">
<p>图5总结了八种多标签学习算法的特性，包括基本思想、标签相关性、计算复杂性、已经测试域和优化（代理）指标。从图5可以知道，汉明损失和排序损失是最流行的度量指标之一，并且在前文也对其进行了理论分析。此外，值得注意的是，通过Random k-Labelsets优化的子集精度仅是对k-Labelsets而不是整个标签空间。</p>
<p>图5中的测试领域是该算法对应的原始文件中使用效果最好的数据类型。然而，所有这些有代表性的多标签学习算法都是通用的，并且可以应用于各种数据类型。然而，每种学习算法的计算复杂度对其适用于不同规模的数据起着关键的作用。这里，可以从三个主要方面来研究数据可伸缩性，包括训练实例的数量（即<span class="math inline">\(m\)</span>）、维度（即<span class="math inline">\(d\)</span>）和可能的标签类别数量（即<span class="math inline">\(q\)</span>）。此外，当实例的维度远远大于类标签的数量时（即<span class="math inline">\(d \gg q\)</span>），使用将标签类别作为实例空间的额外特征的策略将对算法不会有太多改善。</p>
<p>作为研究最多的监督学习框架，图5中的几种算法都采用二元分类器作为多标签数据学习的中间步骤。对二元分类的转换的最初来源于著名的AdaBoost.MH算法，在该算法中每个多标签训练实例<span class="math inline">\(\left ( x_{i}, Y_{i} \right )\)</span>被转换成<span class="math inline">\(q\)</span>个二元实例<span class="math inline">\(\left \{ \left ( \left [ x_{i}, y_{j} \right ],\phi \left ( Y_{i}, y_{j} \right ) \right ) \mid 1 \leq j \leq q\right \}\)</span>。该算法被认为是一种高阶方法，其中<span class="math inline">\(\mathcal{Y}\)</span>中的标签被认为是<span class="math inline">\(\mathcal{X}\)</span>的附加特征，并且通过共享实例<span class="math inline">\(x\)</span>把彼此之间进行关联，只要二元学习算法<span class="math inline">\(\mathcal{B}\)</span>能够捕获特征之间的依赖关系。二元分类转换的其他方法可以通过诸如堆叠聚合或纠错输出码（ECOC）等技术来实现。</p>
<blockquote>
<p>AdaboSt.MH的说明：基于AdaBoost.M1算法的一种标签转化方案的多标签算法。</p>
</blockquote>
<blockquote>
<p>堆叠聚合的说明：</p>
</blockquote>
<blockquote>
<p>ECOC的说明：能够将多类分类问题转化为多个二分类问题，而且利用纠错输出码本身具有纠错能力的特性，可以提高监督学习算法的预测精度。</p>
</blockquote>
<p>此外，适应一阶算法的方法不能简单地认为是二元相关性与特定二元学习者的结合。例如，ML-KNN不仅仅是二元相关性与KNN的结合，而是利用贝叶斯推理与相邻信息进行推理。同时，ML-DT也不仅仅是二元相关性与决策树结合组成为单一决策树，而构建了<span class="math inline">\(q\)</span>个决策树，以适应所有类别的标签（基于多标签熵）。</p>
<h1 id="相关学习的设置">相关学习的设置</h1>
<p>与多标签学习相关的学习设置有很多值得讨论的问题，如多实例学习、有序分类、多任务学习、数据流分类等。</p>
<p>在多实例学习研究的问题中每个示例是由一袋实例和一个（二元）标签来描述。只有当该袋实例中至少包含了一个正实例，那么该袋就被认为是正例。与输出（标签）空间具有对象歧义（复杂语义）的多标签学习模型相比，多实例学习可以被认为是在输入（实例）空间具有对象的歧义性的模型。目前在多标签数据中多实例学习已经有了一些初步的尝试。</p>
<p>有序分类研究的问题是对所有类别的标签进行自然排序。在多标签学习中，可以对每类标签的相关性进行排序，以便将各个标签（<span class="math inline">\(y_{j} \in \left \{ -1, +1 \right \}\)</span>）进行分级归类（<span class="math inline">\(y_{j} \in \left \{ m_{1}, m_{2}, \cdots , m_{k} \right \} \ where \ m_{1} &lt; m_{2} &lt; \cdots &lt; m_{k}\)</span>）。因此，分级多标签学习只能提供模糊的序号，而不是标签相关性的明确判断。现有的研究表明，分级多标签学习可以通过将其转化为一组有序分类问题（每类标签都是一个问题），或一组标准的多标签学习问题（每个成员级别是一个问题）来解决。</p>
<p>多任务学习主要研究的是多个任务并行训练的问题，利用相关任务的训练信息作为归纳偏差，以此来提高其他任务的泛化性能。然而，多任务学习与多标签学习之间存在着本质的区别。首先，在多标签学习中，所有的示例共享相同的特征空间，而在多任务学习中，任务可以在相同的特征空间，也可以在不同的特征空间。其次，多标签学习的目的是预测与对象相关联的标签子集，而多任务学习的目的是同时完成多个任务，并不关心哪个任务子集应该与哪个对象相关联（该学习算法中把标签认为是一个任务），因为它通常假定每个对象与所有任务相关。第三，在多标签学习中，处理标签空间较大的问题很常见，但在多任务学习中，考虑大量任务是不合理的。然而，多任务学习的技术可能可以用来帮助多标签学习。</p>
<p>数据流分类主要研究现实世界中的对象在线生成和实时处理的问题。目前，多标签的流媒体数据广泛存在于即时新闻、电子邮件、微博等现实场景中。作为流媒体数据分析的常见挑战，如何有效地对多标签数据流进行分类是处理概念漂移问题的关键因素。现有的数据流分类模型通过在新的一批示例到达时更新分类器来实现概念漂移。一般采用采用衰落假设，即随着时间的推移，过去数据的影响逐渐下降，或者每当检测到概念漂移时保持变化检测器报警。</p>
<h1 id="结论">结论</h1>
<p>在本文中，回顾了最先进的多标签学习的范式、算法和相关的学习设置。此外，本文并没有尝试回顾所有的学习技术，而是选择八个最具代表性的多标签学习算法进行描述。在下表中总结了一些用于多标签学习的在线资源，包括学术活动（教程、研讨会、专题）、公开可用的软件和数据集。</p>
<img src="/review-multilabel-alg/online-resources.png" class title="多标签学习的在线资源">
<p>目前，虽然标签之间的相关性已经被用于各种多标签学习技术，但是在标签相关性的使用上还没有任何关于底层概念或任何原理性机制。最近的研究表明，标签之间的相关性可能是不对称的，即一个标签对另一个标签的影响不一定在相反的方向上是相同的，或者是局部相同，即不同的实例共享不同的标签相关性，很少有全局适用的相关性。然而，充分理解标签相关性，特别是对于具有较大输出空间的场景，仍然是多标签学习的目标。</p>
<p>在第三节重点介绍多标签学习算法的特性。这篇综述的一个对不同多标签学习算法优缺点的深入研究的补充。在文献中，我们可以找到一个最近尝试进行的具有广泛性的实验比较，即12个多标签学习算法与16个评价指标进行了比较。有趣的是，分类和排名指标最佳的算法是基于集成学习技术，即预测决策树的随机森林。然而，在更广泛的范围内或集中在某个类型内模型比较是值得进一步探讨的课题。</p>
]]></content>
      <categories>
        <category>论文</category>
        <category>多标签学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>多标签分类</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux系统装机工具和方法</title>
    <url>/osinitial/</url>
    <content><![CDATA[<h1 id="linux常用软件安装">Linux常用软件安装</h1><h2 id="vim">VIM</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:jonathonf/vim</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install vim</span><br></pre></td></tr></table></figure><h2 id="shadowsocks-qt5">Shadowsocks-Qt5</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:hzwhuang/ss-qt5</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install shadowsocks-qt5</span><br></pre></td></tr></table></figure><a id="more"></a>




<h2 id="libreoffice">LibreOffice</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:libreoffice/ppa</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install libreoffice</span><br></pre></td></tr></table></figure>
<h2 id="shutter">Shutter</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:ubuntuhandbook1/shutter</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install shutter</span><br></pre></td></tr></table></figure>
<h2 id="vscode">VSCode</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install curl</span><br><span class="line">curl https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor &gt; microsoft.gpg</span><br><span class="line">sudo mv microsoft.gpg /etc/apt/trusted.gpg.d/microsoft.gpg</span><br><span class="line">sudo sh -c <span class="string">'echo "deb [arch=amd64] https://packages.microsoft.com/repos/vscode stable main" &gt; /etc/apt/sources.list.d/vscode.list'</span></span><br><span class="line">sudo apt-get install apt-transport-https</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install code</span><br></pre></td></tr></table></figure>
<h2 id="deluge">Deluge</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:deluge-team/ppa</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install deluge</span><br></pre></td></tr></table></figure>
<h2 id="emacs">Emacs</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:kelleyk/emacs</span><br><span class="line">sudo apt update</span><br><span class="line">sudo apt install emacs25</span><br></pre></td></tr></table></figure>
<h2 id="gimp">GIMP</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:otto-kesselgulasch/gimp</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt install gimp</span><br></pre></td></tr></table></figure>
<h2 id="simplescreenrecorder">SimpleScreenRecorder</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:maarten-baert/simplescreenrecorder</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install simplescreenrecorder</span><br></pre></td></tr></table></figure>
<h2 id="timeshift">Timeshift</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-add-repository -y ppa:teejee2008/ppa</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install timeshift</span><br></pre></td></tr></table></figure>
<h2 id="sysmonitor">Sysmonitor</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:fossfreedom/indicator-sysmonitor</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install indicator-sysmonitor</span><br></pre></td></tr></table></figure>
<h2 id="vlc">VLC</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:videolan/master-daily</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install vlc</span><br></pre></td></tr></table></figure>
<h2 id="有道字典">有道字典</h2>
<p>首先到有道词典官网下载deb安装包，注意有道词典Ubuntu版本只支持到Ubuntu 14.04,如果在Ubuntu 16.04上安装会失败，因为官方的Ubuntu版本的deb包依赖gstreamer0.10-plugins-ugly，但是该软件在16.04里面已经没有了。所以我们要下载64位的deepin版的安装包。经过测试，64位的deepin版的deb包在Ubuntu 16.04上安装成功。</p>
<h2 id="guake">Guake</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:webupd8team/unstable</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install guake</span><br></pre></td></tr></table></figure>
<p>让Guake随系统自动启动，定位到"系统-&gt;首选项-&gt;启动程序-&gt;打开'启动程序喜好'程序"，单击'添加'按钮，在弹出的窗口中填写程序名称、命令和注释，命令一定不要写错（可以是程序名字guake，也可以是命令的绝对路径如/usr/bin/guake，建议采用前者），其他可以随便填。</p>
<h2 id="wps">WPS</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install lib32ncurses5 lib32z1 libpng12-0</span><br></pre></td></tr></table></figure>
<p><a href="http://wps-community.org/downloads?vl=fonts#download" target="_blank" rel="noopener">下载最新的WPS地址</a></p>
<blockquote>
<ul>
<li>下载该字体，解压后将整个wps_symbol_fonts中的内容目录拷贝到 /usr/share/fonts/wps-office 目录下</li>
<li>进入字体目录 /usr/share/fonts/wps-office</li>
<li>sudo mkfontdir</li>
<li>sudo mkfontscale</li>
<li>fc-cache</li>
</ul>
</blockquote>
<h2 id="指纹登录">指纹登录</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:fingerprint/fprint</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get upgrade</span><br><span class="line">sudo apt-get install libfprint0 fprint-demo libpam-fprintd</span><br></pre></td></tr></table></figure>
<h2 id="uget">UGet</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:plushuang-tw/uget-stable</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install uget aria2</span><br></pre></td></tr></table></figure>
<h2 id="ugetchromewrapper">uGetChromeWrapper</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:slgobinath/uget-chrome-wrapper</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install uget aria2</span><br></pre></td></tr></table></figure>
<h2 id="远程桌面remminal2w">远程桌面Remmina（L2W）</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-add-repository ppa:remmina-ppa-team/remmina-next</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install remmina remmina-plugin-rdp libfreerdp-plugins-standard</span><br></pre></td></tr></table></figure>
<h2 id="远程桌面xfcew2l">远程桌面XFCE（W2L）</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install xrdp vnc4server xfce4</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"xfce4-session"</span> &gt;~/.xsession</span><br><span class="line">sudo service xrdp restart</span><br></pre></td></tr></table></figure>
<h2 id="主题">主题</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install unity-tweak-tool</span><br><span class="line"></span><br><span class="line">sudo add-apt-repository ppa:noobslab/themes</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install flatabulous-theme</span><br><span class="line"></span><br><span class="line">sudo add-apt-repository ppa:noobslab/icons</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install ultra-flat-icons</span><br></pre></td></tr></table></figure>
<h2 id="蓝牙耳机">蓝牙耳机</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install blueman bluez*</span><br></pre></td></tr></table></figure>
<h2 id="virtualbox">VirtualBox</h2>
<p><a href="https://www.virtualbox.org/wiki/Linux_Downloads" target="_blank" rel="noopener">下载地址</a></p>
<blockquote>
<ul>
<li>Qt 5.3.2 or later. Qt 5.6.2 or later is recommended.</li>
<li>SDL 1.2.7 or later. This graphics library is typically called libsdl or similar.</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo dpkg -i virtualbox-6.0_6.0.4-128413~Ubuntu~xenial_amd64.deb</span><br></pre></td></tr></table></figure>
<h2 id="putty">Putty</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install libgtk2.0-dev</span><br><span class="line"><span class="built_in">cd</span> unix</span><br><span class="line">./configure</span><br><span class="line">make -f Makefile.gtk</span><br><span class="line"><span class="built_in">cd</span> ..</span><br><span class="line">make install</span><br></pre></td></tr></table></figure>
<h2 id="ftp的安装与设置">FTP的安装与设置</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install vsftpd</span><br></pre></td></tr></table></figure>
<p>修改FTP配置文件/etc/vsftpd.conf</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 是否开放本地用户写权限，即是否允许上传</span></span><br><span class="line">write_enable=YES</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为YES时，禁止所有用户访问上级目录，只能访问各自的家目录</span></span><br><span class="line">chroot_local_user=NO</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置vsftpd使用utf8编码的文件系统</span></span><br><span class="line">utf8_filesystem=YES</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果不设置这个，会出现425 Security: Bad IP connecting.类似于这种的错误</span></span><br><span class="line">pasv_enable=YES</span><br><span class="line">pasv_min_port=40000</span><br><span class="line">pasv_max_port=40010</span><br><span class="line">pasv_promiscuous=YES</span><br></pre></td></tr></table></figure>
<h2 id="putty配色">Putty配色</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">* Default Foreground: 255/255/255  </span><br><span class="line">* Default Background: 51/51/51  </span><br><span class="line">* ANSI Black: 77/77/77  </span><br><span class="line">* ANSI Green: 152/251/152  </span><br><span class="line">* ANSI Yellow: 240/230/140  </span><br><span class="line">* ANSI Blue: 205/133/63  </span><br><span class="line">* ANSI Blue Bold 135/206/235  </span><br><span class="line">* ANSI Magenta: 255/222/173 or 205/92/92  </span><br><span class="line">* ANSI Cyan: 255/160/160  </span><br><span class="line">* ANSI Cyan Bold: 255/215/0  </span><br><span class="line">* ANSI White: 245/222/179</span><br></pre></td></tr></table></figure>
<h2 id="telnet安装">Telnet安装</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install openbsd-inetd</span><br><span class="line">sudo apt-get install telnetd</span><br><span class="line">sudo service openbsd-inetd restart</span><br></pre></td></tr></table></figure>
<h2 id="firefox安装">FireFox安装</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:mozillateam/firefox-next </span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install firefox</span><br></pre></td></tr></table></figure>
<h2 id="ubuntu下罗技优联管理关键">Ubuntu下罗技优联管理关键</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install solaar-gnome3</span><br></pre></td></tr></table></figure>
<h2 id="安装ubuntu_live镜像制作软件cubic">安装Ubuntu_Live镜像制作软件cubic</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 081525E2B4F1283B</span><br><span class="line">sudo apt-add-repository ppa:cubic-wizard/release</span><br><span class="line">sudo apt update</span><br><span class="line">sudo apt install cubic</span><br></pre></td></tr></table></figure>
<h2 id="安装tmux">安装Tmux</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install tmux</span><br></pre></td></tr></table></figure>
<h2 id="安装ssh服务">安装SSH服务</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install openssh-server</span><br></pre></td></tr></table></figure>
<h2 id="peek">Peek</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:peek-developers/stable</span><br><span class="line">sudo apt update</span><br><span class="line">sudo apt install peek</span><br></pre></td></tr></table></figure>
<h2 id="kcptun">KCPTUN</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#! /bin/sh</span></span><br><span class="line"><span class="comment">### BEGIN INIT INFO</span></span><br><span class="line"><span class="comment"># Provides:          kcptun</span></span><br><span class="line"><span class="comment"># Required-Start:    </span></span><br><span class="line"><span class="comment"># Required-Stop:</span></span><br><span class="line"><span class="comment"># Should-Start:      </span></span><br><span class="line"><span class="comment"># Default-Start:     1 2 3 4 5</span></span><br><span class="line"><span class="comment"># Default-Stop:</span></span><br><span class="line"><span class="comment"># Short-Description: kcptun</span></span><br><span class="line"><span class="comment"># Description:       Network file systems are mounted by</span></span><br><span class="line"><span class="comment">#                    /etc/network/if-up.d/mountnfs in the background</span></span><br><span class="line"><span class="comment">#                    when interfaces are brought up; this script waits</span></span><br><span class="line"><span class="comment">#                    for them to be mounted before carrying on.</span></span><br><span class="line"><span class="comment">### END INIT INFO</span></span><br><span class="line"></span><br><span class="line">/opt/kcptun-linux-amd64/client_linux_amd64 -l :3000 -r 45.77.188.138:29900 -key <span class="string">"123456"</span> -crypt none -nocomp -datashard 70 -parityshard 30 -mtu 1400 -sndwnd 2048 -rcvwnd 2048 -dscp 46 -quiet -mode fast2</span><br></pre></td></tr></table></figure>
<h2 id="apt-fast">APT-FAST</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:apt-fast/stable</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install apt-fast</span><br></pre></td></tr></table></figure>
<h2 id="kvm">KVM</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install qemu-kvm qemu virt-manager virt-viewer libvirt-bin bridge-utils</span><br></pre></td></tr></table></figure>
<h2 id="sqlitebrowser">SQLiteBrowser</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository -y ppa:linuxgndu/sqlitebrowser</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install sqlitebrowser</span><br></pre></td></tr></table></figure>
<h2 id="masterpdf">MasterPDF</h2>
<p><a href="https://code-industry.net/downloads/" target="_blank" rel="noopener">官网</a></p>
<h2 id="texlive">Texlive</h2>
<p><a href="https://mirrors.tuna.tsinghua.edu.cn/CTAN/systems/texlive/Images/" target="_blank" rel="noopener">清华大学镜像下载地址</a></p>
<blockquote>
<ul>
<li>1、图形化安装</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install perl-tk</span><br><span class="line">sudo mount -o loop texlive2019.iso /mnt</span><br><span class="line"><span class="built_in">cd</span> /mnt/</span><br><span class="line">sudo ./install-tl -gui</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>2、设置字体</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo cp /usr/<span class="built_in">local</span>/texlive/2019/texmf-var/fonts/conf/texlive-fontconfig.conf /etc/fonts/conf.d/09-texlive.conf</span><br><span class="line">sudo <span class="built_in">fc</span>-cache -fsv</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>3、设置环境变量</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> MANPATH=<span class="variable">$&#123;MANPATH&#125;</span>:/usr/<span class="built_in">local</span>/texlive/2019/texmf-dist/doc/man</span><br><span class="line"><span class="built_in">export</span> INFOPATH=<span class="variable">$&#123;INFOPATH&#125;</span>:/usr/<span class="built_in">local</span>/texlive/2019/texmf-dist/doc/info</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$&#123;PATH&#125;</span>:/usr/<span class="built_in">local</span>/texlive/2019/bin/x86_64-linux</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>4、设置系统MAN</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">MANPATH_MAP /usr/<span class="built_in">local</span>/texlive/2019/bin/x86_64-linux /usr/<span class="built_in">local</span>/texlive/2019/texmf-dist/doc/man</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>5、启动环境变量</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">source</span> ~/.bashrc</span><br><span class="line"><span class="built_in">source</span> ~/.profile</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>6、测试</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tex --version</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>7、弹出镜像</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo umount /mnt</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>8、VSCode配置</li>
</ul>
</blockquote>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">"latex-workshop.latex.recipes": [</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="attr">"name"</span>: <span class="string">"xelatex"</span>,</span><br><span class="line">        <span class="attr">"tools"</span>: [</span><br><span class="line">            <span class="string">"xelatex"</span></span><br><span class="line">        ]</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="attr">"name"</span>: <span class="string">"xelatex ➞ bibtex ➞ exlatex × 2"</span>,</span><br><span class="line">        <span class="attr">"tools"</span>: [</span><br><span class="line">            <span class="string">"xelatex"</span>,</span><br><span class="line">            <span class="string">"bibtex"</span>,</span><br><span class="line">            <span class="string">"xelatex"</span>,</span><br><span class="line">            <span class="string">"xelatex"</span></span><br><span class="line">        ]</span><br><span class="line">    &#125;</span><br><span class="line">],</span><br><span class="line">"latex-workshop.latex.tools": [</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="attr">"name"</span>: <span class="string">"xelatex"</span>,</span><br><span class="line">        <span class="attr">"command"</span>: <span class="string">"xelatex"</span>,</span><br><span class="line">        <span class="attr">"args"</span>: [</span><br><span class="line">            <span class="string">"-synctex=1"</span>,</span><br><span class="line">            <span class="string">"-interaction=nonstopmode"</span>,</span><br><span class="line">            <span class="string">"-file-line-error"</span>,</span><br><span class="line">            <span class="string">"%DOC%"</span></span><br><span class="line">        ]</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="attr">"name"</span>: <span class="string">"bibtex"</span>,</span><br><span class="line">        <span class="attr">"command"</span>: <span class="string">"bibtex"</span>,</span><br><span class="line">        <span class="attr">"args"</span>: [</span><br><span class="line">            <span class="string">"%DOCFILE%"</span></span><br><span class="line">        ]</span><br><span class="line">    &#125;</span><br><span class="line">],</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>9、配置宏包的源</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo tlmgr option repository https://mirrors.tuna.tsinghua.edu.cn/CTAN/systems/texlive/tlnet</span><br><span class="line">sudo tlmgr update --self --all</span><br></pre></td></tr></table></figure>
<h2 id="miktex">MiKTeX</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys D6BC243565B2087BC3F897C9277A7293F59E4889</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"deb http://miktex.org/download/ubuntu xenial universe"</span> | sudo tee /etc/apt/sources.list.d/miktex.list</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install miktex</span><br></pre></td></tr></table></figure>
<h2 id="pinta">Pinta</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-fast install pinta</span><br></pre></td></tr></table></figure>
<h1 id="资源下载">资源下载</h1>
<h2 id="android生成key">Android生成Key</h2>
<p><a href="https://pan.baidu.com/s/11KC0kjlk0rHle7SSY7l5RA" target="_blank" rel="noopener">下载地址</a></p>
<p>提取码: pwkg</p>
<h2 id="常用字体">常用字体</h2>
<p><a href="https://pan.baidu.com/s/17htdYo6VgNKE6sjbywqKxQ" target="_blank" rel="noopener">下载地址</a></p>
<p>提取码: jqs7</p>
<h2 id="git-key">Git Key</h2>
<p><a href="https://pan.baidu.com/s/1un69NhkHRY0Xxx5_qFisAw" target="_blank" rel="noopener">下载地址</a></p>
<p>提取码: net4</p>
<h2 id="intellijidea认证服务">IntelliJIDEA认证服务</h2>
<p><a href="https://pan.baidu.com/s/1Mzqf2pCrSbI1bS5Manfy3Q" target="_blank" rel="noopener">下载地址</a></p>
<p>提取码: e2ck</p>
<h2 id="openocd">OpenOCD</h2>
<p><a href="https://pan.baidu.com/s/1SZiGeIYuXT00cWc8I3rO1w" target="_blank" rel="noopener">下载地址</a></p>
<p>提取码: gvsy</p>
<h2 id="switchyomega">SwitchyOmega</h2>
<p><a href="https://pan.baidu.com/s/1GmBNDaNxCNGBnhRzrINfUQ" target="_blank" rel="noopener">下载地址</a></p>
<p>提取码: myjc</p>
<h2 id="tomcat">Tomcat</h2>
<p><a href="https://pan.baidu.com/s/1VF4MOIaXNyNvVXtkIcEArg" target="_blank" rel="noopener">下载地址</a></p>
<p>提取码: dtbt</p>
<h2 id="virtualserialport">VirtualSerialPort</h2>
<p><a href="https://pan.baidu.com/s/1Uf7hZjXDGw9rVPGMNevVwQ" target="_blank" rel="noopener">下载地址</a></p>
<p>提取码: db6k</p>
<h2 id="wps缺失字体">WPS缺失字体</h2>
<p><a href="https://pan.baidu.com/s/1ghDNODeuG7oO9Xb3tG8XSQ" target="_blank" rel="noopener">下载地址</a></p>
<p>提取码: bjip</p>
<h2 id="kcptun配置">KCPTUN配置</h2>
<p><a href="https://pan.baidu.com/s/1oO8y4A38DDgPGnKy6ioZrA" target="_blank" rel="noopener">下载地址</a></p>
<p>提取码: hbmv</p>
<h1 id="系统配置">系统配置</h1>
<h2 id="快捷方式">快捷方式</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[Desktop Entry]</span><br><span class="line">Name=Eclipse Java</span><br><span class="line">Comment=Code Editing. Redefined.</span><br><span class="line">GenericName=Text Editor</span><br><span class="line">Exec=/opt/eclipse/java-oxygen/eclipse/eclipse</span><br><span class="line">Icon=/opt/eclipse/java-oxygen/eclipse/icon.xpm</span><br><span class="line">Type=Application</span><br><span class="line">Categories=Utility;TextEditor;Development;IDE;</span><br><span class="line">Keywords=java;</span><br></pre></td></tr></table></figure>
<h2 id="解决windows与ubuntu双系统下的时间问题">解决Windows与Ubuntu双系统下的时间问题</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install ntpdate</span><br><span class="line">sudo ntpdate time.windows.com</span><br><span class="line">sudo hwclock --localtime --systohc</span><br></pre></td></tr></table></figure>
<p>重新进入Windows系统调整下时间就可以</p>
<h2 id="ubuntu系统下安装最新的nvidia驱动">Ubuntu系统下安装最新的Nvidia驱动</h2>
<blockquote>
<ul>
<li>1、到Nvidia<a href="https://www.nvidia.cn/Download/index.aspx?lang=cn" target="_blank" rel="noopener">官网下载最新.run驱动文件</a></li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>2、添加Nvidia最新PPA</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:graphics-drivers/ppa </span><br><span class="line">sudo apt update</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>3、在/etc/modprobe.d/中创建一个blacklist-nouveau.conf文件，并添加nouveau的禁用配置项</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">blacklist nouveau</span><br><span class="line">blacklist lbm-nouveau</span><br><span class="line">options nouveau modeset=0</span><br><span class="line"><span class="built_in">alias</span> nouveau off</span><br><span class="line"><span class="built_in">alias</span> lbm-nouveau off</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>4、重启计算机在登录页面使用<code>Ctrl+Alt+F1</code>进入字符界面后，键入下面指令，如果没有任何信息，表示Nouveau驱动禁用成功</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">lsmod | grep nouveau</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>5、使用.run文件删除当前所有的Nvidia驱动（不要手动删除，用自动方式更安全）</li>
</ul>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo chmod +x NVIDIA-Linux-x86_64-xxx.run</span><br><span class="line">sudo ./NVIDIA-Linux-x86_64-xxx.run –uninstall</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>6、使用PPA指令进行安装，并重启计算机，就可以在<strong>设置-&gt;详细信息</strong>中看到</li>
</ul>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install nvidia-xxx</span><br><span class="line">sudo reboot</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>7、在<strong>设置-&gt;详细信息</strong>中检查是否安装成功，也可以使用下面指令进行查看</li>
</ul>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nvidia-smi</span><br></pre></td></tr></table></figure>
<h2 id="设置应用程序开机自动启动">设置应用程序开机自动启动</h2>
<blockquote>
<ul>
<li>1、复制/etc/init.d目录下的一个.sh文件，并进行修改</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>2、执行下面的指令，把该启动文件添加到启动列表中</li>
</ul>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo update-rc.d huadian.sh defaults</span><br></pre></td></tr></table></figure>
<h2 id="解决ubuntu下zip文件乱码">解决Ubuntu下ZIP文件乱码</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install unar</span><br><span class="line">unar XXX.zip</span><br></pre></td></tr></table></figure>
<h2 id="ubuntu下增加ttyusb权限">Ubuntu下增加ttyUSB权限</h2>
<blockquote>
<ul>
<li>1、创建USB规则文件</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo gedit /etc/udev/rules.d/70-ttyusb.rules</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>2、添加文件内容</li>
</ul>
</blockquote>
<p>KERNEL=="ttyUSB[0-9]*", MODE="0666"</p>
<blockquote>
<ul>
<li>3、修改USB驱动权限</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo chmod 666 /dev/ttyUSB0</span><br></pre></td></tr></table></figure>
<h2 id="双系统grub配置">双系统GRUB配置</h2>
<blockquote>
<ul>
<li>在终端中输入：sudo gedit /etc/default/grub</li>
<li>将文本”GRUB_DEFAULT=0“中的0改成win7系统的序号4</li>
<li>在终端输入：sudo update-grub2</li>
</ul>
</blockquote>
<h2 id="ubuntu下增加keyboard权限">Ubuntu下增加Keyboard权限</h2>
<blockquote>
<ul>
<li>1、创建Keyboard规则文件</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo gedit /etc/udev/rules.d/70-usbkbd.rules</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>2、添加文件内容</li>
</ul>
</blockquote>
<p>KERNEL=="event*", NAME="input/%k", MODE="0666"</p>
<blockquote>
<ul>
<li>3、修改Keyboard驱动权限</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo chmod 666 /dev/input/by-id/usb-Logitech_USB_Receiver-if02-event-kbd</span><br></pre></td></tr></table></figure>
<h2 id="ubuntu下libpython安装错误解决">Ubuntu下libpython安装错误解决</h2>
<p>Ubuntu更新“libpython3.6-stdlib_3.6.5-5~16.04.york1_amd64.deb”时错误导致无法更新</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo dpkg --install --force all /var/cache/apt/archives/libpython3.6-stdlib_3.6.5-5~16.04.york1_amd64.deb</span><br><span class="line"></span><br><span class="line">sudo apt install -f</span><br></pre></td></tr></table></figure>
<h2 id="解决内存占满导致死机的问题">解决内存占满导致死机的问题</h2>
<p>编辑内存清空脚本</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line">sync &amp;&amp; <span class="built_in">echo</span> 1 | sudo tee /proc/sys/vm/drop_caches</span><br><span class="line">sync &amp;&amp; <span class="built_in">echo</span> 2 | sudo tee /proc/sys/vm/drop_caches</span><br><span class="line">sync &amp;&amp; <span class="built_in">echo</span> 3 | sudo tee /proc/sys/vm/drop_caches</span><br></pre></td></tr></table></figure>
<p>利用cron创建定时服务</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo -s</span><br><span class="line"></span><br><span class="line">crontab -e</span><br></pre></td></tr></table></figure>
<p>添加如下内容</p>
<blockquote>
<p>0 * * * * /opt/clearBuffer.sh</p>
</blockquote>
<h2 id="设置树莓派屏幕分辨率">设置树莓派屏幕分辨率</h2>
<p>config.txt设置</p>
<blockquote>
<p>hdmi_group=2</p>
</blockquote>
<blockquote>
<p>hdmi_mode=87</p>
</blockquote>
<blockquote>
<p>hdmi_cvt 800 480 60 6 0 0 0</p>
</blockquote>
<h1 id="开发工具">开发工具</h1>
<h2 id="java">Java</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:webupd8team/java</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install oracle-java8-installer</span><br><span class="line">sudo apt-get install oracle-java8-set-default</span><br></pre></td></tr></table></figure>
<h2 id="apache2">Apache2</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:ondrej/apache2</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt install apache2</span><br></pre></td></tr></table></figure>
<h2 id="php">PHP</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:ondrej/php</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install php7.1 php7.1-dev libapache2-mod-php7.1 libevent-dev php7.1-bz2 php7.1-cgi php7.1-cli php7.1-common php7.1-curl php7.1-gd php7.1-fpm php7.1-imap php7.1-json php7.1-mbstring php7.1-mcrypt php7.1-mysql php7.1-odbc php7.1-snmp php7.1-soap php7.1-sybase php7.1-tidy php7.1-xml php7.1-xmlrpc php7.1-xsl php7.1-zip php-pear</span><br><span class="line">sudo pecl install event</span><br><span class="line">sudo pecl install xdebug</span><br></pre></td></tr></table></figure>
<blockquote>
<p>修改WebServer位置</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/etc/apache2/sites-enabled/000-default.conf</span><br><span class="line"></span><br><span class="line">DocumentRoot /home/taowenyin/MyCode/WebServer</span><br><span class="line"></span><br><span class="line">&lt;Directory /home/taowenyin/MyCode/WebServer&gt;</span><br><span class="line">    Options FollowSymLinks Indexes</span><br><span class="line">    Require all granted</span><br><span class="line">&lt;/Directory&gt;</span><br></pre></td></tr></table></figure>
<h2 id="git">Git</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:git-core/ppa</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt install git</span><br></pre></td></tr></table></figure>
<h2 id="gitkraken">Gitkraken</h2>
<h2 id="node.js">Node.js</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">curl -o- https://raw.githubusercontent.com/creationix/nvm/v0.33.2/install.sh | bash</span><br><span class="line">nvm install node</span><br><span class="line"><span class="comment"># 切换到国内镜像</span></span><br><span class="line">npm config <span class="built_in">set</span> registry https://registry.npm.taobao.org</span><br></pre></td></tr></table></figure>
<h2 id="python的安装与配置">Python的安装与配置</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># sudo add-apt-repository ppa:jonathonf/python-2.7</span></span><br><span class="line"><span class="comment"># sudo add-apt-repository ppa:jonathonf/python-3.7</span></span><br><span class="line"></span><br><span class="line">sudo add-apt-repository ppa:deadsnakes/ppa</span><br><span class="line"></span><br><span class="line">sudo apt-get update</span><br><span class="line"></span><br><span class="line">sudo apt-get install python3.7</span><br><span class="line">sudo apt-get install python2.7</span><br><span class="line"></span><br><span class="line"><span class="comment"># 把Python2、3加入update-alternatives选择列表，最后1表示优先级，数字越大优先级越高</span></span><br><span class="line">sudo update-alternatives --install /usr/bin/python python /usr/bin/python2.7 1</span><br><span class="line">sudo update-alternatives --install /usr/bin/python python /usr/bin/python3.7 2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 选择需要的默认Python版本</span></span><br><span class="line">sudo update-alternatives --config python</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改Python3.5和3.7加入update-alternatives选择列表，最后1表示优先级，数字越大优先级越高</span></span><br><span class="line">sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.7 1</span><br><span class="line">sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.5 2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 选择需要的默认Python3版本</span></span><br><span class="line">sudo update-alternatives --config python3</span><br></pre></td></tr></table></figure>
<h2 id="android环境变量">Android环境变量</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> ANDROID_HOME=/home/taowenyin/Android/Sdk</span><br><span class="line"><span class="built_in">export</span> ANDROID_SDK_HOME=/home/taowenyin/Android</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$ANDROID_HOME</span>/platform-tools:<span class="variable">$ANDROID_HOME</span>/tools</span><br></pre></td></tr></table></figure>
<h2 id="c和32bit库">C++和32bit库</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install lib32ncurses5 lib32z1 libgl1-mesa-dev ttf-wqy-*</span><br></pre></td></tr></table></figure>
<h2 id="intellijidealicenseserver">IntelliJIDEALicenseServer</h2>
<p><a href="http://blog.lanyus.com/" target="_blank" rel="noopener">下载地址和使用说明</a></p>
<blockquote>
<ul>
<li>在终端中输入并添加：sudo gedit /etc/rc.loacl</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/opt/IntelliJIDEALicenseServer/IntelliJIDEALicenseServer_linux_amd64 -p 1017 -u taowenyin</span><br></pre></td></tr></table></figure>
<h2 id="tomcat9安装过程">Tomcat9安装过程</h2>
<blockquote>
<ul>
<li>1、下载Tomcat，并移动到opt目录下</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget http://mirrors.shu.edu.cn/apache/tomcat/tomcat-9/v9.0.13/bin/apache-tomcat-9.0.13.tar.gz</span><br><span class="line">tar -xzf apache-tomcat-9.0.13.tar.gz</span><br><span class="line">sudo chmod 777 /opt/ -R</span><br><span class="line">sudo mv apache-tomcat-9.0.13 /opt/tomcat</span><br><span class="line">sudo nano /opt/tomcat/conf/tomcat-users.xml</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>2、编辑Tomcat管理用户，及管理权限</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo nano /opt/tomcat9/conf/tomcat-users.xml</span><br></pre></td></tr></table></figure>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">tomcat-users</span> <span class="attr">xmlns</span>=<span class="string">"http://tomcat.apache.org/xml"</span></span></span><br><span class="line"><span class="tag">              <span class="attr">xmlns:xsi</span>=<span class="string">"http://www.w3.org/2001/XMLSchema-instance"</span></span></span><br><span class="line"><span class="tag">              <span class="attr">xsi:schemaLocation</span>=<span class="string">"http://tomcat.apache.org/xml tomcat-users.xsd"</span></span></span><br><span class="line"><span class="tag">              <span class="attr">version</span>=<span class="string">"1.0"</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 设置可以访问管理 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">role</span> <span class="attr">rolename</span>=<span class="string">"manager"</span>/&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 设置可以访问管理员 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">role</span> <span class="attr">rolename</span>=<span class="string">"admin"</span>/&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 设置可以访问管理页面 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">role</span> <span class="attr">rolename</span>=<span class="string">"manager-gui"</span>/&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 设置可以访问管理员页面 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">role</span> <span class="attr">rolename</span>=<span class="string">"admin-gui"</span>/&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 设置可以访问管理页面和管理员页面的账户 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">user</span> <span class="attr">username</span>=<span class="string">"admin"</span> <span class="attr">password</span>=<span class="string">"admin"</span> <span class="attr">roles</span>=<span class="string">"manager-gui,admin-gui,manager-script"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">tomcat-users</span>&gt;</span></span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>3、创建系统级的服务访问</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo nano /etc/systemd/system/tomcat.service</span><br></pre></td></tr></table></figure>
<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line">[Unit]</span><br><span class="line"><span class="attribute">Description</span>=Apache Tomcat Web Application Container</span><br><span class="line"><span class="attribute">After</span>=network.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line"><span class="attribute">Type</span>=forking</span><br><span class="line"></span><br><span class="line"><span class="attribute">Environment</span>=JAVA_HOME=/usr/lib/jvm/java-8-oracle</span><br><span class="line"><span class="attribute">Environment</span>=CATALINA_PID=/opt/tomcat/temp/tomcat.pid</span><br><span class="line"><span class="attribute">Environment</span>=CATALINA_HOME=/opt/tomcat</span><br><span class="line"><span class="attribute">Environment</span>=CATALINA_BASE=/opt/tomcat</span><br><span class="line"><span class="attribute">Environment</span>='CATALINA_OPTS=-Xms512M -Xmx1024M -server -XX:+UseParallelGC'</span><br><span class="line"><span class="attribute">Environment</span>='JAVA_OPTS=-Djava.awt.headless=true -Djava.security.egd=file:/dev/./urandom'</span><br><span class="line"></span><br><span class="line"><span class="attribute">ExecStart</span>=/opt/tomcat/bin/startup.sh</span><br><span class="line"><span class="attribute">ExecStop</span>=/opt/tomcat/bin/shutdown.sh</span><br><span class="line"></span><br><span class="line"><span class="attribute">User</span>=[登录账户]</span><br><span class="line"><span class="attribute">Group</span>=[登录账户组]</span><br><span class="line"><span class="attribute">UMask</span>=0007</span><br><span class="line"><span class="attribute">RestartSec</span>=10</span><br><span class="line"><span class="attribute">Restart</span>=always</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line"><span class="attribute">WantedBy</span>=multi-user.target</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo systemctl daemon-reload</span><br><span class="line">sudo systemctl start tomcat.service</span><br><span class="line">sudo systemctl restart tomcat.service</span><br><span class="line">sudo systemctl <span class="built_in">enable</span> tomcat.service</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>4、修改webapps/manager和webapps/host-manager下的context.xml为Tomcat下的各应用添加远程访问</li>
</ul>
</blockquote>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">Context</span> <span class="attr">antiResourceLocking</span>=<span class="string">"false"</span> <span class="attr">privileged</span>=<span class="string">"true"</span> &gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">Valve</span> <span class="attr">className</span>=<span class="string">"org.apache.catalina.valves.RemoteAddrValve"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">allow</span>=<span class="string">"\d+\.\d+\.\d+\.\d+"</span> /&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">Manager</span> <span class="attr">sessionAttributeValueClassNameFilter</span>=<span class="string">"java\.lang\.(?:Boolean|Integer|Long|Number|String)|org\.apache\.catalina\.filters\.CsrfPreventionFilter\$LruCache(?:\$1)?|java\.util\.(?:Linked)?HashMap"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">Context</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h2 id="mysql5.7远程登录设置">MySQL5.7远程登录设置</h2>
<p>安装MySQL</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget https://dev.mysql.com/get/mysql-apt-config_0.8.11-1_all.deb</span><br><span class="line">sudo dpkg -i mysql-apt-config_0.8.11-1_all.deb</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get -y install mysql-server</span><br></pre></td></tr></table></figure>
<p>编辑MySQL配置文件</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo nano /etc/mysql/mysql.conf.d/mysqld.cnf</span><br></pre></td></tr></table></figure>
<p>在下面行的开头加上#，注释掉该行，然后保存退出：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">bind</span>-address = 127.0.0.1</span><br></pre></td></tr></table></figure>
<p>登录MySQL，执行修改权限指令，并重启服务器：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql -u root -p</span><br><span class="line">grant all privileges on *.* to root@<span class="string">"%"</span> identified by <span class="string">"[root账户密码]"</span> with grant option;</span><br><span class="line">flush privileges;</span><br><span class="line"><span class="built_in">exit</span>;</span><br><span class="line">service mysql restart</span><br></pre></td></tr></table></figure>
<h2 id="为angular开启apache的rewrite功能">为Angular开启Apache的Rewrite功能</h2>
<blockquote>
<ul>
<li>1、在网站根目录下创建文件.htaccess，并填入如下内容</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">RewriteEngine On</span><br><span class="line">    <span class="comment"># If an existing asset or directory is requested go to it as it is</span></span><br><span class="line">    RewriteCond %&#123;DOCUMENT_ROOT&#125;%&#123;REQUEST_URI&#125; -f [OR]</span><br><span class="line">    RewriteCond %&#123;DOCUMENT_ROOT&#125;%&#123;REQUEST_URI&#125; -d</span><br><span class="line">    RewriteRule ^ - [L]</span><br><span class="line">    <span class="comment"># If the requested resource doesn't exist, use index.html</span></span><br><span class="line">RewriteRule ^ /index.html</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>2、开启Apache的Rewrite功能</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo a2enmod rewrite</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>3、修改apache2.conf下所有Directory的AllowOverride设置为All</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;Directory /&gt;</span><br><span class="line">     Options Indexes FollowSymLinks</span><br><span class="line">     AllowOverride All</span><br><span class="line">     Require all denied</span><br><span class="line">&lt;/Directory&gt;</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>4、修改/etc/apache2/sites-available下的000-default.conf中的DocumentRoot为/var/www/html/dist/ElectricIOTWeb</li>
</ul>
</blockquote>
<h2 id="解决labview的ni_application_web_server端口与tomcat冲突的问题8080">解决LabView的Ni_Application_Web_Server端口与Tomcat冲突的问题（8080）</h2>
<p>进入下面的网站，点击Web服务器配置，在HTTP端口中修改相应端口号</p>
<figure class="highlight http"><table><tr><td class="code"><pre><span class="line"><span class="attribute">http://localhost:3582</span></span><br></pre></td></tr></table></figure>
<h2 id="解决labview的mdns_responder_service与intellij_idea_java_web调试端口冲突的问题1099">解决LabView的mDNS_Responder_Service与IntelliJ_IDEA_Java_Web调试端口冲突的问题（1099）</h2>
<p>在Debug设置里面修改JMX端口号，避开1099</p>
<h2 id="查看端口被占用的情况">查看端口被占用的情况</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 获取PID号</span><br><span class="line">netstat -aon|findstr &quot;端口号&quot;</span><br><span class="line"></span><br><span class="line"># 根据PID号查询相应的进程</span><br><span class="line">tasklist|findstr &quot;PID号&quot;</span><br></pre></td></tr></table></figure>
<h2 id="gcc-arm-none-eabi和openocd安装">gcc-arm-none-eabi和OpenOCD安装</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:team-gcc-arm-embedded/ppa</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install gcc-arm-none-eabi</span><br><span class="line">sudo apt-get install openocd</span><br></pre></td></tr></table></figure>
<h2 id="clion安装stm32开发环境">CLion安装STM32开发环境</h2>
<p><a href="https://me.csdn.net/PoJiaA123" target="_blank" rel="noopener">参考链接</a></p>
<blockquote>
<ul>
<li>1、下载STM32CubeMx（注：Ubuntu 16.04只能使用V4.27.0）和CLion软件。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>2、安装gcc-arm-none-eabi和OpenOCD。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>3、在CLion中安装“OpenOCD + STM32CubeMX support for ARM embedded development”插件。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>4、在CLion的Build标签下的OpenOCD选项中设置OpenOCD目录（注：Ubuntu中使用命令行安装的OpenOCD不需要设置）。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>5、使用STM32CubeMx生成SW4STM32项目，但不要打开项目。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>6、使用CLion打开并选择项目（注：一切默认），并在工具栏Tool中现在“Update CMake project with STM32CubeMx project”。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>7、期间会跳出“Board config file”，选择一个和自己MCU相同的板子。</li>
</ul>
</blockquote>
<p><strong>改为C++编程</strong></p>
<blockquote>
<ul>
<li>8、修改main.c为main.cpp。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>9、修改CMakeLists.txt中的PROJECT(HelloSTM32ROS C CXX ASM)为PROJECT(HelloSTM32ROS CXX C ASM)（注：默认实际已经支持C++）。</li>
</ul>
</blockquote>
<h2 id="clion使用openocd进行调试stm32f103">CLion使用OpenOCD进行调试STM32F103</h2>
<blockquote>
<ul>
<li>1、在CLion中选择Board Config File为stm3210b_eval.cfg。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>2、在终端中执行下面的指令，使得OpenOCD与STM32链接。</li>
</ul>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">openocd -f /usr/share/openocd/scripts/interface/stlink-v2.cfg -f /usr/share/openocd/scripts/target/stm32f1x.cfg</span><br></pre></td></tr></table></figure>
<h2 id="ubuntu下安装stlink">Ubuntu下安装STLink</h2>
<blockquote>
<ul>
<li>1、准备工作。</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install libusb-1.0-0</span><br><span class="line">sudo apt-get install libgtk-3-dev</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>2、下载并安装STLink</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/texane/stlink.git</span><br><span class="line"><span class="built_in">cd</span> stlink/</span><br><span class="line">make release </span><br><span class="line">make debug </span><br><span class="line"><span class="built_in">cd</span> build/</span><br><span class="line">cmake -DCMAKE_BUILD_TYPE=Debug ..</span><br><span class="line">make</span><br><span class="line"><span class="built_in">cd</span> Release/</span><br><span class="line">sudo make install</span><br><span class="line">sudo ldconfig</span><br></pre></td></tr></table></figure>
<h2 id="解决stm32cubemx生成的代码不能用openocd和st-link调试">解决STM32CubeMX生成的代码不能用OpenOCD和ST-Link调试</h2>
<p><strong>原因：</strong> 由于STM32CubeMX默认生成的代码没有开启调试，使得OpenOCD、Keil等软件都不能进行仿真</p>
<blockquote>
<ul>
<li>1、在SYS的Debug选项中选择“Trace Asynchronous Sw“</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>2、用烧录软件重新烧录（可能的方法，在烧录之前把Boot0和Boot1拉高，烧录完成后再拉低）</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>3、就可以进行调试</li>
</ul>
</blockquote>
<h2 id="opencv3.4.1">OpenCV(&gt;=3.4.1)</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mkdir build</span><br><span class="line"><span class="built_in">cd</span> build</span><br><span class="line">cmake -DCMAKE_BUILD_TYPE=RELEASE -DCMAKE_INSTALL_PREFIX=/usr/<span class="built_in">local</span> -DINSTALL_PYTHON_EXAMPLES=ON -DINSTALL_C_EXAMPLES=ON -DOPENCV_EXTRA_MODULES_PATH=../../opencv_contrib-3.4.2/modules -DPYTHON3_EXECUTABLE=/usr/bin/python3.5 -DPYTHON3_INCLUDE_DIR=/usr/include/python3.5 -DPYTHON3_INCLUDE_DIR2=/usr/include/x86_64-linux-gnu/python3.5m -DPYTHON3_LIBRARY=/usr/lib/x86_64-linux-gnu/libpython3.5m.so -DPYTHON3_NUMPY_INCLUDE_DIRS=/usr/<span class="built_in">local</span>/lib/python3.5/dist-packages/numpy/core/include/ -DPYTHON2_EXECUTABLE=/usr/bin/python3.5 -DPYTHON2_INCLUDE_DIR=/usr/include/python3.5 -DPYTHON2_INCLUDE_DIR2=/usr/include/x86_64-linux-gnu/python3.5m -DPYTHON2_LIBRARY=/usr/lib/x86_64-linux-gnu/libpython3.5m.so -DPYTHON2_NUMPY_INCLUDE_DIRS=/usr/<span class="built_in">local</span>/lib/python3.5/dist-packages/numpy/core/include/ -DBUILD_EXAMPLES=ON ..</span><br><span class="line">make;sudo make install</span><br><span class="line">sudo gedit /etc/ld.so.conf.d/opencv.conf</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>填入/usr/local/lib</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo ldconfig</span><br><span class="line">sudo gedit /etc/bash.bashrc</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>填入PKG_CONFIG_PATH=$PKG_CONFIG_PATH:/usr/local/lib/pkgconfig和export PKG_CONFIG_PATH</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">source</span> /etc/bash.bashrc</span><br><span class="line">sudo updatedb</span><br></pre></td></tr></table></figure>
<h2 id="qt环境配置">QT环境配置</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export QTDIR=/opt/Qt/Qt5.12.1/5.12.1/gcc_64</span><br><span class="line">export PATH=$QTDIR/bin:$PATH</span><br><span class="line">export LD_LIBRARY_PATH=$QTDIR/lib:$LD_LIBRARY_PATH</span><br></pre></td></tr></table></figure>
<h2 id="添加最新的boost库">添加最新的Boost库</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:mhier/libboost-latest</span><br><span class="line">sudo apt-get update</span><br></pre></td></tr></table></figure>
<h2 id="虚拟串口的使用">虚拟串口的使用</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">python3 VirtualSerialPort.py</span><br></pre></td></tr></table></figure>
<h2 id="设置vscode自动格式化">设置VSCode自动格式化</h2>
<blockquote>
<ul>
<li>1、“Format On Save”设为true</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>2、“Detect Indentation”设为false</li>
</ul>
</blockquote>
<h2 id="sw4stm32项目转化为c">SW4STM32项目转化为C++</h2>
<blockquote>
<ul>
<li>1、右键项目，选择“Convert to C++”</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>2、修改main.c为main.cpp</li>
</ul>
</blockquote>
<h2 id="修改github的ssh端口解决22端口超时问题">修改GitHub的SSH端口解决22端口超时问题</h2>
<blockquote>
<ul>
<li>1、在id_rsa和id_rsa.pub创建config文件</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>2、写入如下内容</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Host github.com</span><br><span class="line">User wenyin.tao@163.com</span><br><span class="line">Hostname ssh.github.com</span><br><span class="line">PreferredAuthentications publickey</span><br><span class="line">IdentityFile ~/.ssh/id_rsa</span><br><span class="line">Port 443</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>3、修改GitHub端口</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ssh -T git@github.com</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>资源</category>
      </categories>
  </entry>
  <entry>
    <title>【2019】DeepGBM：A Deep Learning Framework Distilled by GBDT for Online Prediction Tasks</title>
    <url>/deepgbm/</url>
    <content><![CDATA[<h1 id="摘要">0.摘要</h1><p>在现实应用中，在线预测是最基本的任务。典型的在线预测任务具备两个特征，分别是表格式的输入空间和在线数据生成。具体说，就是表格式输入空间表示既有稀疏分类特征也有稠密的数值特征，而在线数据生成则是具有连续生成具有动态分布数据的任务。因此，利用表格输入空间进行有效学习和快速适应在线生成的数据是获取在线预测模型的两个重要挑战。虽然采用GBDT和NN已经在实践中被广泛使用，当时它们都有其自身的缺点。特别地，GBDT很难适应动态生成的在线数据，并且在面对稀疏的分类特征时往往是无效的；另一方面，在面对稠密的数值特征时，NN很难获取令人满意的性能。在这篇文章中，作者提出了一个新的学习框架——DeepGBM，该框架集成了NN和GBDT的优点并提出了两个新的NN组件：</p><a id="more"></a>

<p>1、CatNN：专注于处理稀疏的分类特征。</p>
<p>2、GBDT2NN：通过GBDT对稠密数值特征提取。</p>
<p>在这两个组件的支持下，DeepGBM可以同时利用分类和数值特性，同时保持高效在线更新的能力。</p>
<img src="/deepgbm/deepgbm.png" class title="DeepGBM框架">
<h1 id="介绍">1.介绍</h1>
<p>在线预测在许多实际工业应用中是非常重要的一类任务，如广告搜索中的点击预测、Web搜索中的内容排序、推荐系统中的内容优化、交通规划中的行程时间估计等。</p>
<p>一个典型的在线预测任务中通常具有两个基本特征，即表格式的输入空间和在线数据生成。其中，表格式的输入空间意味着在线预测任务的输入特征可以包括分类和表格数值特征。例如，在广告点击预测任务的特征空间中通常包含广告类别等分类特征空间，以及查询与广告文本相似性等数值特征空间，在线数据生成意味着这些任务的实际数据是以在线方式生成，并且数据分布是实时动态的。例如，在新闻推荐系统中可以实时生成大量的数据，并且在不断涌现的新闻中可以在不同的时间内产生动态的特征分布。</p>
<p>因此，要为在线预测任务寻求有效的基学习器模型，就必须解决两个主要挑战：</p>
<p>1、在表格式输入空间中如何学习有效的模型？</p>
<p>2、如何适配在线数据生成的模型？</p>
<p>目前，两类机器学习模型被广泛应用于在线预测任务，分别时GBDT和NN。不幸的是，这两个方法都不能同时很好的应对上面的两个挑战。换言之，在解决在线预测任务时，GBDT或NN都会产生各自的优缺点。</p>
<img src="/deepgbm/different-models.png" class title="不同模型的比较">
<h2 id="gbdt优缺点">1.1GBDT优缺点</h2>
<p>1、GBDT的主要优势在于它能够有效地处理密集的数值特征。因为，GBDT可以在每次迭代中选取信息增益最大的特征来构建树，因此它可以自动地选择和组合有用的数值特征，以更好地适应训练目标。这就是为什么GBDT在点击预测、Web搜索排名和其他公认的预测任务中展示了它的有效性。当时，GBDT在在线预测任务中有两个主要的弱点。首先，由于GBDT中<strong>学习到的树是不可微的，所以在联机模式下更新GBDT模型是很困难的。</strong>从0开始的不断训练使得GBDT在学习在线预测任务时效率很低，这一弱点阻碍了GBDT对超大规模数据的学习，<strong>因为将大量数据加载到内存中进行学习通常是不切实际的。</strong></p>
<p>2、GBDT无法在稀疏分类特征上进行学习。特别是将分类特征转换成稀疏、高维的one-hot编码后，稀疏特征的信息增益将变得非常小，<code>因为在稀疏特征情况下的具有不平衡增益的分割与不分割几乎相同。</code>因此，GBDT就不能利用稀疏特征来有效的构建决策树。尽管有一些分类编码方式可以直接将分类值转换为稠密的数值，但在对不同分类进行编码转换时，由于很多编码值可能非常相似，当时系统并不知道，所以就会造成与原始数据之间的误差。通过列举可能的二元分类，分类特征也可以直接用于决策树的学习，然而这种方法在分类特征稀疏的情况下，由于每类数据太少，因此统计信息会有误差，往往就会造成训练数据的过拟合。简而言之，虽然GBDT对稠密的数值特征具有较好的学习性，但还是有两个明显缺点，即难以适应在线数据生成和对于稀疏分类特征学习的无效性，这导致GBDT在许多在线预测任务中失败，特别是那些需要在线调整模型和包含许多稀疏分类特征的模型。</p>
<h2 id="nn优缺点">1.2NN优缺点</h2>
<p>神经网络的优势在于通过采用<strong>批处理模式的反向传播算法和公认的对稀疏分类特征具有较好学习性的嵌套结构</strong>对在线任务中的大规模数据学习的有效性。最近的一些研究表明，在如点击预测和推荐系统等在线预测任务中使用神经网络得到了成功的应用。然而，神经网络的主要挑战在于它在学习稠密的数值表格特征方面较弱。全连接神经网络（FCNN）虽然可以直接用于稠密的数值特征，但由于其全连接的模型结构导致了非常复杂的优化超平面方面很容易陷入局部最优，因此常常性能不理想。因此，在许多具有稠密数值表特征的任务中，神经网络性能不比GBDT好。综上所述，尽管神经网络能够有效地处理稀疏的分类特征，并且适应在线数据生成，但是对于稠密的数值表格特征学习仍然很难得到有效的模型。</p>
<p>本文提出的DeepGBM框架主要包含了两个组件：</p>
<p>1、CatNN：接收分类特征的NN结构。</p>
<p>2、GBDT2NN：接收数值特征的NN结构。</p>
<p>为了充分利用GBDT在学习数值特征方面的优势，GBDT2NN把GBDT学习到的内容转化为神经网络的建模过程。具体来说，为了提高知识提取的有效性，GBDT2NN不仅传递了预先训练的GBDT知识，而且还融合了所得到的树结构中隐含的特征重要性和数据划分知识。这样，在达到与GBDT相当的性能的同时，采用神经网络结构的GBDT2NN在面对在线数据生成时，可以很容易地通过不断涌现的数据进行模型更新。</p>
<p>DeepGBM由两个基于神经网络的组件CatNN和GBDT2NN组成，在保持高效在线学习的重要能力的同时，可以在类别特征和数值特征上具有较强的学习能力。本文的三个主要贡献：</p>
<p>1、结合GBDT和NN的优点提出了DeepGBM方法，在保留有效在线更新模型能力的同时，利用分类和数值两种特征，对各种具有表格数据的预测任务进行处理。</p>
<p>2、通过考虑GBDT模型学习树中输入选择、结构和输出，提出了一种将GBDT模型中学到的知识提取为神经网络模型的方法。</p>
<p>3、实验表明DeepGBM是一个成熟的模型，已经能够被用于各类预测任务中，并且具有较好的性能。通过枚举可能的二分类，分类特征也可以直接用于树学习，但是这种方法在分类特征稀疏的情况下会造成训练数据过度拟合。</p>
<h1 id="相关工作">2.相关工作</h1>
<h2 id="在在线预测任务中应用gbdt">2.1在在线预测任务中应用GBDT</h2>
<p>1、在线更新树模型：XGBoost、LightGBM等对在线更新树模型提供了简单的方法，它们保持树结构不变，并通过新数据更新叶输出，但是这种方法的性能离预期的太远。此外，还有人提出了当有新数据时，就重新查找树的分割点，但是由于忽略的历史数据，因此性能也不稳定。</p>
<p>2、树中的分类特征：为了方便决策树能更好的处理它们，通过一些编码方法将稀疏的分类值转换为稠密的数值。CatBoost使用类似数字编码来对分类特征编码，但是它会引起信息损失。</p>
<h2 id="在在线预测任务中应用nn">2.2在在线预测任务中应用NN</h2>
<p>由于FCNN中超平面的优化非常复杂，容易陷入局部最优解，即使采用了归一化和正则化技术，但是对于具有稠密数值特征的数据来说FCNN的性能并不优于GBDT。另一种是把稠密的数值特征进行离散化，形成分类的形式，从而可以更好地处理特征分类的相关工作。但是由于输出仍将需要连接到完全连接层，因此离散化实际上无法提高处理数值特征的效率，并且离散化会增加模型的复杂度，并且由于模型参数的增加而导致过拟合。</p>
<h2 id="组合nn和gbdt">2.3组合NN和GBDT</h2>
<p>1、Tree-link NN（像决策树的神经网络）：像GoogLeNet在一定程度上具有决策树的特点。但这些模型都主要面向机器视觉，而不是具有数据表输入的在线预测任务。还有如NNRF算法，该算法利用树状神经网络和随机特征选择来提高表格数据学习的效果，但是NNRF只使用随机特征组合，而不利用GBDT等训练数据的统计信息。</p>
<p>2、Convert Trees to NN（把决策树转化为神经网络）：这些工作通常效率很低，因为它们使用冗余且非常稀疏的神经网络来表示简单的决策树。当有许多树时，这种转换方案必须构造一个非常宽的神经网络来表示它们，这很难得到实际应用。</p>
<p>3、Combining NN and GBDT（组合NN和GBDT）：Facebook使用叶指数预测作为Logistic回归的输入分类特征。微软用GBDT拟合神经网络的残差。然而，由于GBDT中的在线更新模型问题没有得到解决，因此这些工作无法有效地使用。Facebook在其中也指出了这个问题，因为他们框架中的GBDT模型需要每天重新训练，才能获得良好的在线性能。</p>
<h1 id="deepgbm">3.DeepGBM</h1>
<p>CatNN是处理分类特征的NN结构，GBDT2NN是处理稠密数值特征的NN结构。</p>
<h2 id="处理稀疏分类特征的catnn">3.1处理稀疏分类特征的CatNN</h2>
<p>为了解决在线预测问题，NN已经被广泛的应用于学习分类特征的预测模型，如Wide&amp;Deep、PNN、DeepFM、xDeepFM。由于CATNN的目标是与这些模型的目标相同，因此可以直接把现有的神经网络结构应用在CatNN中。CatNN于前期的研究相似，主要使用嵌套技术（Embedding Technology）把高维的稀疏向量转化为稠密向量。此外，作者还利用前面研究得到的FM组件和Deep组件来学习特征之间的相互作用，并且CatNN并不限制于这两个组件，它可以使用任何的其他类似NN组件。</p>
<p>嵌套技术（Embedding Technology）是通过低维稠密数据来表示高维稀疏向量的一种方法：</p>
<p><span class="math display">\[\begin{equation}
    E_{V_{i}}\left(x_{i}\right)=\text { embedding_lookup}\left(V_{i}, x_{i}\right)
\end{equation}\]</span></p>
<p>其中<span class="math inline">\(x_{i}\)</span>是第<span class="math inline">\(i\)</span>维特征的值，<span class="math inline">\(V_{i}\)</span>存储了第<span class="math inline">\(i\)</span>个特征所有嵌套（embeddings）并且能够被用于反向传播，而<span class="math inline">\(E_{V_{i}}\left(x_{i}\right)\)</span>将返回<span class="math inline">\(x_{i}\)</span>对应的嵌套向量。基于此，作者使用FM组件学习1阶线性函数和2阶特征之间的相互作用：</p>
<p><span class="math display">\[\begin{equation}
    y_{FM}(x)=w_{0}+\langle w, x\rangle+\sum_{i=1}^{d} \sum_{j=i+1}^{d}\left\langle E_{V_{i}}\left(x_{i}\right), E_{V_{j}}\left(x_{j}\right)\right\rangle x_{i} x_{j}
\end{equation}\]</span></p>
<p>其中<span class="math inline">\(d\)</span>表示特征的数量，<span class="math inline">\(w_{0}\)</span>和<span class="math inline">\(w\)</span>是线性部分的参数<span class="math inline">\(\left \langle \cdot , \cdot \right \rangle\)</span>是内积操作。</p>
<p>然后，使用Deep组件来学习高阶特征之间的相互作用。</p>
<p><span class="math display">\[\begin{equation}
    y_{Deep}(x)=\mathcal{N}\left(\left[E_{V_{1}}\left(x_{1}\right)^{T}, E_{V_{2}}\left(x_{2}\right)^{T}, \ldots, E_{V_{d}}\left(x_{d}\right)^{T}\right]^{T} ; \theta\right)
\end{equation}\]</span></p>
<p>其中<span class="math inline">\(\mathcal{N}\left ( x, \theta \right )\)</span>表示输入为<span class="math inline">\(x\)</span>，参数为<span class="math inline">\(\theta\)</span>的多层NN网络模型。通过组合这FM和Deep组件，CatNN网络最终的输出为：</p>
<p><span class="math display">\[\begin{equation}
    y_{Cat}\left ( x \right )=y_{FM}\left ( x \right ) + y_{Deep}\left ( x \right )
\end{equation}\]</span></p>
<h2 id="处理稠密数值空间的gbdt2nn">3.2处理稠密数值空间的GBDT2NN</h2>
<p>本节首先介绍如何把一棵树提取为NN，然后推广这个想法至在GBDT中提取多棵树。</p>
<h3 id="提取一棵树">3.2.1提取一棵树</h3>
<p>之前的树提取工作大多依赖于已学习到的函数来进行模型知识的转换，从而使得新生成的模型与原模型相似。由于树与神经网络的本质不同，因此除了传统的模型提取方法外，树模型中的更多知识也可以被提取并转化为神经网络。例如树中的特征选择和权重，以及树结构中所隐含的数据划分，都是树中的重要知识。</p>
<p><strong>1、树特征选择：</strong>与神经网络相比，树模型的一个特点是不会使用所有的输入特征，因为它的学习会根据统计信息进行选择适合训练目标的特征。因此，我们可以根据树选择的特征来传递这些信息，以提高神经网络模型的学习效率，而不是使用所有的输入特征。形式上可以定义<span class="math inline">\(\mathbb{I}^{t}\)</span>作为树<span class="math inline">\(t\)</span>中已选择特征的指示器，然后只需要使用<span class="math inline">\(x\left [ \mathbb{I}^{t} \right ]\)</span>作为NN的输入。</p>
<p><strong>2、树结构：</strong>决策树的结构其实质上是将数据划分为多个不重叠的区域（叶子），即将数据聚类成不同的类，同一叶中的数据属于同一类。由于神经网络和决策树的本质不同，因此把直接把决策树转化为神金网络并不容易。幸运的是，由于神经网络已被证明能够逼近任何函数，所以我们可以使用神经网络模型来逼近树结构的函数，并实现结构信息的提取。如图2所示，可以使用神经网络来拟合树生成的聚类结果，从而使神经网络逼近决策树的结构函数。形式上将树结构函数<span class="math inline">\(t\)</span>表示为<span class="math inline">\(C^{t}\left ( x \right )\)</span>，该函数返回样本<span class="math inline">\(x\)</span>输出的叶索引，即树生成的聚类结果。然后使用神经网络模型来逼近结构函数<span class="math inline">\(C^{t}\left ( \cdot \right )\)</span>，学习过程可以表示为：</p>
<p><span class="math display">\[\begin{equation}
    \underset{\theta}{min}\frac{1}{n}\sum_{i=1}^{n}{\mathcal{L}}&#39;\left ( \mathcal{N}\left ( x^{i}\left [ \mathbb{I}^t \right ]; \theta \right ),L^{t,i} \right )
\end{equation}\]</span></p>
<p>其中<span class="math inline">\(n\)</span>表示训练样本的个数，<span class="math inline">\(x^{i}\)</span>表示是第<span class="math inline">\(i\)</span>个训练样本，<span class="math inline">\(L^{t,i}\)</span>是第<span class="math inline">\(x^{i}\)</span>样本以one-hot形式表示的叶索引<span class="math inline">\(C^{t}\left ( x^{i} \right )\)</span>，<span class="math inline">\(\mathbb{I}^{t}\)</span>是当前树<span class="math inline">\(t\)</span>中所用到特征的索引，<span class="math inline">\(\theta\)</span>是神经网络模型<span class="math inline">\(\mathcal{N}\)</span>的参数，并且可以通过方向传播进行更新，<span class="math inline">\({\mathcal{L}}&#39;\)</span>是多分类问题的损失函数，如交叉熵。因此，通过学习就可以得到一个网络模型<span class="math inline">\(\mathcal{N}\left ( \cdot ; \theta \right )\)</span>。由于神经网络具有很强的表达能力，学习的神经网络模型会完全逼近决策树的结构函数。</p>
<img src="/deepgbm/nn-approximate-tree.png" class title="神经网络逼近决策树">
<p><strong>3、树输出：</strong>在前面的步骤中学习了从输入到树结构的映射，所以要获取树的输出，只需要知道从树结构到树输出的映射。然而，由于每个叶索引都有对应的叶值，所以该映射并不需要学习。因此，可以将树<span class="math inline">\(t\)</span>的叶值表示为<span class="math inline">\(q^{t}\)</span>，<span class="math inline">\(q_{i}^{t}\)</span>表示第i个叶的叶值。因此就可以通过<span class="math inline">\(p^{t}=L^{t} \times q^{t}\)</span>把<span class="math inline">\(L^{t}\)</span>映射到树输出。</p>
<p>结合上面的方法，从<span class="math inline">\(t\)</span>树获取得到的神经网络的输出可以表示为：</p>
<p><span class="math display">\[\begin{equation}
    y^{t} \left ( x \right ) = \mathcal{N}\left ( x\left [ \mathbb{I}^{t}; \theta \right ] \right ) \times q^{t}
\end{equation}\]</span></p>
<h3 id="多树的提取">3.2.2多树的提取</h3>
<p>要把单树提取的思想推广到GBDT中，一个直接的方法就是把有多少树就有多少神经网络，并且一一对应（<span class="math inline">\(\#NN=\#tree\)</span>）。但是结构提取的目标（<span class="math inline">\(O\left ( \left | L \right | \times \#NN \right )\)</span>）具有高维性，因此该方法的效率很低。为了提高效率，提出了叶嵌套提取法（Leaf Embedding Distillation）和树分组法（Tree Grouping）来分别对<span class="math inline">\(\left | L \right |\)</span>和<span class="math inline">\(\#NN\)</span>进行降维。</p>
<p><strong>1、叶嵌套提取法（Leaf Embedding Distillation）：</strong>如图3所示，作者采用嵌套技术来降低结构提取目标<span class="math inline">\(L\)</span>的维数，同时对信息进行重新训练。更具体地说，由于叶指数和叶值之间存在双射关系，因此使用叶值来学习嵌套，从形式上说，这个学习的过程可以表示为：</p>
<p><span class="math display">\[\begin{equation}
    \underset{w,w_{0},\omega^{t}}{min}\frac{1}{n}\sum_{i=1}^{n}{\mathcal{L}}&#39;&#39;\left ( w^{T}\mathcal{H}\left ( L^{t,i};\omega^{t} \right ) + w_{0}, p^{t,i} \right )
\end{equation}\]</span></p>
<p>其中<span class="math inline">\(H^{t,i} = \mathcal{H}\left ( L^{t,i};\omega^{t} \right )\)</span>是一个具有参数<span class="math inline">\(\omega^{t}\)</span>的单层全连通网络，它将一个one-hot叶索引<span class="math inline">\(L^{t,i}\)</span>转换为稠密嵌套的<span class="math inline">\(H^{t,i}\)</span>，<span class="math inline">\(p^{t,i}\)</span>是样本<span class="math inline">\(x^{i}\)</span>的叶预测值，<span class="math inline">\({\mathcal{L}}&#39;&#39;\)</span>是与树中相同的损失函数，<span class="math inline">\(w\)</span>和<span class="math inline">\(w_{0}\)</span>是将嵌套映射为叶值的参数。这种方法，作者使用了嵌套式的稠密数据作为目标来逼近树结构函数，从而不是使用稀疏高维on-hot数据<span class="math inline">\(L\)</span>，其学习过程可以表示为：</p>
<p><span class="math display">\[\begin{equation}
    \min _{\boldsymbol{\theta}} \frac{1}{n} \sum_{i=1}^{n} \mathcal{L}\left(\mathcal{N}\left(\boldsymbol{x}^{i}\left[\mathbb{I}^{t}\right] ; \boldsymbol{\theta}\right), \boldsymbol{H}^{t, i}\right)
\end{equation}\]</span></p>
<p>其中<span class="math inline">\(\mathcal{L}\)</span>表示拟合<code>嵌套式稠密（Dense Embedding）</code>的类似于L2回归损失函数。由于<span class="math inline">\(H^{t, i}\)</span>的维数要比<span class="math inline">\(L\)</span>小得多，因此嵌套式叶提取在多树提取中更为有效，并且该方法使用更少的神经网络参数。</p>
<img src="/deepgbm/leaf-embedding-distillation.png" class title="叶嵌套提取法">
<p><strong>2、树分组法（Tree Grouping）：</strong>为了降低神经网络的纬度，作者把树进行了分组，并且使用一个神经网络从一组树中提取神经网络模型，但是分组存在两个问题，即如何对树进行分组和如何从一组树中提取神经网络模型，其步骤如下：</p>
<p>（1）对于分组策略已经有了许多方法，例如平均随机分组、按顺序分组、基于重要性或相似性分组等，在本文中，作者使用平均随机分组。假设有<span class="math inline">\(m\)</span>个树，并且把这些数分为<span class="math inline">\(k\)</span>组，那么每组中的树就有<span class="math inline">\(s=\left \lceil \frac{m}{k} \right \rceil\)</span>，并且第<span class="math inline">\(j\)</span>树分组为<span class="math inline">\(\mathbb{T}_{j}\)</span>，它包含了来自GBDT中的随机<span class="math inline">\(s\)</span>个树。</p>
<p>（2）为了从多棵树中提取模型，作者扩展多棵树的叶嵌套提取法。其形式为，假如某个树分组为<span class="math inline">\(\mathbb{T}\)</span>，那么就扩展式子7从多棵树中学习的叶嵌套。</p>
<p><span class="math display">\[\begin{equation}
    \underset{w,w_{0},\omega^{T}}{min}\frac{1}{n}\sum_{i=1}^{n}{\mathcal{L}}&#39;&#39;\left ( w^{T}\mathcal{H}\left ( \parallel_{t \in \mathbb{T}}\left ( L^{t,i} \right );\omega^{\mathbb{T}} \right ) + w_{0}, \underset{t \in \mathbb{T}}{\sum}p^{t,i} \right )
\end{equation}\]</span></p>
<p>其中<span class="math inline">\(\parallel_{t \in \mathbb{T}}\)</span>是一个链接操作，<span class="math inline">\(G^{\mathbb{T},i} = \mathcal{H}\left ( \parallel_{t \in \mathbb{T}}\left ( L^{t,i} \right );\omega^{\mathbb{T}} \right )\)</span>是一个单层的全连接神经网络用于转换多个ont-hot向量，即为了在<span class="math inline">\(\mathbb{T}\)</span>中使用嵌套式稠密数据<span class="math inline">\(G^{\mathbb{T},i}\)</span>，因此连接多个one-hot叶索引向量。然后将新的嵌套作为神经网络模型的提取目标，其学习过程可以表示为：</p>
<p><span class="math display">\[\begin{equation}
    \mathcal{L}^{\mathbb{T}}=\min _{\boldsymbol{\theta}^{\mathbb{T}}} \frac{1}{n} \sum_{i=1}^{n} \mathcal{L}\left(\mathcal{N}\left(x^{i}\left[\mathbb{I}^{\mathbb{T}}\right] ; \boldsymbol{\theta}^{\mathbb{T}}\right), G^{\mathbb{T}, i}\right)
\end{equation}\]</span></p>
<p>其中<span class="math inline">\(\mathbb{I}^{\mathbb{T}}\)</span>是在树分组<span class="math inline">\(\mathbb{T}\)</span>中所使用的特征集。当树分组<span class="math inline">\(\mathbb{T}\)</span>中树的数量很大时，<span class="math inline">\(\mathbb{I}^{\mathbb{T}}\)</span>可能会包含很多的特征，从而影响特征选择，因此，可以只能根据特征重要性在中使用最重要的特征。综上所述，从树分组<span class="math inline">\(\mathbb{T}\)</span>中提取的神经网络最终输出为：</p>
<p><span class="math display">\[\begin{equation}
    y_{\mathbb{T}}(\boldsymbol{x})=\boldsymbol{w}^{T} \times \mathcal{N}\left(\boldsymbol{x}\left[\mathbb{I}^{\mathbb{T}}\right] ; \theta^{\mathbb{T}}\right)+w_{0}
\end{equation}\]</span></p>
<p>并且包含<span class="math inline">\(k\)</span>个树分组的GBDT模型输出为：</p>
<p><span class="math display">\[\begin{equation}
    y_{GBDT2NN}(x)=\sum_{j=1}^{k} y_{\mathbb{T}_{j}}(x)
\end{equation}\]</span></p>
<p>综上所述，由于使用了叶嵌套提取法和树分组法，GBDT2NN可以有效地将GBDT中的许多树提取为一个紧凑的神经网络模型。此外，除了树的输出，树的特征选择和结构信息也被有效地提取到神经网络模型中。</p>
<h2 id="训练deepgbm">3.3训练DeepGBM</h2>
<h3 id="离线状态下端到端训练">3.3.1离线状态下端到端训练</h3>
<p>为了训练DeepGBM，首先需要使用离线数据训练GBDT模型，然后使用式子9得到GBDT中树的嵌套叶。然后就进行端到端的DeepGBM训练。DeepGBM的输出可以表示为：</p>
<p><span class="math display">\[\begin{equation}
    \hat{y}(\boldsymbol{x})=\sigma^{\prime}\left(w_{1} \times y_{GBDT2NN}(\boldsymbol{x})+w_{2} \times y_{Cat}(\boldsymbol{x})\right)
\end{equation}\]</span></p>
<p>其中<span class="math inline">\(w_{1}\)</span>和<span class="math inline">\(w_{2}\)</span>是GBDT2NN和CatNN结合后需要训练的参数，<span class="math inline">\(\sigma^{\prime}\)</span>是转换的输出，类似于二分类的<span class="math inline">\(sigmoid\)</span>函数。最后就可以使用下面这个端到端的损失函数训练：</p>
<p><span class="math display">\[\begin{equation}
    \mathcal{L}_{offline}=\alpha \mathcal{L}^{\prime \prime}(\hat{y}(x), y)+\beta \sum_{j=1}^{k} \mathcal{L}^{\mathbb{T}_{j}}
\end{equation}\]</span></p>
<p>其中<span class="math inline">\(y\)</span>是样本<span class="math inline">\(x\)</span>的训练目标，<span class="math inline">\(\mathcal{L}^{\prime \prime}\)</span>是一个类似于分类任务中的交叉熵损失函数，<span class="math inline">\(\mathcal{L}^{\mathbb{T}}\)</span>是在式子10中的树组<span class="math inline">\(\mathbb{T}\)</span>的嵌套损失函数。<span class="math inline">\(k\)</span>是树组的个数，<span class="math inline">\(\alpha\)</span>和<span class="math inline">\(\beta\)</span>是预先给定的超参数，分别用于控制端到端损失和嵌套损失的强度。</p>
<h3 id="在线模型更新">3.3.2在线模型更新</h3>
<p>由于GBDT模型是离线训练的，因此在在线更新中的嵌套学习中使用GBDT模型会影响在线实时性。因此，在在线模型更新中并不包括更新<span class="math inline">\(\mathcal{L}^{\mathbb{T}}\)</span>，同时在线更新模型的损失函数为：</p>
<p><span class="math display">\[\begin{equation}
    \mathcal{L}_{online}=\mathcal{L}^{\prime \prime}(\hat{y}(\mathbf{x}), y)
\end{equation}\]</span></p>
<p>这个只能用于计算端到端的损失。因此，当DeepGBM在线时，只需要通过<span class="math inline">\(\mathcal{L}_{online}\)</span>来更新模型的新数据，而不需要把GBDT和训练过程进行再次训练。综上所述，DeepGBM能非常有效地执行在线任务，并且它还可以很好地处理稠密数值特征和稀疏分类特征。</p>
]]></content>
      <categories>
        <category>论文</category>
      </categories>
      <tags>
        <tag>集成学习</tag>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>【2019】LightMC：A Dynamic And Efficient Multiclass Decomposition Algorithm</title>
    <url>/lightmc/</url>
    <content><![CDATA[<h1 id="摘要">0.摘要</h1><p>解决多分类问题目前的做法有三种，分别是OVA（一对多）、OVO（一对一）和ECOC（纠错输出码），其中OVA和OVO非常简单，但是忽略了类与类之间的关系，而ECOC虽然考虑了类与类之间的关系，但是其性能取决与编码矩阵和解码策略，而要通过找到一个合适的解码策略来发现一个有效的编码矩阵是非常耗时和不确定的。因此，作者提出了一个高效的动态多类分解算法LightMC。该算法不同于以往的固定编码矩阵和解码策略，LightMC采用可微的解码策略，在每次迭代中通过反向传播来训练基础学习器，使其能够动态地优化编码矩阵和解码策略，从而提高多分类的总体精度。</p><a id="more"></a>

<h1 id="介绍">1.介绍</h1>
<p>多分类问题是把一个数据集分为三个或者更多的分类。在一个标准的多分类学习过程中，一共有<span class="math inline">\(K &gt; 2\)</span>个数据类（例如：<span class="math inline">\(Y=\left \{ C_{1}, C_{2}, \cdots , C_{K} \right \}\)</span>），有<span class="math inline">\(n\)</span>个训练集（例如：<span class="math inline">\(S=\left \{ \left \{ x_{1}, y_{1} \right \}, \left \{ x_{2}, y_{2} \right \}, \cdots , \left \{ x_{n}, y_{n} \right \} \right \}\)</span>），每个训练实例都除以<span class="math inline">\(K\)</span>个分类中的一个，并且还有分类器函数<span class="math inline">\(f\left ( x \right )\)</span>，当有一个新实例<span class="math inline">\(x\)</span>时，可以预测该实例属于哪个分类。</p>
<p>目前多分类最热的方法就是多类分解方法，即是将多分类问题变为一系列互不相关的二分类问题，然后把这些二分类问题进行重新组合，从而解决原始的多分类问题。</p>
<p><strong>本篇论文的主要贡献：</strong></p>
<p>1、作者提出的动态分解算法LightMC在输出精度和效率上都优于传统的ECOC。</p>
<p>2、作者定义了一个可微的解码策略，并通过已知的反向传播算法，得到了一种动态优化编码矩阵的有效算法。</p>
<p>3、通过对多个公共大数据集的大量实验分析，证明了新分解算法的有效性和高效性。</p>
<h2 id="ova和ovo">1.1OVA和OVO</h2>
<p>1、OVA：训练K个不同的基学习器，当第<span class="math inline">\(i\)</span>个学习在所有实例中学得的正例就属于<span class="math inline">\(C_{i}\)</span>，否则就不属于<span class="math inline">\(C_{i}\)</span>。</p>
<p>2、OVO：训练<span class="math inline">\(\frac{K \times \left ( K - 1 \right )}{2}\)</span>个不同的基学习器，每一个学习器用于区分两个分类。</p>
<p>OVA和OVO虽然简单，并且也被广泛使用，但是缺陷也很明显，即忽略了类和类之间的关联，例如“Kitty”分类与“猫”的类分的关联性就远大于与“狗”分类的关联性。因此采用OVA和OVA的训练时就不能利用分类之间的关联性降低分类的成本，并提高分类的精度，因此存在低效，且计算复杂度高的问题。同时，当K很大时，且处理大型分类数据时，会导致极高的训练成本。</p>
<h2 id="ecoc">1.2ECOC</h2>
<p>ECOC优于OVA和OVO，一定程度上解决了OVA和OVO中类之间完全独立的问题。ECOC依赖于一个编码矩阵，该编码矩阵为实例定义了一个转换标签，从而把多分类问题变为二分类问题，然后通过去相关和纠错的方式进行重新组合。在ECOC中，通过为不同的分类对生成不同的距离，使得分类之间的相关性能够应用到整个学习过程中。假如“猫”、“Kitty”、“狗”对应的编码矩阵为（1,1,1）、（1,1,-1）和（-1,-1,-1），那么学习模型就可以保证”猫“和”Kitty“之间的距离比”猫“和”Kitty“之间的距离更近。由于编码矩阵的长度和基学习器的数量可以比<span class="math inline">\(K\)</span>小得多，因此基于ECOC的方法可以显著降低OVA和OVO的计算复杂度，特别是类别数量K非常大时。</p>
<p>基于ECOC方法的模型性能高度依赖编码矩阵和解码策略的设计，而随机生成的编码矩阵对结果具有高度的不确定性。为了解决这一问题，在优化编码矩阵方面做了很多工作。由于其多分类问题的复杂性，因此要找到一个优化矩阵是不可能的，甚至次优化矩阵也很难找到，因此无法得到优化矩阵阻碍了ECOC的应用。</p>
<h2 id="lightmc">1.3LightMC</h2>
<p>不同于以往ECOC使用固定的编码矩阵和解码策略，LightMC能够动态的优化编码矩阵和解码策略，并且在每次迭代时对基学习器进行训练，从而达到更为精确的多类分类。为了达到这个目的，LightMC利用了一种可微的解码策略，使得可以通过梯度下降来进行优化，从而保证进一步降低训练损失。LightMC除了提高最终分类精度和获得更利于分类性能的编码矩阵和解码策略外，LightMC还可以显著提高效率，因为它节省了搜索次优编码矩阵的时间。LightMC在模型的训练过程中优化了编码矩阵，不需要再花费太多的时间来初始化编码矩阵，即使是随机编码矩阵也可以得到令人满意的结果。</p>
<h1 id="相关工作">2.相关工作</h1>
<h2 id="ecoc-1">2.1ECOC</h2>
<p>在ECOC中，每一个分类<span class="math inline">\(k\)</span>都有一个编码字<span class="math inline">\(M_{k}\)</span>，该编码字<span class="math inline">\(M_{k}\)</span>表示在第<span class="math inline">\(j\)</span>个基分类器中类<span class="math inline">\(k\)</span>中数据的标签。所有编码字组合一个编码矩阵<span class="math inline">\(M \in \left \{ 1,-1 \right \}^{K \times L}\)</span>，其中L是一个编码字的长度和所有基学习器的数量。所有基学习器的输出为<span class="math inline">\(o=\left \{ o_{1}, o_{2}, \cdots , o_{L} \right \}\)</span>，最后通过解码策略就可以得到多分类的结果。</p>
<p><span class="math display">\[\begin{align}
    \hat{y} = argmin_{k}\left ( t \right ),where\ t_{k}=\frac{1}{2}\sum_{j=1}^{L}\left | M_{kj} - sgn\left ( o_{j} \right ) \right |
\end{align}\]</span></p>
<p>其中<span class="math inline">\(\hat{y}\)</span>表示预测的分类，<span class="math inline">\(sgn\)</span>是一个符号函数，当<span class="math inline">\(o \geq 0\)</span>时<span class="math inline">\(sgn\left ( 0 \right ) = 1\)</span>，否则就为<span class="math inline">\(-1\)</span>。解码策略采用Hamming解码，通过选择Hamming距离最小的类来进行预测，此外，通过Hamming解码还能够纠正基学习期中一定数量的错误。</p>
<p>与传统的分解方法相比，ECOC方法具有许多优点：</p>
<p>1、通过编码矩阵来表示不同类对之间距离，使得能够将类之间的相关性集成到分类模型中，从而进一步提高分类精度。</p>
<p>2、由于代码长度<span class="math inline">\(L\)</span>（即基学习器的数量）可能比类<span class="math inline">\(K\)</span>的数量小得多，因此ECOC方法比OVA和OVO更有效，特别是当<span class="math inline">\(K\)</span>非常大时。</p>
<p>但是ECOC方法的性能主要取决于编码矩阵的设计，并且寻找一个最优的编码矩阵的复杂度是NP-难问题。因此，要找到一个最优的编码矩阵是不可能的，甚至找到一个次优的编码矩阵也是很难，这个问题也制约了ECOC的使用。</p>
<h2 id="其他工作">2.2其他工作</h2>
<p>近年来，许多人试图改进找到合适的编码举证来改进ECOC分解方法。例如，对分类空间进行分层划分，从而以生成相应的编码；通过遗传算法来产生具有较好性能的编码矩阵；通过使用频谱分解找到良好的编码矩阵或通过采用连续值编码矩阵，以及在松弛编码矩阵上的整数约束，可以对ECOC方法有显著改善。虽然这些研究对ECOC的改进具有一定的帮助，但是还是有两个主要的挑战：</p>
<p>1、效率：为了提高多分类的精度，前面的许多工作是设计一个具有<span class="math inline">\(L\)</span>长度的编码矩阵（长度从<span class="math inline">\(K-1\)</span>到<span class="math inline">\(K^{2}\)</span>），这使得基学习器几乎与OVA和OVO所需的模型一样多。这使得ECOC方法在大规模分类问题中非常低效。</p>
<p>2、可扩展性：事实上，以前的ECOC方法研究主要在小规模分类数据下进行，通常只有几十个类和数千个样本组成。</p>
<h1 id="lightmc-1">3.LightMC</h1>
<p>为了解决ECOC方法的两个主要问题，LightMC没有在训练前确定编码矩阵和解码策略，而是试图通过直接优化目标函数，并结合基学习器的训练，动态改善ECOC分解。更具体地说，LightMC引入了一种新的可微解码策略，使得LightMC能够在基学习器训练过程中直接通过梯度下降来优化编码矩阵和解码策略。LightMC有两方面的优势：</p>
<p>1、有效性：LightMC没有将编码矩阵和解码策略的设计与基学习器的训练分离，而是通过联合优化编码矩阵、解码策略和基学习器，从而进一步提高了ECOC分类的精度。</p>
<p>2、高效性：由于在后续的训练中会自动优化编码矩阵，因此LightMC可以显著降低在训练前找到一个好的编码矩阵所需的时间成本。</p>
<img src="/lightmc/lightmc.png" class title="LightMC的算法">
<p>图1是LightMC算法的流程：</p>
<p>1、首先通过现有的ECOC方法初始化编码矩阵。</p>
<p>2、为了充分利用基学习器的训练信息，LightMC采用交替优化算法，将基学习器的学习与编解码优化相结合，即训练基学习器时，编解码策略是固定的，反之则是动态的。</p>
<p>3、这样的联合学习将反复运行，直到整个训练结果收敛。</p>
<p>需要注意的是，不同于在训练前确定编码矩阵，LightMC开发了一个端到端的解决方案，以迭代的方式联合训练基学习器和分解模型。算法1就是LightMC算法的描述，在该算法中有两个步骤：</p>
<p>1、解码训练：用于优化解码策略。</p>
<p>2、编码矩阵训练：用于优化编码举证。</p>
<h2 id="新的可微解码策略softmax解码">3.1新的可微解码策略：Softmax解码</h2>
<p>为了寻找最优的编码与解码策略，就需要对全局目标函数进行直接优化。由于大多数现有的解码策略是不可微的，因此广泛使用的反向传播方法无法用来直接优化全局目标函数。为了解决这个问题，设计一种在保持纠错特性的同时具有可微性的解码策略是至关重要的。</p>
<p>通过研究式1中的解码策略，可以知道有两个不可微的函数，分别是：<span class="math inline">\(sgn\)</span>和<span class="math inline">\(argmin\)</span>。根据文献可以知道，当生成的距离结果为<code>Manhattan（L1）距离</code>时，可以直接删除<span class="math inline">\(sgn\)</span>函数，并且仍然保持纠错特性。此外，<span class="math inline">\(argmin\)</span>函数可以由被广泛使用的<span class="math inline">\(softmax\)</span>函数代替，该函数可以近似于<span class="math inline">\(argmin\)</span>函数，并且具有连续的输出概率和可微的特性，步骤如下：</p>
<p>1、把<span class="math inline">\(argmin\)</span>替换成<span class="math inline">\(argmax\)</span>，同时把<span class="math inline">\(M_{kj}\)</span>的符号颠倒。通过这个方法，当第<span class="math inline">\(j\)</span>个分类器<span class="math inline">\(o_{j}\)</span>输出为<span class="math inline">\(M_{kj}\)</span>时，距离将是最大值而不是最小值。</p>
<p>2、把<span class="math inline">\(argmax\)</span>替换为<span class="math inline">\(softmax\)</span>，则整个解码策略变为：</p>
<p><span class="math display">\[\begin{align}
    \hat{y} = softmax\left ( t \right ),where\ t_{k}=\frac{1}{2}\sum_{j=1}^{L}\left | -M_{kj} - o_{j} \right |
\end{align}\]</span></p>
<p>其中<span class="math inline">\(t_{k}\)</span>表示分类器输出和分类<span class="math inline">\(k\)</span>的编码之间的相似性。尽管在本算法中使用了L1损失，但在文献中提到的L2损失或其他距离函数也同样适用，并且会产生类似的结果。通过上述转换，解码策略将把最高得分分配给最接近输出向量的类，即输出向量具有纠错属性。基于这个原因就可以采用被广泛使用的梯度下降算法来直接优化解码策略。此外，新的解码函数可以重写为单层softmax回归的形式，因为等式2中的距离函数满足：</p>
<p><span class="math display">\[\begin{align}
    \left | -M_{kj} - o_{j} \right | = \left\{
    \begin{matrix}
        1-o_{j}, &amp; M_{kj}=-1 \\ 
        1+o_{j}, &amp; M_{kj}=1
    \end{matrix}\right.
    =1+M_{kj}o_{j}
\end{align}\]</span></p>
<p>因此允许将解码策略重写为：</p>
<p><span class="math display">\[\begin{align}
    \begin{matrix}
        t_{k}=\frac{1}{2}\sum_{j=1}^{L}\left ( 1+M_{kj}o_{j} \right )=\frac{1}{2}\left ( M_{k}^{T}o+L \right )\\
        let\ \theta_{k}=M_{k},b_{k}=L \\ 
        \hat{y}=softmax(t),\ t_{k} = \frac{1}{2}\left ( \theta_{k}^{T}o + b_{k} \right )
    \end{matrix}
\end{align}\]</span></p>
<p>其形式与具有softmax激活的单层线性模型完全相同。因此，就可以使用梯度下降来训练由<span class="math inline">\(M\)</span>初始化的softmax参数<span class="math inline">\(\Theta\)</span>，以减低整体误差。考虑到导数计算的方便性，我们选择了与softmax函数一起使用的多类交叉熵作为损失函数。单个数据点上的总损失可以表示为：</p>
<p><span class="math display">\[\begin{align}
    \begin{matrix}
        J = -\sum_{k=1}^{K}\left ( 1-y_{k} \right )\log\left ( 1-\hat{y_{k}} \right )+y_{k}\log\left (\hat{y_{k}} \right )\\ 
        where\ \Theta\ is\ updated\ by\ \theta_{k}^{t}=\theta_{k}^{t-1}-\gamma_{1}\frac{\partial J}{\partial\theta_{k}^{t-1}}
    \end{matrix}
\end{align}\]</span></p>
<p>其中<span class="math inline">\(\gamma_{1}\)</span>为学习率，<span class="math inline">\(y\)</span>是从原始标签转化而来的one-hot向量。这个优化的过程被称为解码训练（TrainDecoding），如算法2。像普通的梯度下降一样，数据被划分成小批量，用于计算当前梯度，以便进行一次梯度更新。这里也可以使用L1/L2正则化来提高泛化能力。由于梯度下降能够有效性保证了迭代过程中整体损失的减少，从而保证了解码训练（TrainDecoding）是一种有效的改进译码策略的方法。</p>
<h2 id="编码矩阵优化">3.2编码矩阵优化</h2>
<p>如果softmax解码的输入<span class="math inline">\(o\)</span>可以通过反向传播来更新，那么就能够进一步降低总体训练损失。那么对应的更新过程就可以定义为<span class="math inline">\(o^{t}=o^{t-1}-\gamma_{2}\frac{\partial J}{\partial o^{t-1}}\)</span>，其中<span class="math inline">\(\gamma_{2}\)</span>表示学习率。但是，<span class="math inline">\(o\)</span>不能够直接被更新，因为<span class="math inline">\(o\)</span>是基学习器的输出。幸运的是，优化编码矩阵<span class="math inline">\(M\)</span>可以间接地更新<span class="math inline">\(o\)</span>，从而进一步降低整体训练损失。</p>
<p>当<span class="math inline">\(M_{kj}\)</span>被用于训练基学习器<span class="math inline">\(j\)</span>时，它已经决定了数据属于哪一个分类<span class="math inline">\(k\)</span>。假设基学习器能够完美地匹配给定的学习目标，那么对于分类器<span class="math inline">\(j\)</span>来说，任何属于<span class="math inline">\(k\)</span>类的数据该分类类器的输出总是满足<span class="math inline">\(o_{j}^{i}=M_{kj}\)</span>。因此，<span class="math inline">\(M\)</span>的变化会影响基学习器的学习目标，从而进一步影响基学习器的输出<span class="math inline">\(o\)</span>。由于在当基学习器能够完美地匹配给定的学习目标时，梯度<span class="math inline">\(\frac{\partial J}{\partial M_{kj}}\)</span>与<span class="math inline">\(G_{ij} = \frac{\partial J}{\partial o_{j}^{i}}\)</span>相同，此时就可以使用梯度下降的方法来优化<span class="math inline">\(M\)</span>：<span class="math inline">\(M_{kj}^{t}=M_{kj}^{t-1}-\gamma_{2}\frac{\partial J}{\partial M_{kj}^{t-1}}=M_{kj}^{t-1}-\gamma_{2}\frac{\partial J}{\partial G_{kj}^{t-1}}\)</span>。</p>
<p>但是，实践中往往没有完美的基学习器，所以也就不能用上面的方法来优化直接<span class="math inline">\(M\)</span>，因为<span class="math inline">\(\frac{\partial J}{\partial M_{kj}} \neq G_{ij}\)</span>。然而，有许多数据样本可以用于单个类<span class="math inline">\(k\)</span>，即一个<span class="math inline">\(M_{kj}\)</span>对应多个<span class="math inline">\(G_{ij}\)</span>，其中<span class="math inline">\(y_{i}=k\)</span>。因此，可以不必使用不稳定梯度点<span class="math inline">\(G_{ij}\)</span>，而是使用每个类的平均梯度来对<span class="math inline">\(\frac{\partial J}{\partial o_{j}^{i}}\)</span>进行更稳定的估计：</p>
<p><span class="math display">\[\begin{align}
    \frac{\partial J}{\partial M_{kj}} :=\frac{1}{\left | \Omega_{k} \right |}\underset{i \in \Omega_{k}}{\sum G_{ij}},\ where\ \Omega_{k}=\left \{ i \mid y_{i} = k \right \}
\end{align}\]</span></p>
<p>然后利用该估计更新编码矩阵。该优化算法在算法3中已经描述，这一算法与一般的反向传播算法几乎相同，不同的是在执行更新之前使用整个批处理数据来计算平均梯度。实验结果也证明了该方法的有效性，即通过优化全局目标函数，可以对编码矩阵进行一定的细化，以减少损失，提高泛化能力。</p>
<h2 id="讨论">3.3讨论</h2>
<p>1、效率：与现有的ECOC方法相比，LightMC更高效，因为它可以在训练之前使用更少的时间来找到编码矩阵。同时，由于编码矩阵在后续的训练中会被动态地细化，因此它甚至可以产生可比较的性能。此外，LightMC只需要很少的额外优化计算成本，这与单层线性模型的成本相同，并且比神经网络和GBDT等强大的基学习器相比成本要小得多。</p>
<p>2、小批量编码优化方法：由于使用整个批处理进行更新，因此算法3的在内存的使用效率上较低。实际上，切换到较小的批处理（mini-batch）进行更新是很自然的，因为平均梯度也可以在mini-batch中进行计算。</p>
<p>3、分布式编码：在现有的ECOC方法中使用二进制编码。另一方面，LightMC采用分布式编码进行连续优化。显然，分布式编码（也称为<code>嵌入</code>）比二进制编码包含了更多的信息，这使得LightMC能够利用更多的信息来处理类之间的相关性。</p>
<p>4、基学习器的交替训练：在算法1中，如果基学习器不是Boosting学习器，而是如神经网络，那么每次迭代时都可以调用LightMC。而对于Boosting学习器，LightMC从第<span class="math inline">\(i_{s}\)</span>轮开始，每<span class="math inline">\(\frac{1}{\alpha}\)</span>轮调用一次。这是因为在Boosting学习器中有一个学习率<span class="math inline">\(\alpha\)</span>，它会在每次迭代时减少模型的输出。因此，Boosting学习器需要更多的迭代来适应新的训练目标。因此，使用循环次数<span class="math inline">\(i_{s}\)</span>和每<span class="math inline">\(\frac{1}{\alpha}\)</span>次调用一次LightMC可以提高效率，而不需要在每个迭代中调用LightMC。</p>
<p>5、与神经网络中的Softmax层比较：softmax解码的形式确实与神经网络中的softmax层相类似，但是它们确实不同的。</p>
<p>（1）神经网络中的softmax层实际上与OVA分解相同，它不使用编码矩阵来编码类之间的相关性。</p>
<p>（2）它们使用不同的优化方案，优化神经网络中的softmax层主要是为了降低了每个样本的损失，而softmax解码则是优化了每个类的损失（公式6）。softmax在神经网络的实际应用中，很难说哪一种方法更好，甚至最近的一些研究发现，在使用固定的softmax层时，精度几乎相同。</p>
]]></content>
      <categories>
        <category>论文</category>
      </categories>
      <tags>
        <tag>集成学习</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>GBDT理解和推导</title>
    <url>/gbdt/</url>
    <content><![CDATA[<h1 id="gbdt介绍">1.GBDT介绍</h1><p>GBDT（Gradient Boosting Descision Tree，梯度上升决策树），与AdaBoost不同的是，GBDT限定了基模型只能是CART回归树（不是分类树），其次是采用梯度提升的思想来不断更新模型。</p><blockquote>
<p>例：假设一个人的年龄为30岁，现在要使用模型来预测。第一次拟合目标为30，假设预测值为20，两者相差10；第二次拟合目标不再是30，而是对应的前面的残差10，假设这次预测值为8，这次残差为2；此时如果继续预测，那下一次的拟合目标就是2。以此类推，每次拟合的目标都是上一次的残差值。</p>
</blockquote><a id="more"></a>


<p>GBDT其实就是参考这一思想。假设前一个迭代的集成模型为<span class="math inline">\(H_{t-1}\left ( x \right )\)</span>，对应的损失函数为<span class="math inline">\(L\left ( y, H_{t-1}\left ( x \right ) \right )\)</span>，那么在本轮的迭代中就需要找到一个拟合目标，并训练一个模型<span class="math inline">\(h_{t}\left ( x \right )\)</span>，使得本轮的集成模型为<span class="math inline">\(H_{t}\left ( x \right ) = H_{t-1}\left ( x \right ) + h_{t}\left ( x \right )\)</span>，并且对应的损失函数<span class="math inline">\(L\left ( y, H_{t}\left ( x \right ) \right )\)</span>最小。在实际应用中，如果拟合的目标只是残差的话，是有一定的局限性，因为只有损失函数为平方损失函数时，那么拟合目标为残差才能降低损失函数，但是是其他损失函数，那么拟合目标就不是仅仅是残差。</p>
<p>如果损失函数为均方差损失函数<span class="math inline">\(L\left ( y, H_{t}\left ( x \right ) \right ) = \left ( \frac{1}{2} \right ) \ast \left ( y_{i} - H_{t}\left ( x \right ) \right )^{2}\)</span>，那么对<span class="math inline">\(H_{t}\left ( x \right )\)</span>进行求导，就可以得到负梯度：</p>
<p><span class="math display">\[\begin{align}
    -\left [ \frac{\partial L\left ( y, H_{t}\left ( x \right ) \right )}{\partial H_{t}\left ( x \right )} \right ] = \left ( y - H_{t}\left ( x \right ) \right )
\end{align}\]</span></p>
<p>此时，拟合的目标恰好为残差，所以只有平方损失函数的拟合目标为残差。为了把GBDT扩展到更复杂的损失函数，<strong>因此提出了使用上一轮的损失函数的负梯度来作为本轮的拟合目标。</strong></p>
<h1 id="为什么使用负梯度">2.为什么使用负梯度</h1>
<p>为了证明拟合目标的损失函数的负梯度就能够降低损失函数，需要在证明的过程中使用泰勒一阶展开公式，因此泰勒一阶展开的形式为：</p>
<p><span class="math display">\[\begin{align}
    f\left ( x \right ) \approx f\left ( x_{0} \right ) + {f}&#39;\left ( x_{0} \right )\left ( x - x_{0} \right )
\end{align}\]</span></p>
<p>而在<span class="math inline">\(t\)</span>轮的集成模型可以表示为：</p>
<p><span class="math display">\[\begin{align}
    H_{t}\left ( x \right ) = H_{t-1}\left ( x \right ) + h_{t}\left ( x \right )
\end{align}\]</span></p>
<p>因此，第<span class="math inline">\(t\)</span>轮的损失函数<span class="math inline">\(L\left ( y, H_{t}\left ( x \right ) \right )\)</span>在<span class="math inline">\(H_{t-1}\left ( x \right )\)</span>处进行一阶泰勒展开为：</p>
<p><span class="math display">\[\begin{align}
    \begin{matrix}
        L\left ( y, H_{t}\left ( x \right ) \right ) \approx L\left ( y, H_{t-1}\left ( x \right ) \right ) + \frac{\partial L\left ( y, H_{t-1}\left ( x \right ) \right )}{\partial H_{t-1}\left ( x \right )}\left ( H_{t}\left ( x \right ) - H_{t-1}\left ( x \right ) \right )\\ 
        = L\left ( y, H_{t-1}\left ( x \right ) \right ) + \frac{\partial L\left ( y, H_{t-1}\left ( x \right ) \right )}{\partial H_{t-1}\left ( x \right )}h_{t}\left ( x \right )
    \end{matrix}
\end{align}\]</span></p>
<p>如果想要<span class="math inline">\(L\left ( y, H_{t}\left ( x \right ) \right ) \leq L\left ( y, H_{t-1}\left ( x \right ) \right )\)</span>，那么上式中的<span class="math inline">\(\frac{\partial L\left ( y, H_{t-1}\left ( x \right ) \right )}{\partial H_{t-1}\left ( x \right )}h_{t}\left ( x \right ) \leq 0\)</span>，那么就可以使：</p>
<p><span class="math display">\[\begin{align}
    h_{t}\left ( x \right ) = -\frac{\partial L\left ( y, H_{t-1}\left ( x \right ) \right )}{\partial H_{t-1}\left ( x \right )}
\end{align}\]</span></p>
<p>此时</p>
<p><span class="math display">\[\begin{align}
    L\left ( y, H_{t}\left ( x \right ) \right ) = L\left ( y, H_{t-1}\left ( x \right ) \right ) - \left ( \frac{\partial L\left ( y, H_{t-1}\left ( x \right ) \right )}{\partial H_{t-1}\left ( x \right )} \right )^2
\end{align}\]</span></p>
<p>从上面的过程可以知道<span class="math inline">\(h_{t}\left ( x \right ) = -\frac{\partial L\left ( y, H_{t-1}\left ( x \right ) \right )}{\partial H_{t-1}\left ( x \right )}\)</span>，因此如果第<span class="math inline">\(t\)</span>轮的基模型<span class="math inline">\(h_{t}\left ( x \right )\)</span>拟合的目标是<span class="math inline">\(t-1\)</span>轮损失函数的负梯度，那么第<span class="math inline">\(t\)</span>轮的损失函数最小。如果损失函数为平方损失函数是，负梯度也就变为了残差。<strong>GBDT的求解过程可以认为是在函数空间的梯度下降，也就是将带求解的模型函数作为梯度下降中要求解的参数。</strong></p>
<h1 id="gbdt算法流程">3.GBDT算法流程</h1>
<p>假设训练集<span class="math inline">\(D=\left \{ \left ( x^{\left ( 1 \right )}, y^{\left ( 1 \right )} \right ),\left ( x^{\left ( 2 \right )}, y^{\left ( 2 \right )} \right ), \cdots ,\left ( x^{\left ( m \right )}, y^{\left ( m \right )} \right ) \right \}\)</span>，其中样本的个数为<span class="math inline">\(m\)</span>，<span class="math inline">\(y^{\left ( i \right )} \in \left \{ -1, +1 \right \}, i \in \left \{ 1, 2, \cdots , m \right \}\)</span>，并且基模型的个数为<span class="math inline">\(T\)</span>。</p>
<p>1、初始化模型<span class="math inline">\(H_{0}\left ( x \right ) = 0\)</span>。</p>
<p>2、令<span class="math inline">\(t = 1, 2, \cdots , T\)</span>，循环执行下面的语句：</p>
<p>（1）对于每个样本<span class="math inline">\(i = 1, 2, \cdots , m\)</span>，计算<span class="math inline">\(t-1\)</span>轮损失函数的负梯度<span class="math inline">\(\frac{\partial L\left ( y^{\left ( i \right )}, H_{t-1}\left ( x^{\left ( i \right )} \right ) \right )}{\partial H_{t-1}\left ( x^{\left ( i \right )} \right )}\)</span>，并更新<span class="math inline">\(t\)</span>轮样本的标签<span class="math inline">\(y^{\left ( i \right )}\)</span>。</p>
<p>（2）根据更新后的<span class="math inline">\(\left ( x^{\left ( i \right )}, y^{\left ( i \right )} \right ), i = 1, 2, \cdots , m\)</span>训练得到第<span class="math inline">\(t\)</span>轮的基模型（CART回归树）<span class="math inline">\(h_{t}\left ( x \right )\)</span>。</p>
<p>（3）生成第<span class="math inline">\(t\)</span>轮集成模型<span class="math inline">\(H_{t}\left ( x \right ) = H_{t-1}\left ( x \right ) + h_{t}\left ( x \right )\)</span>。</p>
<h1 id="gbdt正则化">4.GBDT正则化</h1>
<p>GBDT正则化常见的有三种方式：</p>
<p>1、类似于AdaBoost中的学习率<span class="math inline">\(v\)</span>，则集成模型变为</p>
<p><span class="math display">\[\begin{align}
    H_{t}\left ( x \right ) = H_{t-1}\left ( x \right ) + vh_{t}\left ( x \right )
\end{align}\]</span></p>
<p><span class="math inline">\(v\)</span>的取值范围是<span class="math inline">\(0 &lt; v \leq 1\)</span>，对于同样的训练集学习效果，<span class="math inline">\(v\)</span>越小就表示需要更多的迭代次数，通常用学习率和迭代最大次数一起来决定算法的拟合效果。</p>
<p>2、采用控制子采样（SubSample）的比例，也就是说只是用一部分样本训练，但是如果采样比例过小的话，在降低方差的同时也会提高偏差，因此不能过低。</p>
<p>3、控制每个决策树（基模型）的复杂度，也就是对决策树进行一些剪枝操作。</p>
<h1 id="参考资料">参考资料</h1>
<p><a href="https://mp.weixin.qq.com/s?__biz=MzU1MjYzNjQwOQ==&amp;mid=2247485694&amp;idx=1&amp;sn=9a07363ba6e3993d6bc8d03cff491bb6&amp;chksm=fbfe5268cc89db7e994c698fad2cc51ff4820d175d3e93def7ce21a820327391d5c1863077e3&amp;mpshare=1&amp;scene=1&amp;srcid=1111PiSfwyQV742U1d1y0N3i&amp;sharer_sharetime=1573563502132&amp;sharer_shareid=98178f12e3ad48ff6ea4030d6be95e39&amp;pass_ticket=RJ0j6JdTxwW3VpGnjiKrEDd90fErtIsefimQSsFiOiGzIaL7WonIFJuRM%2BUUNQ1P#rd" target="_blank" rel="noopener">XGBoost都已经烂大街了，你还不知道GBDT是咋回事？</a></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>集成学习</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>【2017】LightGBM：A Highly Efficient Gradient Boosting Decision Tree</title>
    <url>/lightgbm/</url>
    <content><![CDATA[<h1 id="介绍">1.介绍</h1><p>以往的GBDT中，当数据特征纬度较高、数据量较大时，其算法的效率和可扩展性都不是很理想，主要原因是需要针对每一个特征的信息增益来确认其分割特征点。为了解决这个问题，作者提出两个新方法，分别是GOSS（Gradient-based One-Side Sampling，基于梯度的单边采样）和EFB（Exclusive Feature Bundling，互斥特征捆绑）。</p><a id="more"></a>

<p>1、GOSS：<strong>核心思想是把很大一部分小梯度的排除在计算信息增益之外，而主要使用具有大梯度的数据来计算信息增益。</strong>原因是具有大梯度的特征在计算信息增益时扮演了更为主要的角色，因此GOSS可以在小数据量的情况下获取较为精确的信息增益估计。此外，在采样率的情况下，且信息增益的取值范围较大时，这种方法相较于均匀随机采样可以得到得到更精确的增益估计。</p>
<p>2、EFB：<strong>核心思想是把相互排斥的特征（即很少同时取相同值）捆绑在一起，从而减少特征的数量。</strong>要找到精确的互斥特征绑定是一个NP难问题，但是通过贪心算法进行实现很好的近似，从而实现在特征数量最少的同时，也不影响分割点确定的准确性，该问题可以转化为<code>图着色问题</code>。</p>
<h1 id="准备工作">2.准备工作</h1>
<h2 id="gbdt和复杂度分析">2.1GBDT和复杂度分析</h2>
<p>GBDT的主要性能瓶颈在于学习决策树，而学习决策树最耗时的是寻找最佳分割点。目前最受欢迎的寻找最优分割点的算法有两个，分别是pre-sorted算法（预排序算法）和histogram-based算法（基于直方图的算法）。</p>
<p>1、pre-sorted算法（预排序算法）：该算法在预排序的特征值上枚举了所有可能的分割点。该算法能够找到最优分割点，但是在训练速度和内存损耗上效率不高。</p>
<p>2、histogram-based算法（基于直方图的算法）：不同于pre-sorted算法，histogram-based算法不需要在分类后的特征值上寻找分割点，而是将连续的特征值存储到离散的存储箱（bins）中，并在训练过程中利用这些存储箱（bins）构造每个叶节点的特征直方图。<strong>由于histogram-based算法在内存损耗和训练速度上更为高效，因此作者采用这种方法来寻找最佳分割点。</strong>该算法的构建直方图的复杂度是<span class="math inline">\(O\left ( \#data \times \#feature \right )\)</span>，最佳分割点的复杂度是<span class="math inline">\(O\left ( \#bin \times \#feature \right )\)</span>，其中由于bin的数量通常比数据data要小很多，所以分割点寻找的复杂度主要在直方图的构建，那么如果能够降低数据或者特征，那么就可以大大提高GBDT的训练速度。</p>
<h2 id="相关工作">2.2相关工作</h2>
<p>GBDT的实现已经有许多，如XGBoost、pGBRT、Scikit-learn，以及GBM。其中使用R语言实现的Scikit-learn和GBM中，寻找最优分割点就采用的pre-sorted算法，而在pGBRT中则是histogram-based算法，XGBoost同时支持两种算法。由于XGBoost是其他算法中最为出色的，因此作者采用XGBoost作为验证的基准。</p>
<p>为了降低训练数据的大小，通常的做法是对数据进行下采样，把权重小于固定阈值的数据进行过滤。例如，在SGB中每次迭代中使用随机子集来训练弱学习器，或在训练过程中采用动态采样率。然而，除了SGB外，这些工作都是基于AdaBoost，不能够直接用于GBDT，因为GBDT在初始化时数据没有相应的权重。虽然SGB能够支持GBDT，但是它会是GBDT损失经度，所以也不是一个理想的选择。</p>
<p>类似的，为了降低训练集中特征的数量，通常的做法是过滤掉相对较弱的特征，如利用PCA（主成分分析）或者PP（投影追踪）来完成。但是，这些方法主要依赖于特征所包含的冗余信息，而在实践中这些做法并不总是正确，因为所谓特征就是具有独特的性质，而如果直接删除，那么都会在一定程度上影响训练的效果。</p>
<p>在实际应用中，大规模数据往往非常稀疏。采用pre-sorted算法的GBDT通过忽略数据值为0的特征来降低训练成本，但是采用histogram-based算法的GBDT就不能针对稀疏问题进行有效的解决，这是因为histogram-based算法不管数据的特征值是0还是非0都需要检索特征箱中的值，而基于histogram-based算法的GBDT正是利用这一稀疏特性。</p>
<p>所以针对以往研究的局限性，提出了两种新方法，分别是基于梯度的单边采样（GOSS）和互斥特征捆绑（EFB），算法如图1。</p>
<img src="/lightgbm/goss-efb.png" class title="GOSS和EFB的算法">
<center>
图1：GOSS和EFB的算法
</center>
<h1 id="gossgradient-based-one-side-sampling基于梯度的单边采样">3.GOSS（Gradient-based One-Side Sampling，基于梯度的单边采样）</h1>
<p>使用GOSS在进行采样时对小梯度的数据只进行部分采样，因此为了补偿减少数据造成的分布影响，在计算信息增益时，GOSS对小梯度数据引入了一个常数乘法器，具体方法如下：</p>
<p>1、GOSS首先把数据按照它们的梯度绝对值进行排序。</p>
<p>2、选择前面<span class="math inline">\(a \times 100\%\)</span>的数据。</p>
<p>3、在剩余的数据中随机选取<span class="math inline">\(b \times 100\%\)</span>的数据。</p>
<p>4、在计算信息增益时将具有小梯度的数据放大<span class="math inline">\(\frac{1-a}{b}\)</span>倍。</p>
<p>通过GOSS，就可以把模型的更多注意力放在梯度较大、训练不足的数据上，并且不会改变原始数据的分布。</p>
<h1 id="原理分析">3.1原理分析</h1>
<p>GBDT使用决策树来得到一个函数，该函数把输入空间<span class="math inline">\(\mathcal{X}^{\mathcal{S}}\)</span>转化为一个梯度空间<span class="math inline">\(\mathcal{G}\)</span>。假设有一个服从<code>独立同分布</code>的具有<span class="math inline">\(n\)</span>个实例的训练集<span class="math inline">\(\left \{ x_{1}, x_{2}, \cdots , x_{n} \right \}\)</span>，其中<span class="math inline">\(x_{i}\)</span>是一个在<span class="math inline">\(\mathcal{X}^{\mathcal{S}}\)</span>空间中具有<span class="math inline">\(s\)</span>维的向量。在每次梯度增强的迭代中，模型损失函数的负梯度就可以表示为<span class="math inline">\(\left \{ g_{1}, g_{2}, \cdots , g_{n} \right \}\)</span>。决策树通过最大信息增益来把模型分为不同的节点。在GBDT中，信息增益的计算常常通过分裂后的方差来获得。</p>
<p><strong>定义：</strong>假设<span class="math inline">\(O\)</span>是训练集在决策树中的固定节点。此节点上点<span class="math inline">\(d\)</span>处的分割特征<span class="math inline">\(j\)</span>的方差增益可以定义为：</p>
<p><span class="math display">\[\begin{align}
    \begin{matrix}
        V_{j \mid O}\left ( d \right )=\frac{1}{n_{O}}\left ( \frac{\left ( \sum_{\left \{ x_{i} \in O\ :\ x_{xj} \leq d \right \}}g_{i} \right )^{2}}{n_{l \mid O}^{j}\left ( d \right )} + \frac{\left ( \sum_{\left \{ x_{i} \in O\ :\ x_{xj} &gt; d \right \}}g_{i} \right )^{2}}{n_{r \mid O}^{j}\left ( d \right )} \right )\\ 
        where\ n_{O}=\sum I\left [ x_{i} \in O \right ]\\ 
        \ n_{l \mid O}^{j}\left ( d \right )=\sum I\left [ x_{i} \in O\ :\ x_{ij} \leq d \right ]\ and\ n_{r \mid O}^{j}\left ( d \right )=\sum I\left [ x_{i} \in O\ :\ x_{ij} &gt; d \right ]
    \end{matrix}
\end{align}\]</span></p>
<p>上式中的<span class="math inline">\(\frac{\left ( \sum_{\left \{ x_{i} \in O\ :\ x_{xj} \leq d \right \}}g_{i} \right )^{2}}{n_{l \mid O}^{j}\left ( d \right )}\)</span>表示左支的平均信息增益，<span class="math inline">\(\frac{\left ( \sum_{\left \{ x_{i} \in O\ :\ x_{xj} &gt; d \right \}}g_{i} \right )^{2}}{n_{r \mid O}^{j}\left ( d \right )}\)</span>表示右支的平均信息增益，最后求得以节点<span class="math inline">\(d\)</span>和特征点<span class="math inline">\(j\)</span>为分割点的平均方差增益。</p>
<p>对于特征<span class="math inline">\(j\)</span>来说，决策树选择<span class="math inline">\(d_{j}^{\ast}=argmax_{d}V_{j}\left ( d \right )\)</span>节点，并且计算最大增益<span class="math inline">\(V_{j}\left ( d_{j}^{\ast} \right )\)</span>。然后，数据以<span class="math inline">\(d_{j\ast}\)</span>节点处的<span class="math inline">\(j^{\ast}\)</span>个特征进行分割为左右两个子节点。</p>
<p>在GOSS方法中：</p>
<p>1、把训练集中的所有数据按照梯度的绝对值进行降序排列。</p>
<p>2、保留最大梯度的前<span class="math inline">\(a\%\)</span>，并得到一个子集<span class="math inline">\(A\)</span>。</p>
<p>3、对于剩余数据集<span class="math inline">\(A^{c},\ \left ( 1-a \right ) \times 100\%\)</span>中的实例都是具有较小梯度，作者随机从该数据集中采样一个子数据集<span class="math inline">\(B\)</span>，该子数据集的大小为<span class="math inline">\(b \times \left | A^{c} \right |\)</span>。</p>
<p>4、最后作者在<span class="math inline">\(A \cup B\)</span>的集合中求方差增益<span class="math inline">\(\tilde{V_{j}}\left ( d \right )\)</span>来对数据进行分割。</p>
<p><span class="math display">\[\begin{align}
    \begin{matrix}
        \tilde{V_{j}}\left ( d \right ) = \frac{1}{n}\left ( \frac{\left ( \sum_{x_{i} \in A_{l}}g_{i} + \frac{1-a}{b}\sum_{x_{i} \in B_{l}} g_{i} \right )^{2}}{n_{l}^{j}\left ( d \right )} + \frac{\left ( \sum_{x_{i} \in A_{r}}g_{i} + \frac{1-a}{b}\sum_{x_{i} \in B_{r}} g_{i} \right )^{2}}{n_{r}^{j}\left ( d \right )} \right )\\ 
        where \ A_{l} = \left \{ x_{i} \in A\ :\ x_{ij} \leq d \right \} \\ 
        A_{r} = \left \{ x_{i} \in A\ :\ x_{ij} &gt; d \right \} \\ 
        B_{l} = \left \{ x_{i} \in B\ :\ x_{ij} \leq d \right \} \\ 
        B_{r} = \left \{ x_{i} \in B\ :\ x_{ij} &gt; d \right \}
    \end{matrix}
\end{align}\]</span></p>
<p>由于对数据进行了部分采样，因此系数<span class="math inline">\(\frac{1-a}{b}\)</span>的作用是把<span class="math inline">\(B\)</span>的梯度规则化到<span class="math inline">\(A^{c}\)</span>大小。因此GOSS中，作者使用一个较小的数据集来计算得到<span class="math inline">\(\tilde{V_{j}}\left ( d \right )\)</span>，而不是使用所有的实例来精确的获取精确的分割点<span class="math inline">\(V_{j \mid O}\left ( d \right )\)</span>，从而大幅减少计算的成本。同时，结果证明了GOSS不会损失太多的训练精度，并且结果优于随机采样。</p>
<p><strong>定理：</strong>假设GOSS中的近似误差为<span class="math inline">\(\varepsilon \left ( d \right ) = \left | \tilde{V_{j}}\left ( d \right ) - V_{j \mid O}\left ( d \right ) \right |\)</span>，并且左梯度<span class="math inline">\(\bar{g}_{l}^{j}\left ( d \right )=\frac{\sum_{x_{i} \in \left ( A \cup A^{c} \right )_{l}}\left | g_{i} \right |}{n_{l}^{j}\left ( d \right )}\)</span>，右梯度<span class="math inline">\(\bar{g}_{r}^{j}\left ( d \right )=\frac{\sum_{x_{i} \in \left ( A \cup A^{c} \right )_{r}}\left | g_{i} \right |}{n_{r}^{j}\left ( d \right )}\)</span>。当概率至少为<span class="math inline">\(1-\delta\)</span>时，就可以得到</p>
<p><span class="math display">\[\begin{align}
    \begin{matrix}
        \varepsilon \left ( d \right ) \leq C_{a,b}^{2}\ln\frac{1}{\delta} \cdot max\left \{ \frac{1}{n_{l}^{j}\left ( d \right )}, \frac{1}{n_{r}^{j}\left ( d \right )} \right \}+2DC_{a,b}\sqrt{\frac{\ln\frac{1}{\delta}}{n}}\\ 
        where\ C_{a,b}=\frac{1-a}{\sqrt{b}}max_{x_{i} \in A^{c}}\left | g_{i} \right |\ and \ D=max\left ( \bar{g}_{l}^{j}\left ( d \right ),\bar{g}_{r}^{j}\left ( d \right ) \right )
    \end{matrix}
\end{align}\]</span></p>
<p>根据这个定理，可以得到以下推论：</p>
<p>1、GOSS的逼近比为<span class="math inline">\(\mathcal{O}\left ( \frac{1}{n_{l}^{j}\left ( d \right )} + \frac{1}{n_{r}^{j}\left ( d \right )} + \frac{1}{\sqrt{n}}\right )\)</span>。假如决策树分裂的不太平衡时（例如，<span class="math inline">\(n_{l}^{j}\left ( d \right ) \geq \mathcal{O}\left ( \sqrt{n} \right )\)</span>并且<span class="math inline">\(n_{r}^{j}\left ( d \right ) \geq \mathcal{O}\left ( \sqrt{n} \right )\)</span>），近似误差主要有上式中的第二项为主，并且当$n <span class="math inline">\(时\)</span>(  )$趋近于0。这就意味着当数据量很大时，近似值非常准确。</p>
<p>2、在GOSS中，随机采样的一个特殊情况就是<span class="math inline">\(a = 0\)</span>。在大多情况下，GOSS可以在<span class="math inline">\(C_{0, \beta} &gt; C_{a , \beta - a}\)</span>的性能优于随机采样，该条件等价于当<span class="math inline">\(\alpha_{a} = \frac{max_{x_{i} \in A \cup A^{c}}\left | g_{i} \right |}{max_{x_{i} \in A^{c}}\left | g_{i} \right |}\)</span>时，<span class="math inline">\(\frac{\alpha_{a}}{\sqrt{\beta}} &gt; \frac{1-a}{\sqrt{\beta - a}}\)</span>。</p>
<p>作者认为在GOSS中泛化误差为<span class="math inline">\(\varepsilon_{gen}^{GOSS}\left ( d \right )=\left | \tilde{V}_{j}\left ( d \right )-V_{\ast}\left ( d \right ) \right |\)</span>，即通过GOSS对训练集进行抽样后计算得到的方差增益与真实分布计算得到的真实方差增益之间的差距。作者推导得到<span class="math inline">\(\varepsilon_{gen}^{GOSS}\left ( d \right ) \leq \left | \tilde{V}_{j}\left ( d \right )-V_{j}\left ( d \right ) \right | + \left | V_{j}\left ( d \right )-V_{\ast}\left ( d \right ) \right | \triangleq \varepsilon_{GOSS}\left ( d \right ) + \varepsilon_{gen}\left ( d \right )\)</span>，因此如果GOSS近似是正确的，那么GOSS的泛化误差将接近于使用全部数据计算得到的泛化误差。此外，抽样会增加基础学习者的多样性，这可能有助于提高泛化性能</p>
<h1 id="efbexclusive-feature-bundling互斥特征捆绑">4.EFB（Exclusive Feature Bundling，互斥特征捆绑）</h1>
<img src="/lightgbm/gb-mef.png" class title="贪婪绑定和合并互斥特征的算法">
<center>
图2：贪婪绑定和合并互斥特征的算法
</center>
<p>高位数据通常非常稀疏，并且特征空间的稀疏性就给了作者设计一个接近无损降低特征数量的可能。具体来说，在稀疏特征空间中，许多特征是互斥的，即它们从不同时取非零值，因此就可以安全的把这些互斥特征绑定到一个特征上。通过一个精心设计的特征搜索算法，就可以从特征集中构建与单个特征相同的特征直方图。使用这种方法，直方图构建的复杂度就由<span class="math inline">\(O\left ( \#data \times \#feature \right )\)</span>变为<span class="math inline">\(O\left ( \#data \times \#bundle \right )\)</span>，并且<span class="math inline">\(\#bundle \ll \#feature\)</span>。从而在不影响精度的前提下，显著提升GBDT的训练速度。<strong>在该算法中主要有两个问题需要解决，分别是如何确定哪些特性需要被绑定在一起和如何构造绑定的特征包。</strong></p>
<p><strong>定理：</strong>将特征划分为最小数量的互斥包是NP难问题。</p>
<p><strong>证明：</strong>该问题可以归结为图着色问题。由于图着色问题是NP-难问题，因此可以推导出将特征划分为最小数量的互斥包也是NP难问题。</p>
<p>给定任意图着色问题的实例<span class="math inline">\(G=\left ( V,E \right )\)</span>，通过实例<span class="math inline">\(G\)</span>中的每一个特征得到一个关联矩阵<span class="math inline">\(\left | V \right |\)</span>，该矩阵中每一行都是一个特征。很容易看出，在该问题中，一个互斥特征集对应于一组具有相同颜色的顶点，反之亦然。</p>
<p>1、对于<strong>如何确定哪些特性需要被绑定在一起的问题（算法3）</strong>而言：在上面的定理中已经表明寻找最有特征集是一个NP-难问题，这也就表明要在有限时间内找到解是不可能的。为了找到一个较好的近似算法，作者首先将特征绑定定义为图着色问题，如果两个特征不是互斥，那么就以特征为顶点，并为两个特征之间添加一条边。然后使用贪心算法，从而实现图着色到特征邦定集的良好结果（具有恒定的近似比）。此外，还有有相当多的特性，并不是完全互斥，但也很少有同时为非零的情况。如果该算法允许少量的冲突，那么就可以得到更少的特征集，从而进一步提高计算效率。通过简单的计算，随机地改变一小部分特征值最多影响训练的精度为<span class="math inline">\(\mathcal{O}\left ( \left [ \left ( 1-\gamma \right )n \right ]^\frac{-2}{3} \right )\)</span>，其中<span class="math inline">\(\gamma\)</span>是每个个特征集中的最大冲突率（即不是完全互斥的比例）。因此，如果选择一个相对较小的<span class="math inline">\(\gamma\)</span>，就将能够在精度和效率之间达到一个很好的平衡。</p>
<p>基于上面的结论，作者为互斥特征集设计了算法3，具体流程如下：</p>
<p>（1）构建一个带加权边的图，其权值对应于特征之间的总冲突。</p>
<p>（2）按照特征在图中的度进行降序排列。</p>
<p>（3）检查有序列表中的每个特征，并小于一个较小的冲突值时就（由<span class="math inline">\(\gamma\)</span>控制）把其分配给现有的特征集，反之则创建一个新的特征集。</p>
<p>算法3的时间复杂度为<span class="math inline">\(O\left ( \#feature^{2} \right )\)</span>，并且在训练前只运行一次。当特征的数量不是很大时，这种复杂性是可以接受的，但如果有数百万个特征，仍然可能受到影响。为了进一步提高效率，作者提出在不构建图的情况下更为有效的排序策略：</p>
<p><strong>按非零值计数排序，这与按度排序相似，因为非零值越多，冲突的概率就越高。</strong></p>
<p>2、对于<strong>如何构造绑定的特征包的问题（算法4）</strong>而言：这个问题的关键是如何保证在新的特征集中可以识别出原始特征。由于在直方图算法中存储了离散的特征集，而不是连续的特征值，因此我们可以通过<strong>在不同的特征集中放置互斥特征来构造特征集，并且通过向原始特征值添加偏移来完成</strong>。</p>
<p>（1）假设在一个特征集中有两个特征，其中原始特征A的取值为[0, 10)，原始特征B的取值为[0, 20)。</p>
<p>（2）在特征B中添加一个偏移量10，使得特征B变为[10, 30)。</p>
<p>（3）这样就可以安全的把特征A和B进行合并，从而形成一个新的特征集来代替原始的特征A和B，取值范围为[0, 30]。</p>
<p>EFB算法可以将大量的互斥特征捆绑到较少的特征集上，从而有效地避免了对零特征值的不必要计算。此外，也可以为每个特征使用一个表来记录具有非零值的数据，从而忽略零特征值来直方图的算法。通过在表中寻找数据可以把一个特征的直方图构建的计算成本从<span class="math inline">\(O\left ( \#data \right )\)</span>变为<span class="math inline">\(O\left ( \#non-zero-data \right )\)</span>。但是，这种方法需要额外的内存和计算成本来维护整个树生长过程中的每个特征表，并且这个优化并不与EFB相冲突，当特征集稀疏时仍然可以使用EFB。</p>
<h1 id="参考资料">参考资料</h1>
<p><a href="https://zhuanlan.zhihu.com/p/38516467" target="_blank" rel="noopener">LightGBM原理分析</a></p>
<p><a href="https://www.msra.cn/zh-cn/news/features/lightgbm-20170105" target="_blank" rel="noopener">开源 | LightGBM：三天内收获GitHub 1000 星</a></p>
<p><a href="https://www.zhihu.com/question/51644470" target="_blank" rel="noopener">如何看待微软新开源的LightGBM?</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/34698733" target="_blank" rel="noopener">从结构到性能，一文概述XGBoost、Light GBM和CatBoost的同与不同</a></p>
<p><a href="https://blog.csdn.net/qq_24519677/article/details/82811215" target="_blank" rel="noopener">Lightgbm基本原理介绍</a></p>
<p><a href="https://baike.baidu.com/item/%E5%9B%BE%E7%9D%80%E8%89%B2%E9%97%AE%E9%A2%98" target="_blank" rel="noopener">图着色问题</a></p>
]]></content>
      <categories>
        <category>论文</category>
      </categories>
      <tags>
        <tag>集成学习</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>AdaBoost理解和推导</title>
    <url>/adaboost/</url>
    <content><![CDATA[<h1 id="adaboost介绍">1.AdaBoost介绍</h1><p>AdaBoost（Adaptive Boosting，自适应增强集成算法），即自动通过<strong>前一个基模型对样本预测的误差来调整当前模型中的权重</strong>，然后基于新的权重来训练得到新的模型，以此反复直到训练的基模型个数达到设定的数量，最后把<strong>所有基模型的训练结果</strong>通过组合策略进行集成，从而得到最终模型。基模型的种类很多，可以是决策树，也可以是神经网络等。这个过程对结果的预测主要有两部分的提高：</p><a id="more"></a>

<p>1、训练过程的提高：通过<strong>增加</strong>之前基模型中的分类或回归错误的样本权重，并<strong>降低</strong>了正确分类和回归的样本权重，从而使得<strong>错误分类的样本在下一个基模型中能够被更大的关注。</strong></p>
<p>2、模型结果的提高：在最后的组成策略中采用<strong>增加</strong>分类或回归误差较小的模型权重，<strong>降低</strong>分类或回归误差较大的模型权重，从而<strong>提高正确模型的贡献率。</strong></p>
<p>AdaBoost要解决的问题主要有四个，分别是：</p>
<p>1、如何计算模型的预测的误差率？</p>
<p>2、如何确定基模型的权重</p>
<p>3、如何更新样本的权重</p>
<p>4、如何将多个基模型组合在一起</p>
<p>假设训练集<span class="math inline">\(D=\left \{ \left ( x^{\left ( 1 \right )}, y^{\left ( 1 \right )} \right ),\left ( x^{\left ( 2 \right )}, y^{\left ( 2 \right )} \right ), \cdots ,\left ( x^{\left ( m \right )}, y^{\left ( m \right )} \right ) \right \}\)</span>，其中样本的个数为<span class="math inline">\(m\)</span>，<span class="math inline">\(y^{\left ( i \right )} \in \left \{ -1, +1 \right \}, i \in \left \{ 1, 2, \cdots , m \right \}\)</span>，并且基模型的个数为<span class="math inline">\(T\)</span>，第<span class="math inline">\(t\)</span>个基模型中每个训练样本的权重为<span class="math inline">\(D_{t}=\left ( \omega_{t,1}, \omega_{t,2}, \cdots , \omega_{t,m} \right )\)</span>。</p>
<h1 id="分类问题中adaboost的算法流程">2.分类问题中AdaBoost的算法流程</h1>
<p>1、初始化数据集在<strong>第一个</strong>基模型中的样本权重：<span class="math inline">\(D_{1}=\left ( \omega_{1,1}, \omega_{1,2}, \cdots , \omega_{1,m} \right )\)</span>，其中<span class="math inline">\(\omega_{1,i} = \frac{1}{m}, i = 1, 2, \cdots , m\)</span>。</p>
<p>2、令<span class="math inline">\(t = 1, 2, \cdots, T\)</span>，循环执行下面的语句：</p>
<p>（1）使用带有权重的训练集<span class="math inline">\(D_{t}\)</span>训练一个基模型<span class="math inline">\(h_{t}\)</span>。</p>
<p>（2）计算基模型<span class="math inline">\(h_{t}\)</span>的误差率<span class="math inline">\(\epsilon_{t} = \sum_{i=1}^{m}\omega_{t,i}I\left ( x^{\left ( i \right )} \neq y^{\left ( i \right )} \right )\)</span>，其中<span class="math inline">\(I \in \left \{ 0, 1 \right \}\)</span>为指示函数。</p>
<p>（3）计算基模型<span class="math inline">\(h_{t}\)</span>在最终集成模型中的权重<span class="math inline">\(\alpha_{t}=\frac{1}{2}\ln \frac{1-\epsilon_{t}}{\epsilon_{t}}\)</span>。</p>
<p>（4）更新下一个基模型中样本的权重<span class="math inline">\(D_{t+1}=\left ( \omega_{t+1,1}, \omega_{t+1,2}, \cdots , \omega_{t+1,m} \right )\)</span>，其中</p>
<p><span class="math display">\[\begin{align}
    \omega_{t+1,i} = \frac{\omega_{t,i}}{Z_{t}}exp\left ( -\alpha_{t}y^{\left ( i \right )}h_{t}\left ( x^{\left ( i \right )} \right ) \right ), i = 1, 2, \cdots, m
\end{align}\]</span></p>
<p>其中，<span class="math inline">\(Z_{t} = \sum_{i=1}^{m}\omega_{t,i}exp\left ( -\alpha_{t}y^{\left ( i \right )}h_{t}\left ( x^{\left ( i \right )} \right ) \right )\)</span>是一个归一化因子。</p>
<p>3、组合各基模型，从而生成最终模型<span class="math inline">\(H\left ( x \right ) = \sum_{t=1}^{T}\alpha_{t}h_{t}\left ( x \right )\)</span>。</p>
<p>结合上面的流程就可以回答AdaBoost要解决的四个主要问题：</p>
<p><strong>1、如何计算模型的预测的误差率：</strong>模型的预测误差<span class="math inline">\(\epsilon_{t}\)</span>通过计算带权重的样本错误分类比例来获得样本的预测误差。</p>
<p><strong>2、如何确定基模型的权重：</strong>基模型模型的权重<span class="math inline">\(\alpha_{t}\)</span>随着模型的预测误差<span class="math inline">\(\epsilon_{t}\)</span>的减小而增大，也就是说基模型的预测误差越小，最后对于最终集成模型的权重越大。由于一般模型的准确率都大于0.5，因此<span class="math inline">\(1 - \epsilon_{t} \geq 0.5\)</span>，所以<span class="math inline">\(\frac{1 - \epsilon_{t}}{\epsilon_{t}} \geq 1\)</span>，所以<span class="math inline">\(\alpha_{t}=\frac{1}{2}\ln \frac{1-\epsilon_{t}}{\epsilon_{t}} \geq 0\)</span>。</p>
<p><strong>3、如何更新样本的权重：</strong>由于<span class="math inline">\(y^{\left ( i \right )} \in \left \{ -1, +1 \right \}, h_{t}\left ( x^{\left ( i \right )} \right ) \in \left \{ -1, +1 \right \}\)</span>，因此<span class="math inline">\(\omega_{t+1,i} = \frac{\omega_{t,i}}{Z_{t}}exp\left ( -\alpha_{t}y^{\left ( i \right )}h_{t}\left ( x^{\left ( i \right )} \right ) \right )\)</span>可以等价为：</p>
<p><span class="math display">\[\begin{align}
    \omega_{t+1,i} = \left\{\begin{matrix}
    \frac{\omega_{t,i}exp\left ( \alpha_{t} \right )}{Z_{t}} &amp; h_{t}\left ( x^{\left ( i \right )} \right ) \neq y^{i}\\ 
    \frac{\omega_{t,i}exp\left ( -\alpha_{t} \right )}{Z_{t}} &amp; h_{t}\left ( x^{\left ( i \right )} \right ) = y^{i}
    \end{matrix}\right.
\end{align}\]</span></p>
<p>相比于正确分类的样本，错误分类的样本权重被放大了<span class="math inline">\(exp\left ( 2 \alpha t \right ) = \frac{1 - \epsilon_{t}}{\epsilon_{t}}\)</span>倍，因此也是<span class="math inline">\(\alpha_{t}=\frac{1}{2}\ln \frac{1-\epsilon_{t}}{\epsilon_{t}} \geq 0\)</span>中<span class="math inline">\(\frac{1}{2}\)</span>的由来。</p>
<p><strong>4、如何将多个基模型组合在一起：</strong>将各个模型的权重<span class="math inline">\(\alpha_{t}\)</span>与对应的基模型的预测结果<span class="math inline">\(h_{t}\left ( x \right )\)</span>相乘后求和。</p>
<h1 id="adaboost的损失函数">3.AdaBoost的损失函数</h1>
<p>在AdaBoost中，第<span class="math inline">\(t\)</span>轮计算中，通过带有权重的数据集<span class="math inline">\(D_{t}\)</span>得到一个基模型<span class="math inline">\(h_{t}\left ( x \right )\)</span>和对应的权重<span class="math inline">\(\alpha_{t}\)</span>，这时得到的集成模型为<span class="math inline">\(H_{t}\left ( x \right )\)</span>，其对应的损失函数为：</p>
<p><span class="math display">\[\begin{align}
    L\left ( y, H_{t}\left ( x \right ) \right ) = \sum_{i=1}^{m}exp\left ( -y^{\left ( i \right )}H_{t}\left ( x^{\left ( i \right )} \right ) \right )
\end{align}\]</span></p>
<p>其中<span class="math inline">\(m\)</span>为样本总数，<span class="math inline">\(t\)</span>为当前训练的基模型的个数和第<span class="math inline">\(t\)</span>个基模型。</p>
<p>通过AdaBoost最终模型的表达式<span class="math inline">\(H\left ( x \right ) = \sum_{t=1}^{T}\alpha_{t}h_{t}\left ( x \right )\)</span>可以得到：</p>
<p><span class="math display">\[\begin{align}
    H_{t}\left ( x \right ) = H_{t-1}\left ( x \right ) + \alpha_{t}h_{t}\left ( x \right )
\end{align}\]</span></p>
<p>可以得到损失函数为</p>
<p><span class="math display">\[\begin{align}
    L\left ( y, H_{t}\left ( x \right ) \right ) = \sum_{i=1}^{m}exp\left ( -y^{\left ( i \right )}\left ( H_{t-1}\left ( x^{\left ( i \right )} \right ) + \alpha_{t}h_{t}\left ( x^{\left ( i \right )} \right ) \right ) \right)
\end{align}\]</span></p>
<p>有了损失函数，最小化损失函数即可得到<span class="math inline">\(\alpha_{t}\)</span>和<span class="math inline">\(h_{t}{\left ( x \right )}\)</span>，即</p>
<p><span class="math display">\[\begin{align}
    \left ( \alpha_{t}, h_{t}\left ( x \right ) \right ) = arg \, \underset{\alpha ,h\left ( x \right )}{min} \sum_{i=1}^{m}exp\left ( -y^{\left ( i \right )}\left ( H_{t-1}\left ( x^{\left ( i \right )} \right ) + \alpha_{t}h_{t}\left ( x^{\left ( i \right )} \right ) \right ) \right)
\end{align}\]</span></p>
<h1 id="参考资料">参考资料</h1>
<p><a href="https://mp.weixin.qq.com/s?__biz=MzU1MjYzNjQwOQ==&amp;mid=2247485628&amp;idx=1&amp;sn=d11d3dbfb414614b980d72227549273c&amp;chksm=fbfe522acc89db3cd36f7e875b389e1ce344bf1effe31877c1b0e7dd3111b46ec8ebe4c0eb9a&amp;mpshare=1&amp;scene=1&amp;srcid=1111G2VhASsJxwHgzxgzAh4U&amp;sharer_sharetime=1573470134351&amp;sharer_shareid=98178f12e3ad48ff6ea4030d6be95e39&amp;pass_ticket=MkqdUCw3eQi%2F6alllcv%2FqGjCfFLhCgOjS2xNHsKL6Fi6sWeIqC%2BZz2YVHbV2%2Fo9q#rd" target="_blank" rel="noopener">AdaBoost：一个经典的自适应增强集成算法</a></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>集成学习</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
</search>
