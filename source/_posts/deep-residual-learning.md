---
title: 【2016】Deep Residual Learning for Image Recognition
mathjax: true
date: 2020-05-19 09:28:20
updated: {{ date }}
tags: [深度学习]
categories: [论文]
---

# 摘要

深度神经网络越来越难训练。作者提出了一种残差学习框架，该框架能够简化那些非常深的网络训练。该框架使得网路层能够根据其输入来学习残差函数，而非原始函数。通过全面的试验表明，残差网络能够更加容易的优化，并且能够从较深的网络中获取更好的精度。在ImageNet数据集上，作者对152层的残差网络进行了评价，虽然该网络是VGG网络深度的8倍，但是仍然具有较低的复杂性。一个残差网络的组合模型在ImageNet的测试集上达到了3.57%的错误率。这个结果在ILSVRC2015的分类分类任务中取得了第一名。同时，我们也分析了在CIFAR-10数据集上100层和1000层的残差网络。

表达的深度在很多视觉识别任务中是至关重要的。仅仅只是采用了较深的表达，便在COCO目标检测数据集上获得28%的性能提升。深度残差网络是作者参加ILSVRC和COCO2015竞赛上所使用的模型基础，且作者在ImageNet检测、ImageNet定位、COCO检测以及COCO分割上均获得了第一名的成绩。

# 介绍

深度卷积神经网络在图像分类上取得了一系列的突破。深度网络以端到端的多层方式很好的集成了低、中、高层的的特征和分类器，特征的层次可以通过多层的堆叠来丰富。最近的研究表明，网络的深度是至关重要的，在具有挑战性的ImageNet数据集上，最好的结果都采用了16-30层深度的网络模型。此外，许多有难度的视觉识别任务也能够从非常深的模型上获得较好的结果。

**在网络深度的驱使下，一个新的问题产生了：训练一个更好的网络是不是就像堆叠更多的网络层一样容易？** 回答这个问题的障碍就是困扰人们很久的梯度消失/梯度爆炸问题，这个问题从一开始就阻止了模型的收敛。然而，这个问题已经在很大程度上通过归一化和中间归一化层得到了解决，使得数十层的网络能够通过反向传播的随即梯度下降能够是进行收敛。

当深度网络开始收敛时，退化问题又暴露了出来：随着网络深度的增加，模型的精度会饱和（这并不奇怪），然后迅速的退化。出乎意料的是，这种退化并不是过拟合造成的，并且当向模型增加更多层时会造成更高的错误率。作者的实验也证明了这一点，图1是一个典型的例子。

{% asset_img training-error.png 训练与测试误差 %}

图1：在CIFAR-10数据集上20层和56层普通网络的训练误差（左）和测试误差（右）。更深的网络有更高的训练误差和测试误差。在ImageNet数据集上有类似现象，如图4所示。

退化的出现表示并非所有的系统都容易被优化。作者比较了一个网络的浅层结构和对应的深层结构。有一个解决方案可以构建更深的模型：添加一个**恒等映射层**，而其他层则直接从浅层模型映射过来。这个结构的深度模型表示一个深度模型不应该有比浅层网络更高的错误率。但是实现表明，作者目前无法找到一个与这种构建的解决方案相当或者更好的方案（或者说无法在可行的时间内实现）。

在本论文中，作者通过引入了**深度残差网络框架** 解决了退化问题。我们明确的让这些层来拟合残差映射，而不是让每一个堆叠的层直接来拟合所需的底层映射。形式上，假设所需的底层映射为$\mathcal{H}(\mathbf{x})$，作者让叠加的非线性层拟合另一个映射$\mathcal{F}(\mathbf{x}):=\mathcal{H}(\mathbf{x})-\mathbf{x}$。那么原始映射就被重新转化为$\mathcal{F}(\mathbf{x})+\mathbf{x}$。作者推断残差映射比原始未参考的印社更容易优化。在极端情况下，如果一个恒等映射是最优的，那么把残差优化为0比通过堆叠线性层来拟合恒等要更加容易。

式子中的$\mathcal{F}(\mathbf{x})+\mathbf{x}$能够通过有“快捷连接”的前向神经网络来实现（如图2所是）。“快捷连接”能够跳过一个或多个网路层。在作者的例子中。“快捷连接”的作用只是简单的执行恒等映射，并“快捷连接”的输出被添加到堆叠的网络层的输出上。恒等映射的“快捷连接”并没有增加额外的参数和计算复杂度。整个网络依然可以通过反向传播SGD实现端到端的训练，并且可以在不修改求解器的情况下使用公共库来轻松实现。

{% asset_img building-block.png 残差学习 %}

图2：残差学习：一个基本结构

作者在ImageNet上进行了全面的实验，以显示退化问题，并评估作者的方法。通过实验表明：1）作者的深度残差网络可以很容易的优化，而相应“普通”网络（通过简单的堆叠层）在深度增加时表现出更高的训练误差。2）作者的深度残差网络能够很容易的从网络深度大幅增加中获得精度增益，而产生的结果比以前的网络要好得多。

CIFAR-10数据集中也有类似的现象，这表明了作者提出的方法的优化难度和效果并不仅仅是对于一个特定数据集。作者在这个数据集上成功的提出了超过100层的训练模型，并探索了超过1000层的模型。

在ImageNet分类数据集上，作者通过极深的残差网络获得了很好的结果。作者的152层残差网络是ImageNet上出现的最深的网络，但其复杂度仍然低于VGG网络。作者的组合模型的top-5错误率仅为3.57%，并赢得了ILSVRC 2015分类竞赛的第一名。在其他识别人物中，极深的模型也表现出很好的泛化性能，使的作者在ILSVRC和COCO 2015比赛中进一步赢得了ImageNet检测、ImageNet定位、COCO检测和COCO分割的第一名。这有利的证据表明，残差学习的原理是通用的，作者期待它能够用于其他视觉问题和非视觉问题中。

# 相关工作

**残差表达：** 在图像识别中，VLAD是残差向量对应于字典进行编码的一种表达形式，并且Fisher向量可以看做是VLAD的一个概率版本。对于图像的检索和分类具有强力的浅层表达。对于向量量化，残差向量编码比原始向量的编码更为有效。

在低级视觉和计算机图形学中，为了求解偏微分方程，通常使用Multigrid方法将系统重新表达为多尺度的子问题来解决，其中每个子问题又是解决粗细尺度之间的残差问题。Multigrid的另一个形式是分层基预处理，它依赖于两个尺度之间的残差向量的变量。实验证明，这些求解器的收敛速度要比标准的求解器快得多，却没有意识到该方法快的原因是残差特性导致的。这些方法表明一个好的重新表达或者预处理可以很容易的进行优化。

**快捷连接：** 快捷连接的实践和理论已经研究了很长时间。快捷连接早期在多层感知机训练上的研究是添加一个线性层，把网络输入与输出进行连接。在相关文献中，一些中间层直接到辅助分类器来处理梯度爆炸/消失。还有文献提出了通过快捷连接实现层响应、梯度、传播误差的中心化方法。还有文献，提出了有一个快捷分支和一些较深分支组成的“inception”层。

在作者工作的同时，“高速网络”提供了门控功能的快捷连接。这些门控依赖于数据，并且需要一些参数，而作者的恒等快捷连接则不需要参数。当一个快捷门控关闭时（接近于0），高速网络中的层表现为非残差函数。恰恰相反，作者的形式总是一个残差函数，恒等快捷也不会关闭，并且在学习额外的残差函数时，所有的信息都能够通过。此外，“高速网络”没有显示出当网络深度增加时（例如，超过100层）带来的精度提高。

# 深度残差学习

## 残差学习

作者将$\mathcal{H}(\mathbf{x})$认为是一个由部分堆叠层（不必是整个网络）来拟合的底层映射，而$\mathbf{x}$表示这个层的输入。如果加入多个非线性层能够逐渐逼近一个复杂函数，那么也相当于假设它们可以逐渐逼近一个残差函数，例如$\mathcal{H}(\mathbf{x})-\mathbf{x}$（假设输入和输出具有相同的纬度）。因此，作者希望这些堆叠层能显式的近似残差函数$\mathcal{F}(\mathbf{x}):=\mathcal{H}(\mathbf{x})-\mathbf{x}$，而不是近似底层映射$\mathcal{H}(\mathbf{x})$。因此原始函数就变为$\mathcal{F}(\mathbf{x})+\mathbf{x}$。虽然这种形式都应该可以逐渐逼近期望函数（假设成立），但学习的难易程度却不同。

这种重表达的动机是因为退化问题的反直觉现象（如图1，左）。正如作者在介绍里讨论的，如果增加的层能以恒等映射来构建，那么一个更深模型的训练错误率不应该比它对应的浅层模型更大。退化问题表明了一个求解器通过多个非线性层来近似恒等映射是困难的。而伴随着残差学习的重表达，如果恒等映射是最优的，那么求解器驱使多个非线性层的权重趋向于零来逼近恒等映射。

在真实的案例中，恒等映射不太可能是最优解，但是作者的重表达能够帮助解决这个问题。如果最优函数更趋近于恒等映射而不是0映射，那么对于求解器来说寻找关于恒等映射的扰动比学习一个新的函数要容易的多。通过实验（如图7所示），学习到的残差函数通常只有很小的响应，说明了恒等映射提供了合理的预处理。

## 通过快捷方式实现恒等映射

作者在每个堆叠层上采用残差学习。一个残差学习的模块如图2所示。从形式上说，在本论文中作者认为一个残差学习模块可以被定义为（式子1）：

$$\mathbf{y}=\mathcal{F}\left(\mathbf{x},\left\{W_{i}\right\}\right)+\mathbf{x}$$

这里的$\mathbf{x}$和$\mathbf{y}$分别表示每层的输入和输出向量。$\mathcal{F}\left(\mathbf{x},\left\{W_{i}\right\}\right)$表示已经学习的残差映射。在如图2的例子中有两个层，$\mathcal{F}=W_{2} \sigma\left(W_{1} \mathbf{x}\right)$，其中$\sigma$表示ReLU的激活函数，为了简化符号省略了偏置项。$\mathcal{F}+\mathbf{x}$通过快捷连接和元素加法来执行。在加法之后我们再执行另一个非线性操作（如图2所示的$\sigma(\mathbf{y})$）。

在式子1中的快捷连接既没有引入额外的参数也没有增加计算的复杂度。这不仅在实践中具有吸引力，而且在作者比较普通网络和残差网络时也非常重要。作者可以公平的同时比较在相同参数数量、深度、宽度和计算成本（除了可以忽略的元素加成）下的普通网络和残差网络。

在式子1中$\mathbf{x}$和$\mathcal{F}$的维度必须相同。如果不相同（如改变了输入、输出的通道数），那么就需要在快捷连接上执行一个线性映射$W_{s}$来匹配输入、输出的纬度（式子2）：

$$\mathbf{y}=\mathcal{F}\left(\mathbf{x},\left\{W_{i}\right\}\right)+W_{s} \mathbf{x}$$

在式子（1）中也可以使用方阵$W_{s}$。但是通过实验可以知道恒等映射已经足以解决退化问题，并且是经济的，因此$W_{s}$只是用来解决维度不匹配的问题。

残差函数的形式$\mathcal{F}$是很灵活的。在本论文中引入了一个有两层或三层的$\mathcal{F}$函数（如图5所示），当然更多层也是可行的。但如果$\mathcal{F}$只有一层，式子（1）就和线性函数$\mathbf{y}=W_{1} \mathbf{x}+\mathbf{x}$类似，因此也就不具有优势。

作者还发现不仅是对于全连接层，对于卷积层也是同样适用的。函数$\mathcal{F}\left(\mathbf{x},\left\{W_{i}\right\}\right)$可以表示多个卷积层。在两个特征图的通道之间执行元素级的加法。

## 网络结构

作者在多个普通网络和残差网络上进行了测试，并都观测到了一致的现象。接下来作者将在ImageNet上对两个模型进行讨论。

**普通网络：** 作者的普通网络结构（如图3所示，中间）是受VGG网络（如图3所示，左边）原理启发。巻积层一般使用$3 \times 3$巻积核，并且按照下面两个简单的设计规则：（1）对于相同的输出特征尺寸，那么层的巻积核的数量就是相同的；（2）如果将特征映射大小减半，则巻积核的数量将增加一倍，以保持每层的时间复杂性。作者直接通过步长为2的巻积层执行下采样。该网络以一个全局平均池层和一个1000通道的softmax层完全连接作为结束。图3（中间）中加权层的总数为34。

{% asset_img imagenet-model.png ImageNet网络结构实例 %}

图3：ImageNet网络结构实例。左边：VGG-19模型是一个参考。中间：有34个参数层的普通网络。右边：有34个参数层的残差网络。虚线表示的快捷连接表示增加维度。表1展示了更多细节和其他变体。

值得注意的是，作者的模型比VGG网络具有更少的巻积核和更低的复杂性（如图3所示，左边）。作者34层的结构含有36亿个FLOPs（乘-加），而该模型仅仅只有VGG-19 （196亿个FLOPs）的18%。

**残差网络：** 基于普通网络，作者插入了快捷连接（如图3所示，右边），将网络转化为对应的残差版本。当输入和输出具有相同维度时，恒等快捷连接（式子1）能够被直接使用（图3中的实线快捷连接）。当维度增加时（图3中的虚线快捷连接），作者认为可以有两个选项：（1）快捷连接继续执行恒等映射，通过添加0数组来填充增加的维度。该选项不增加额外参数；（2）使用式子（2）中的映射快捷连接来匹配维度（通过$1 \times 1$巻积）。对于这两个选项，当快捷连接跨越两种尺寸的特征图时，均使用步长为2的卷积。

## 实现

作者对ImageNet的实现遵循了文献中的实践。调整图像的大小使它的短边长度随机的从文献中采样来增大图像的尺寸。从图像或其水平翻转中随机采样224×224个裁剪，并减去每像素的平均值。使用文献中的标准颜色增强。作者在每次卷积之后和激活之前都采用批处理规范化（BN）。作者使用文献中的方法初始化权重，并从头开始训练所有的普通网络和残差网络。作者使用大小为256的mini-batch进行SGD。学习率从0.1开始，当误差趋于平稳时，学习率除以10，然后对模型进行高达$60 \times 10^{4}$次的迭代训练。作者还使用了0.0001的重量衰减和0.9的动量。按照文献中的做法，作者不使用dropout。

在测试中，为了进行比较研究，我们采用标准的10-crop测试。为了获得最好的结果，作者采用了文献中所示的完全巻积形式，并在多个尺度（调整图像大小，使较短的边位于{224，256，384，480，640}）的结果上取平均分。

# 实验

## ImageNet分类

作者在包含1000个类别的ImageNet 2012分类数据集上评估他们的方法。训练集包含128万张图像，验证集包含5万张图像。作者在10万张测试图像上进行测试，并对top-1和top-5 的错误率进行评估。

**普通网络：** 作者首先评估了18层和34层的普通网络。32层普通网络如图3（中间）所示。18层普通网络也有类似的形式。表1是网络结构的描述。

表1：ImageNet的网络结构。括号中表示构建块，这些块是堆叠在一起（也可以看如图5所示）。通过步长为2的conv3_1、conv4_1和conv5_1执行下采样。

{% asset_img imagenet-architectures.png ImageNet网络结构实例 %}

表2中的结果展示了34层的普通网络在验证集上相比于18层的普通网络有更高的错误。为了揭示原因，在图4（左）中，作者比较了他们在训练过程中的训练和验证错误。作者观察到了退化问题——尽管18层普通网络的解空间是34层普通网络的一个子空间，但34层普通网络在整个训练过程中具有较高的训练误差。

表2：ImageNet验证集上的Top-1错误。相比于对应的普通网络，残差网络没有额外的参数。图4显示了训练的过程。

{% asset_img validation-error.png 在ImageNet验证集上的Top-1错误 %}

{% asset_img training-on-imagenet.png ImageNet数据集上的训练过程 %}

图4：ImageNet数据集上的训练过程。细曲线表示训练误差，粗曲线表示中心裁剪时的训练集误差。左边：18层和34层的普通网络。右边：18层和34层的残差网络。图中的残差网络相比于普通网络没有额外的参数。

作者认为这种优化的困难不太可能是由于梯度消失引起的。这些普通网络使用BN进行训练，从而确保前向传播信号具有非零方差。作者同样验证了在反向传递阶段的梯度由于BN而具有良好的范式，所以在前向和反向阶段的信号不会存在消失的问题。事实上，34层普通网络仍然能够达到较好的精度（表3），这表明求解器在一定程度上起作用。作者推测深度普通网络可能具有指数级的低收敛速度，这影响了训练误差的减小。这种优化的困难原因将在未来研究。

**残差网络：** 接下来作者要对18层和34层的残差网络（ResNets）进行评估。网络结构的基本框架和普通网络相同，除了在如图3所示（右边）在每两个$3 \times 3$的卷积核添加一个快捷连接。在第一个比较中（如表2和图4右侧），作者对所有快捷连接使用恒等映射，并对增加维度使用零填充（选项1）。因此，与相应的普通网络相比，残差网络没有增加额外的参数。

从表2和图4得到了三个主要的观察结果。

1、首先，使用残差学习可以扭转层数越多精度越低的情况，即34层残差网络比18层残差网络更好（2.8%）。更重要的是，34层ResNet具有相当低的训练误差，并且可以推广到验证数据。这表明退化问题在这个设置中得到了很好的解决，并且可以从增加的深度中获得精度增益。

2、其次，相较于普通网络，残差网络的top-1的错误率下降了3.5%（表2），这得益于训练错误率的降低（如图4所示，右边 VS 左边）。这也验证了在极深的网络中残差学习的有效性。

3、最后，作者也注意到18层的普通网络和残差网络的准确度很接近（如表2所示），但18层的残差网络收敛的速度更快（如图4所示，右边 VS 左边）。当网络不是那么深时（18层网络），SGD可以在普通网络中找到很好的解。在这种情况下，残差网络通过在早期提供更快的收敛性来简化优化。

**恒等快捷连接 VS 映射快捷连接：** 作者已经展示无参数和恒等映射有助于训练。接下来作者将研究映射快捷连接（式子2）。在表3中作者比较三种方法：

1、当维度增加时使用0填充快捷连接，并且所有快捷连接都是无参数（与表2和图4右边相同）；

2、当维度增加时使用映射快捷连接，而其他快捷连接使用恒等映射；

3、所有的快捷连接都是用映射快捷连接。

表3显示所有的三个选项相比于普通网络都要好得多。方法2比方法1要好一点。作者认为这是因为方法1中的0填充没有进行残差学习。方法3略优于方法2，作者将这个归结于将额外的参数引入到了更多（13个）的映射快捷连接。但是方法1、2、3之间微小的差异表明映射快捷连接对于解决退化问题不是必须的。所以作者在余下的内容中没有使用方法3。恒等映射因其没有额外的复杂度而对于下面介绍的瓶颈结构尤其重要。

表3：ImageNet验证集上的错误率（%，10-crop测试）。VGG-16是基于作者测试的网络。ResNet-50/101/152使用了方法2——利用映射来匹配增加的维度。

{% asset_img validation-error-rate.png ImageNet验证集上的错误率 %}

**深度瓶颈结构：** 接下来作者将描述在ImageNet上的深度网络。考虑到训练时间的显示，作者将修改构建块结构为瓶颈设计。对于每个残差函数$\mathcal{F}$，作者使用一个3层的堆叠来替代2层（如图5所示）。三层结构是$1 \times 1$，$3 \times 3$和$1 \times 1$的卷积，其中$1 \times 1$的卷积层负责减少然后增加（恢复）维度，剩下的$3 \times 3$的卷积层来减少输入和输出的维度。图55是一个例子，两个设计有相似的时间复杂度。

{% asset_img deeper-residual-function.png ImageNet中的一个深度残差函数 %}

图5：ImageNet中的一个深度残差函数$\mathcal{F}$。左：图3中的34层残差网络上的一个构建块（$56 \times 56$的特征映射）。右：在50/101/152的残差网络上的瓶颈构建块。

无参数的恒等快捷连接对于瓶颈结构尤为重要。如果图5中的恒等映射被替换为映射快捷连接，由于快捷连接连接了两个高维的网络层，因此模型的时间复杂度和模型大小将会翻倍，所以恒等快捷连接对于瓶颈设计是更加有效的。

**50层的残差网络：** 作者在34层网络中替换了所有的2层模块为3层瓶颈模块，整个模型也就变成了50层的残差网络（如表1）。作者使用了方法2来增加维度。该模型有38亿个FLOPs。

**101层和152层的残差网络：** 作者通过更多的3层结构（如表1）构建了101层和152层的残差网络。值得注意的是，虽然网络的深度增加了，但是152层的残差网络的复杂度（113亿个FLOPs）仍然低于VGG-16（153 亿个FLOPs）和VGG-19（196亿个FLOPs）。

在很大程度上，50、101、152层的残差网络比34层的残差网络具有更好的精度（如表3和4）。作者也没有观察到退化问题，并且能够从网络深度增加中获得精度增益。所有的指标都表明了深度增加的好处（如表3和4）。

表4：ImageNet验证集上单模型的错误率（$\dagger$表示是在测试集上）

{% asset_img validation-error-rate-single-model.png ImageNet验证集上单模型的错误率 %}

**与最先进方法的比较：** 在表4中，作者比较了之前最好的单模型结果。作者的基准34层残差网络模型获得了非常好的结果。作者的152层残差网络在单模型top-5验证集错误率仅为4.49%。这个结果比以往的组合模型的结果还要好（如表5）。作者将6个不同深度的ResNets合成一个组合模型（在提交结果时只用到2个152层的模型）。这在测试集上的top-5错误率仅为3.57% (Table 5)，这一项在ILSVRC 2015 上获得了第一名的成绩。

表5：组合模型的错误率。在ImageNet测试数据集的top-5错误率

{% asset_img ensembles-error.png 组合模型的错误率 %}

## CIFA-10以及分析

作者对CIFAR-10数据集进行了更多的研究，该数据集包含了10个类别，以及5万张的训练图片和1万张的测试图片。作者提供了在训练集上训练和在测试集上评估的实验。作者关注的是验证极深模型的效果，而不是追求最好的结果，因此我们只使用简单的框架如下。

## PASCAL和MS COCO的目标检测