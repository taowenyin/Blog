---
title: 【2016】Deep Residual Learning for Image Recognition
mathjax: true
date: 2020-05-19 09:28:20
updated: {{ date }}
tags: [深度学习]
categories: [论文]
---

# 摘要

深度神经网络越来越难训练。作者提出了一种残差学习框架，该框架能够简化那些非常深的网络训练。该框架使得网路层能够根据其输入来学习残差函数，而非原始函数。通过全面的试验表明，残差网络能够更加容易的优化，并且能够从较深的网络中获取更好的精度。在ImageNet数据集上，作者对152层的残差网络进行了评价，虽然该网络是VGG网络深度的8倍，但是仍然具有较低的复杂性。一个残差网络的组合模型在ImageNet的测试集上达到了3.57%的错误率。这个结果在ILSVRC2015的分类分类任务中取得了第一名。同时，我们也分析了在CIFAR-10数据集上100层和1000层的残差网络。

表达的深度在很多视觉识别任务中是至关重要的。仅仅只是采用了较深的表达，便在COCO目标检测数据集上获得28%的性能提升。深度残差网络是作者参加ILSVRC和COCO2015竞赛上所使用的模型基础，且作者在ImageNet检测、ImageNet定位、COCO检测以及COCO分割上均获得了第一名的成绩。

# 介绍

深度卷积神经网络在图像分类上取得了一系列的突破。深度网络以端到端的多层方式很好的集成了低、中、高层的的特征和分类器，特征的层次可以通过多层的堆叠来丰富。最近的研究表明，网络的深度是至关重要的，在具有挑战性的ImageNet数据集上，最好的结果都采用了16-30层深度的网络模型。此外，许多有难度的视觉识别任务也能够从非常深的模型上获得较好的结果。

**在网络深度的驱使下，一个新的问题产生了：训练一个更好的网络是不是就像堆叠更多的网络层一样容易？** 回答这个问题的障碍就是困扰人们很久的梯度消失/梯度爆炸问题，这个问题从一开始就阻止了模型的收敛。然而，这个问题已经在很大程度上通过归一化和中间归一化层得到了解决，使得数十层的网络能够通过反向传播的随即梯度下降能够是进行收敛。

当深度网络开始收敛时，退化问题又暴露了出来：随着网络深度的增加，模型的精度会饱和（这并不奇怪），然后迅速的退化。出乎意料的是，这种退化并不是过拟合造成的，并且当向模型增加更多层时会造成更高的错误率。作者的实验也证明了这一点，图1是一个典型的例子。

{% asset_img training-error.png 训练与测试误差 %}

图1：在CIFAR-10数据集上20层和56层平原网络的训练误差（左）和测试误差（右）。更深的网络有更高的训练误差和测试误差。在ImageNet数据集上有类似现象，如图4所示。

退化的出现表示并非所有的系统都容易被优化。作者比较了一个网络的浅层结构和对应的深层结构。有一个解决方案可以构建更深的模型：添加一个**恒等映射层**，而其他层则直接从浅层模型映射过来。这个结构的深度模型表示一个深度模型不应该有比浅层网络更高的错误率。但是实现表明，作者目前无法找到一个与这种构建的解决方案相当或者更好的方案（或者说无法在可行的时间内实现）。

在本论文中，作者通过引入了**深度残差网络框架** 解决了退化问题。我们明确的让这些层来拟合残差映射，而不是让每一个堆叠的层直接来拟合所需的底层映射。形式上，假设所需的底层映射为$\mathcal{H}(\mathbf{x})$，作者让叠加的非线性层拟合另一个映射$\mathcal{F}(\mathbf{x}):=\mathcal{H}(\mathbf{x})-\mathbf{x}$。那么原始映射就被重新转化为$\mathcal{F}(\mathbf{x})+\mathbf{x}$。作者推断残差映射比原始未参考的印社更容易优化。在极端情况下，如果一个恒等映射是最优的，那么把残差优化为0比通过堆叠线性层来拟合恒等要更加容易。

式子中的$\mathcal{F}(\mathbf{x})+\mathbf{x}$能够通过有“快捷连接”的前向神经网络来实现（如图2所是）。“快捷连接”能够跳过一个或多个网路层。在作者的例子中。“快捷连接”的作用只是简单的执行恒等映射，并“快捷连接”的输出被添加到堆叠的网络层的输出上。恒等映射的“快捷连接”并没有增加额外的参数和计算复杂度。整个网络依然可以通过反向传播SGD实现端到端的训练，并且可以在不修改求解器的情况下使用公共库来轻松实现。

{% asset_img building-block.png 残差学习 %}

图2：残差学习：一个基本结构

作者在ImageNet上进行了全面的实验，以显示退化问题，并评估作者的方法。通过实验表明：1）作者的深度残差网络可以很容易的优化，而相应“平原”网络（通过简单的堆叠层）在深度增加时表现出更高的训练误差。2）作者的深度残差网络能够很容易的从网络深度大幅增加中获得精度增益，而产生的结果比以前的网络要好得多。

CIFAR-10数据集中也有类似的现象，这表明了作者提出的方法的优化难度和效果并不仅仅是对于一个特定数据集。作者在这个数据集上成功的提出了超过100层的训练模型，并探索了超过1000层的模型。

在ImageNet分类数据集上，作者通过极深的残差网络获得了很好的结果。作者的152层残差网络是ImageNet上出现的最深的网络，但其复杂度仍然低于VGG网络。作者的组合模型的top-5错误率仅为3.57%，并赢得了ILSVRC 2015分类竞赛的第一名。在其他识别人物中，极深的模型也表现出很好的泛化性能，使的作者在ILSVRC和COCO 2015比赛中进一步赢得了ImageNet检测、ImageNet定位、COCO检测和COCO分割的第一名。这有利的证据表明，残差学习的原理是通用的，作者期待它能够用于其他视觉问题和非视觉问题中。

# 相关工作

**残差表达：** 在图像识别中，VLAD是残差向量对应于字典进行编码的一种表达形式，并且Fisher向量可以看做是VLAD的一个概率版本。对于图像的检索和分类具有强力的浅层表达。对于向量量化，残差向量编码比原始向量的编码更为有效。

在低级视觉和计算机图形学中，为了求解偏微分方程，通常使用Multigrid方法将系统重新表达为多尺度的子问题来解决，其中每个子问题又是解决粗细尺度之间的残差问题。Multigrid的另一个形式是分层基预处理，它依赖于两个尺度之间的残差向量的变量。实验证明，这些求解器的收敛速度要比标准的求解器快得多，却没有意识到该方法快的原因是残差特性导致的。这些方法表明一个好的重新表达或者预处理可以很容易的进行优化。

**快捷连接：** 快捷连接的实践和理论已经研究了很长时间。快捷连接早期在多层感知机训练上的研究是添加一个线性层，把网络输入与输出进行连接。在相关文献中，一些中间层直接到辅助分类器来处理梯度爆炸/消失。还有文献提出了通过快捷连接实现层响应、梯度、传播误差的中心化方法。还有文献，提出了有一个快捷分支和一些较深分支组成的“inception”层。

在作者工作的同时，“高速网络”提供了门控功能的快捷连接。这些门控依赖于数据，并且需要一些参数，而作者的恒等快捷连接则不需要参数。当一个快捷门控关闭时（接近于0），高速网络中的层表现为非残差函数。恰恰相反，作者的形式总是一个残差函数，恒等快捷也不会关闭，并且在学习额外的残差函数时，所有的信息都能够通过。此外，“高速网络”没有显示出当网络深度增加时（例如，超过100层）带来的精度提高。

# 深度残差学习

## 残差学习

## 通过快捷方式实现恒等映射

## 网络结构

## 实现

# 实验

## ImageNet分类

## CIFA-10以及分析

## PASCAL和MS COCO的目标检测