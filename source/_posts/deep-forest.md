---
title: 【2018】Deep Forest
mathjax: true
date: 2019-12-21 19:32:33
updated: {{ date }}
tags: [集成学习, 深度学习]
categories: [论文]
---

# 摘要

目前的深度学习模型大多建立在神经网络的基础上，即通过反向传播进行训练得到的多层参数化可微非线性模块。在本论文中，作者探讨了构建一个不可微模块的深度模型，并且作者推断深度神经网络的成功，其背后有三个特征，分别是逐层处理、模型中的特征转换和足够的模型复杂度。因此，作者提出了gcFroest方法，该方法能够生成具有上面三个特征的深度森林。该方法是一个决策树集成方法，相比于深度神经网络具有更少的参数，并且模型的复杂度能够通过数据依赖的方法来进行调节。实验表明，该方法对于超参具有很强的鲁棒性，在大多数情况下，即使不同领域的数据，该方法也可以在默认设置下获得较好的性能。该研究开创了不可微模块在深度学习中的应用，并且展示了不适用反向传播构建深度学习模块的可能性。

# 介绍

深度学习已经成为各领域的的热点。那么，什么是深度学习？从crowd得到的答案可能是”深度学习是使用深度神经网络的机器学习的子领域“。事实上，深度神经网络（DNNs）再视觉和语音上的成功导致了深度学习的兴起，并且当前所有的深度学习应用都基于神经网络进行构建，或者更专业的说，是通过反向传播训练的多层参数化可微非线性模型。

虽然深度神经网络很强大，但是它有许多的不足。首先，DNNs有许多超参，并且学习的性能依赖于详细的参数调整。事实上，已经有许多学者使用了卷积神经网络，但由于卷积层的结构，他们实际上使用了不同的学习模型来处理不同的操作。事实上，这个问题使得DNNs的训练非常困难，并使得DNNs更像一个艺术，而不是科学或工程，而且DNNs的理论分析也非常困难，因为有太多的干扰因素和几乎无限的参数组合。第二，众所周知，DNNs训练需要大量的训练数据，因此DNNs就很难应用于小规模训练数据量的任务，有时甚至中等规模的训练数据也会失败。需要注意的是，在大数据时代，由于数据标记的成本更好，许多实际的任务缺乏足够数量的数据标记，因此这也导致了DNNs在这些任务中性能较差。众所周知，神经网络是一个黑盒，即决策过程很难被理解，并且学习行为也很难进行理论分析。此外，在训练神经网络之前，并要把网络结构确认，并且预先确定模型的复杂度。作者猜测，深度模型的复杂度通常比深度模型要复杂的多，正如在最近的研究中表明许多DNNs性能的提高是通过快捷链接、剪枝、二值化等方法，因此这些操作都是原来的网络更加简单，并且降低了模型的复杂度。假如模型模型的复杂度能通过数据来动态决定，那就更好了。值得注意的是，尽管DNNs已经发展的很好，但在许多问题上DNNs的表现并不十分优秀，有时甚至不足，例如随机森林、XGBoost仍然在许多Kaggle比赛中获奖。

作者相信为了解决学习任务的复杂度问题，必须对学习模型进行深入研究。然而，目前的深度模型总是建立在神经网络的基础上。基于上面的原因，有充分的理由去探索非神经网络模式的深度模型，换句话说，考虑是否可以与其他模块一起实现深度学习，因为它们有自己的优势，如果能够深入，可能会显示出巨大的潜力。特别是，因为神经网络是多层参数化可微非线性模型，在现实世界中，不是所有的属性都是可微的或最好的模型都是可微的，在本文中，我们试图解决这个基本问题：

”深度学习是否可以使用不可微模型进行实现？“

本文的结果可以帮助理解更多重要的问题，如（1）深度模型是否就是DNNs（或者，深度模型只能够通过可微模型进行构建）；（2）训练深度模型时候可以不适用反向传播？（反向传播必须要可微）；（3）是否有可能有一个深度模型比现有的其他模型（如随机森林或XGBoost）在任务上的表现更好？事实上，机器学习社区已经开发了许多不可微的学习模块，并且理解这些基于不可微模型的深度模型结构将有利于解决这些模块是否可以在深层学习中使用的问题。

本文，作者扩展了他们的初步研究，并提出了使用gcForest（多粒度级联森林）方法来构建深度森林，该方法不同于使用神经网络的深度模型。该模型是一个新颖的具有级联结构的集成决策树，并通过森林进行表达学习。它的表达学习能力可以通过多粒度扫描进一步增强，这使gcForest具有上下文或结构感知能力。其中级联的等级可以被自动确定，使得模型的复杂度能够依赖于数据来进行确认，而不是在训练前手动确定，这使得gcForest也能够在小规模数据上也能表现的很好，并且使得用户能够根据可用的计算资源来控制训练成本。此外，gcForest相比DNNs具有更少的超参。更好的消息是，它的性能对超参数设置非常健壮；通过实验表明，在大多数情况下它能够在默认设置下就获得优异的性能，即使是不同域的不同数据。

# 灵感

## 从DNNs获取的灵感

普遍认为，深度神经网络的成功关键是表达学习能力。那么在DNNs中，表达学习的关键是什么？作者相信是逐层处理。图1提供了一个说明，当层从底部向上时，更高层次的抽象特性会出现。

{% asset_img layer-by-layer-processing.png 深度神经网络中的逐层处理 %}

图1：深度神经网络中的逐层处理：从底部向上，更高层次的抽象特性会出现

如果其他问题不变，那么模型的复杂度越高（即模型容量更大）就会获得更强的学习能力，那么将DNNs的成功性归因于巨大的模型复杂度听起来是合理的。然而，但这不能解释为什么浅层网络不如深层网络来的成功，因为可以通过增加无限的隐藏层单元来提高浅层网络的复杂度。因此，模型的复杂度不能用来说明DNNs的成功。相反，作者推测逐层处理是DNNs中最重要的因素之一，因为在平面网络（例如单隐层网络）中，不管其复杂性有多大，都不具备逐层处理的特性。虽然作者没有严格的论证，但这一猜想为gcForest的设计提供了重要的启示。

但是对于其他的一些学习模型，如决策树和Boosting，这些模型也都是逐层处理，但是它们却没有DNNs成功。作者认为，最重要的区别因素是，与图1所示生成的新特征的DNNs相比，决策树和Boosting在学习过程中总是在原始特征上进行表达，而没有创建新的特征，换句话说，就是模型没有对特征进行转换。此外，DNNs可以被赋予任意高度的模型复杂度，而决策树和Boosting只具有有限的模型复杂度。虽然模型复杂度不能解释DNNs的成功，但它仍然是重要的因素，因为大型模型需要有大型的训练数据。

因此，作者推测DNNs背后三个重要因素是，逐层处理、模型内的特征转换，和足够的模型复杂度。作者将尝试将这些特性赋予非NN模型的深度模型。

## 从集成学习获得的灵感

集成学习是机器学习中的一个范式，即通过多个学习器进行训练，并且通过组合完成一个任务。众所周知，一个集成学习器能够达到更好的性能，比单一的学习器。

为了构建更好的集成学习器，每一个独立学习器必须精准，并且多样。只结合精确的学习器往往不如结合了一些准确的学习器和一些相对较弱的学习器，因为互补性比单纯的准确性更重要。实际上，从误差模糊度分解理论上导出了一个美丽的公式。

\begin{equation}
    E=\bar{E}-\bar{A}
\end{equation}

其中$E$表示一个集成学习器的错误率，$\bar{E}$表示在集成学习器中单独分类器的平均错误率，$\bar{A}$表示各个分类器之间的平均歧义度，也被称为多样性。式子1表示，每个独立分类器越精确，并且多样性越高，那么集成学习器就更好。这为构建一个集成学习器提供了指导，但是该式子不能个作为优化的目标函数，因为歧义项在推导过程中是数学定义的，不能直接操作。后来，集成学习社区设计了许多多样性的度量指标，但是这些多样性的定义没有一个被广泛的接受。事实上，“什么是多样性？”仍是集成学习中的圣杯问题，近期的一些努力可以在其中找到。

在实际应用中，多样性增强的基本策略是在训练过程中引入基于启发式的随机性。粗略地说，主要有四类机制。

>* 1、数据样本操作。它通过生成不同的数据样本来训练每一个学习器。例如，Bagging中的自主采样，AdaBoost中的连续重要性抽样。
>* 2、输入特征操作。它通过生成二重特征子空间来训练每个学习器。例如，通过随机子空间方法为每个学习器随机选择一个子集特征。
>* 3、学习参数操作。它通过为每个学习设置不同参数来生成不同的学习器。例如，为每个单独的神经网络设置不同的初始权值，而不同的分裂选择可以应用于每个单独的决策树。
>* 4、输出表达操作。它通过使用不同的输出表征来产生不同的单个学习器。例如，通过ECOC方法通过纠错编码输出，而反转输出方法则随机改变一些训练实例的标签。

文献表明不同的机制可以一起使用。但值得注意的是，这些机制也不是一直有用。例如，数据样本处理对于稳定的学习器来说效果不好，因为它们的表现不会因为训练数据的轻微修改而显著改变。

下一节将会介绍gcForest，该方法可以被看作决策树的集成方法，几乎使用所有类别的机制来增强多样性。

# gcForest方法

## 级联森林结构

深度神经网络的表达学习主要依赖于对原始特征的逐层处理。在这个认识的启发下，gcForest使用级联结构（如图2所示），即每个级联层都接收上一层特征信息处理结构，并把当前层的处理结果输出到下一层中。

{% asset_img cascade-forest-structure.png 级联森林结构 %}

图2：级联森林结构。假设每个层级都由两个随机森林（黑色）和两个完全随机森林（蓝色）组成。假设有三个类要预测，因此每个林将输出一个三维分类向量，然后将其连接起来以重新表达原始特征输入。

每层都是一个集成决策树森林，即一个集合。这里，作者包含了不同类型的森林以鼓励森林的多样性，因为多样性是集成结构的关键。为了简单，假设使用两个完全随机树森林和两个随机森林。每个完全随机树森林包含500个完全随机树，这些完全随机树都通过在每个树的结点随机选择一个特征进行分割，并且生长树直到只有叶子，每个叶节点仅包含相同类型的实例。同样的，每个随机森林包含500个树，这些树通过随机选择$\sqrt{d}$个特征作为候选特征（$d$是输入特征的数量），并且选择一个最大基尼指数作为分割点。在每个森林中树的数量是一个超参。

假定有一个实例，每个森林能够通过计算得到在每个叶节点上训练实例对于不同类别的概率，并对整个森林中的所有树进行平均，从而产生一个类别分布估计，如图3所示，其中红色部分是实例遍历到叶节点的路径。

{% asset_img traverses-path.png 遍历路径 %}

图3：类别向量生成。叶节点中不同标记表示不同类别。

类别的估计分布形成了一个类向量，并且与原始的特征向量连接后输入到下一级级联层中。例如，假设有三个类，那么每个森林中的每一个都会产生一个三维的类别向量，并且级联层的下一级将会收到$12\left ( =3\times 4 \right )$个增强特征。

需要注意的是，作者采用了最简单的类别向量形式，即相关实例所在叶节点上的类别分布。但是很明显，这样少量的增强特征只能提供非常有限的增强信息，并且当原始特征向量是高维时，它很可能被淹没。通过实验中表明，这种简单的特征增强已经是有益的。假设有更多增强特征的引入，那么可以获得更好的效果。事实上，还可以结合更多的特征，例如表示先验分布的父节点的类别分布，以及表示互补分布的兄弟节点的类别分布等。这些可能可以在未来进行探索。

为了降低过拟合的风险，通过每个森林产生的类别向量都是通过$k$份的交叉验证产生的。具体来说，每个实例将被用作训练数据$k-1$次，并产生$k-1$个类别向量，然后对这些向量进行平均，以产生最终类别向量作为下一级级联层的增强特征。在扩展新层后，整个级联结构的性能在验证集上进行评估，假设性能没有明显增加，那么训练过程终止，因此就确定了整个级联结构的数量。需要注意的是，当训练成本或者计算资源有限时，也可以使用训练误差而不是校验验证误差的方式来控制级联结构的增加。与大多数模型复杂度固定的深度神经网络相比，gcForest能够通过在适当的时候终止训练来自适应模型复杂度。这使得它能够适应不同规模的训练数据，而不仅仅限于大规模的训练数据。

## 多粒度扫描

深度神经网络非常适合处理特征之间的关系，如卷积神经网络能有效处理原始图像中关键像素之间的重要关系；循环神经网络能有效处理序列数据中关键的序列关系。受此启发，作者采用多粒度扫描的方法来增强级联森林。

{% asset_img multi-grained-scanning.png 多粒度扫描 %}

图4：使用滑动窗口扫描重新表示特征。假设有三个类别，原始特征为400维，滑动窗口为100维。

如图4所示，滑动窗口用于扫描原始特征。假设有400个原始特征，窗口大小为100。对于序列数据，通过滑动一个特征的窗口产生一个100维的特征向量，那么一共产生301个特征向量。假如原始特征具有空间相关性，那么一个$20\times 20$的面板就由400个像素组成，然后一个$10\times 10$的窗口能产生121个特征向量（即121个$10\times 10$的面板）。从训练集的正例或负例中提取的特征向被视为正例或者负例，然后通过这些正例或负例生成类别向量，如3.1节所示，从相同大小的窗口中提取的实例被用于训练完全随机树森林和随机森林，然后生成类别向量，这些向量作为转换后的特征与原始特征进行拼接。如图4所示，假设有3个类别和一个100维的滑动窗口，那么每个森林产生301个3维的类别向量，从而得到与原始400维原始特征向量相对应的1806维变换特征向量。

对于从滑动窗口提取的实例，只需为这些实例分配原始训练示例标签，其中有些标签的分配是不正确的。例如，假设某个原始训练实例是“car”标签的正例，很明显许多提取的实例并不包含“car”，因此这些正例将会被分配为错误的标签。这实际上与翻转输出方法有关，翻转输出方法是集成多样性增强的输出表达操作的代表。

注意，当转换的特征向量太长而无法存储时，可以使用特征采样，例如，可以通过对滑动窗口扫描生成的实例进行子采样，因为完全随机的树不依赖于特征分割选择，而随机的林对不准确的特征分割选择不敏感。这种特征采样过程还与随机子空间方法有关，随机子空间方法是集成多样性增强中输入特征操作的代表。

图4只显示了一种尺寸的滑动窗口。通过使用多个不同大小的滑动窗口，将生成不同粒度的特征向量，如图5所示。

{% asset_img multiple-sizes-windows.png 多尺度滑动窗口 %}

图5：gcForest的整体过程。假设有三个分类需要预测，原始的特征有400维，并且有三个尺度的滑动窗口。

## 总体过程和超参

图5展示了gcForest的整体过程。假设原输入有400个原始特征，在多粒度扫描中有三个滑动窗口大小。如果有$m$个训练实例，大小为100个特征的滑动窗口将会产生由$301 \times m$个100维组成的训练数据集。这些数据将被用作训练完全随机树森林和随机森林，每个森林包含500棵树。假设由三个类别需要被预测，那么按照3.1的描述，就会有得到一个1806维的特征向量。这些训练数据的转换将被用于级联森林的第一级训练。

类似的，如果滑动窗口的大小分别是200和300个特征，那么将会为每个原始训练实例生成1206维和606维的特征向量。将转换后的特征向量与上一级生成的类别向量相结合，分别训练第二级和第三级森林。此过程将重复，直到验证性能收敛。换句话说，最终的模型是一个级联模型，每个级联节点都由多个级别组成，每个级别对应于一个扫描颗粒，如图5所示，第一个级联节点包含从层$1_{A}$到层$1_{C}$。对于不同的任务，用户可以在计算资源允许的情况下尝试更多的扫描颗粒。

给定一个测试实例，通过多粒度扫描得到其对应的转换特征表达，然后通过级联层直到最后一级。通过在最后一级聚合四个3维类向量，并用最大聚集值的类来获得NAR预测。最后的预测将通过在最后一级聚合四个3维类向量，并以最大聚集值的类别来获得。

表1总结了在实验环节使用的深度神经网络和gcForest的默认超参。

表1：默认设置和超参的总结。加粗部分是影响较大的超参；“？”表示默认值未知，或通常需要为不同的任务设置不同的参数。

{% asset_img hyper-parameters.png 默认超参 %}

# 实验

# 相关工作

gcForest是一种决策树集成方法。集成方法是一种强有力的机器学习技术，即对于一个任务集成了多个学习器。事实上，一些研究表明，利用具有深神经网络特征的随机森林等集成方法，其性能甚至优于单纯使用深神经网络，但是，作者使用集成方法目的并不是性能，而是构造一个非深度神经网络模式的深度模型。通过使用级联森林结构，作者希望赋予模型具有逐层处理、模型特征变换和足够的模型复杂度的特性。