---
title: 【2018】Deep Forest
mathjax: true
date: 2019-12-21 19:32:33
updated: {{ date }}
tags: [集成学习, 深度学习]
categories: [论文]
---

# 摘要

目前的深度学习模型大多建立在神经网络的基础上，即通过反向传播进行训练得到的多层参数化可微非线性模块。在本论文中，作者探讨了构建一个不可微模块的深度模型，并且作者推断深度神经网络的成功，其背后有三个特征，分别是逐层处理、模型中的特征转换和足够的模型复杂度。因此，作者提出了gcFroest方法，该方法能够生成具有上面三个特征的深度森林。该方法是一个决策树集成方法，相比于深度神经网络具有更少的参数，并且模型的复杂度能够通过数据依赖的方法来进行调节。实验表明，该方法对于超参具有很强的鲁棒性，在大多数情况下，即使不同领域的数据，该方法也可以在默认设置下获得较好的性能。该研究开创了不可微模块在深度学习中的应用，并且展示了不适用反向传播构建深度学习模块的可能性。

# 介绍

深度学习已经成为各领域的的热点。那么，什么是深度学习？从crowd得到的答案可能是”深度学习是使用深度神经网络的机器学习的子领域“。事实上，深度神经网络（DNNs）再视觉和语音上的成功导致了深度学习的兴起，并且当前所有的深度学习应用都基于神经网络进行构建，或者更专业的说，是通过反向传播训练的多层参数化可微非线性模型。

虽然深度神经网络很强大，但是它有许多的不足。首先，DNNs有许多超参，并且学习的性能依赖于详细的参数调整。事实上，已经有许多学者使用了卷积神经网络，但由于卷积层的结构，他们实际上使用了不同的学习模型来处理不同的操作。事实上，这个问题使得DNNs的训练非常困难，并使得DNNs更像一个艺术，而不是科学或工程，而且DNNs的理论分析也非常困难，因为有太多的干扰因素和几乎无限的参数组合。第二，众所周知，DNNs训练需要大量的训练数据，因此DNNs就很难应用于小规模训练数据量的任务，有时甚至中等规模的训练数据也会失败。需要注意的是，在大数据时代，由于数据标记的成本更好，许多实际的任务缺乏足够数量的数据标记，因此这也导致了DNNs在这些任务中性能较差。众所周知，神经网络是一个黑盒，即决策过程很难被理解，并且学习行为也很难进行理论分析。此外，在训练神经网络之前，并要把网络结构确认，并且预先确定模型的复杂度。作者猜测，深度模型的复杂度通常比深度模型要复杂的多，正如在最近的研究中表明许多DNNs性能的提高是通过快捷链接、剪枝、二值化等方法，因此这些操作都是原来的网络更加简单，并且降低了模型的复杂度。假如模型模型的复杂度能通过数据来动态决定，那就更好了。值得注意的是，尽管DNNs已经发展的很好，但在许多问题上DNNs的表现并不十分优秀，有时甚至不足，例如随机森林、XGBoost仍然在许多Kaggle比赛中获奖。

作者相信为了解决学习任务的复杂度问题，必须对学习模型进行深入研究。然而，目前的深度模型总是建立在神经网络的基础上。基于上面的原因，有充分的理由去探索非神经网络模式的深度模型，换句话说，考虑是否可以与其他模块一起实现深度学习，因为它们有自己的优势，如果能够深入，可能会显示出巨大的潜力。特别是，因为神经网络是多层参数化可微非线性模型，在现实世界中，不是所有的属性都是可微的或最好的模型都是可微的，在本文中，我们试图解决这个基本问题：

”深度学习是否可以使用不可微模型进行实现？“

本文的结果可以帮助理解更多重要的问题，如（1）深度模型是否就是DNNs（或者，深度模型只能够通过可微模型进行构建）；（2）训练深度模型时候可以不适用反向传播？（反向传播必须要可微）；（3）是否有可能有一个深度模型比现有的其他模型（如随机森林或XGBoost）在任务上的表现更好？事实上，机器学习社区已经开发了许多不可微的学习模块，并且理解这些基于不可微模型的深度模型结构将有利于解决这些模块是否可以在深层学习中使用的问题。

本文，作者扩展了他们的初步研究，并提出了使用gcForest（多粒度级联森林）方法来构建深度森林，该方法不同于使用神经网络的深度模型。该模型是一个新颖的具有级联结构的集成决策树，并通过森林进行表达学习。它的表达学习能力可以通过多粒度扫描进一步增强，这使gcForest具有上下文或结构感知能力。其中级联的等级可以被自动确定，使得模型的复杂度能够依赖于数据来进行确认，而不是在训练前手动确定，这使得gcForest也能够在小规模数据上也能表现的很好，并且使得用户能够根据可用的计算资源来控制训练成本。此外，gcForest相比DNNs具有更少的超参。更好的消息是，它的性能对超参数设置非常健壮；通过实验表明，在大多数情况下它能够在默认设置下就获得优异的性能，即使是不同域的不同数据。

# 灵感

## 从DNNs获取的灵感

普遍认为，深度神经网络的成功关键是表达学习能力。那么在DNNs中，表达学习的关键是什么？作者相信是逐层处理。图1提供了一个说明，当层从底部向上时，更高层次的抽象特性会出现。

{% asset_img layer-by-layer-processing.png 深度神经网络中的逐层处理 %}

图1：深度神经网络中的逐层处理：从底部向上，更高层次的抽象特性会出现

如果其他问题不变，那么模型的复杂度越高（即模型容量更大）就会获得更强的学习能力，那么将DNNs的成功性归因于巨大的模型复杂度听起来是合理的。然而，但这不能解释为什么浅层网络不如深层网络来的成功，因为可以通过增加无限的隐藏层单元来提高浅层网络的复杂度。因此，模型的复杂度不能用来说明DNNs的成功。相反，作者推测逐层处理是DNNs中最重要的因素之一，因为在平面网络（例如单隐层网络）中，不管其复杂性有多大，都不具备逐层处理的特性。虽然作者没有严格的论证，但这一猜想为gcForest的设计提供了重要的启示。

但是对于其他的一些学习模型，如决策树和Boosting，这些模型也都是逐层处理，但是它们却没有DNNs成功。作者认为，最重要的区别因素是，与图1所示生成的新特征的DNNs相比，决策树和Boosting在学习过程中总是在原始特征上进行表达，而没有创建新的特征，换句话说，就是模型没有对特征进行转换。此外，DNNs可以被赋予任意高度的模型复杂度，而决策树和Boosting只具有有限的模型复杂度。虽然模型复杂度不能解释DNNs的成功，但它仍然是重要的因素，因为大型模型需要有大型的训练数据。

因此，作者推测DNNs背后三个重要因素是，逐层处理、模型内的特征转换，和足够的模型复杂度。作者将尝试将这些特性赋予非NN模型的深度模型。

## 从集成学习获得的灵感

集成学习是机器学习中的一个范式，即通过多个学习器进行训练，并且通过组合完成一个任务。众所周知，一个集成学习器能够达到更好的性能，比单一的学习器。

为了构建更好的集成学习器，每一个独立学习器必须精准，并且多样。只结合精确的学习器往往不如结合了一些准确的学习器和一些相对较弱的学习器，因为互补性比单纯的准确性更重要。实际上，从误差模糊度分解理论上导出了一个美丽的公式。

\begin{equation}
    E=\bar{E}-\bar{A}
\end{equation}

其中$E$表示一个集成学习器的错误率，$\bar{E}$表示在集成学习器中单独分类器的平均错误率，$\bar{A}$表示各个分类器之间的平均歧义度，也被称为多样性。式子1表示，每个独立分类器越精确，并且多样性越高，那么集成学习器就更好。这为构建一个集成学习器提供了指导，但是该式子不能个作为优化的目标函数，因为歧义项在推导过程中是数学定义的，不能直接操作。后来，集成学习社区设计了许多多样性的度量指标，但是这些多样性的定义没有一个被广泛的接受。事实上，“什么是多样性？”仍是集成学习中的圣杯问题，近期的一些努力可以在其中找到。

在实际应用中，多样性增强的基本策略是在训练过程中引入基于启发式的随机性。粗略地说，主要有四类机制。

>* 1、数据样本操作。它通过生成不同的数据样本来训练每一个学习器。例如，Bagging中的自主采样，AdaBoost中的连续重要性抽样。
>* 2、输入特征操作。它通过生成二重特征子空间来训练每个学习器。例如，通过随机子空间方法为每个学习器随机选择一个子集特征。
>* 3、学习参数操作。它通过为每个学习设置不同参数来生成不同的学习器。例如，为每个单独的神经网络设置不同的初始权值，而不同的分裂选择可以应用于每个单独的决策树。
>* 4、输出表达操作。它通过使用不同的输出表征来产生不同的单个学习器。例如，通过ECOC方法通过纠错编码输出，而反转输出方法则随机改变一些训练实例的标签。

文献表明不同的机制可以一起使用。但值得注意的是，这些机制也不是一直有用。例如，数据样本处理对于稳定的学习器来说效果不好，因为它们的表现不会因为训练数据的轻微修改而显著改变。

下一节将会介绍gcForest，该方法可以被看作决策树的集成方法，几乎使用所有类别的机制来增强多样性。

# gcForest方法