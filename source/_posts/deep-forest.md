---
title: 【2018】Deep Forest
mathjax: true
date: 2019-12-21 19:32:33
updated: {{ date }}
tags: [集成学习, 深度学习]
categories: [论文]
---

# 摘要

目前的深度学习模型大多建立在神经网络的基础上，即通过反向传播进行训练得到的多层参数化可微非线性模块。在本论文中，作者探讨了构建一个不可微模块的深度模型，并且作者推断深度神经网络的成功，其背后有三个特征，分别是逐层处理、模型中的特征转换和足够的模型复杂度。因此，作者提出了gcFroest方法，该方法能够生成具有上面三个特征的深度森林。该方法是一个决策树集成方法，相比于深度神经网络具有更少的参数，并且模型的复杂度能够通过数据依赖的方法来进行调节。实验表明，该方法对于超参具有很强的鲁棒性，在大多数情况下，即使不同领域的数据，该方法也可以在默认设置下获得较好的性能。该研究开创了不可微模块在深度学习中的应用，并且展示了不适用反向传播构建深度学习模块的可能性。

# 介绍

深度学习已经成为各领域的的热点。那么，什么是深度学习？从crowd得到的答案可能是”深度学习是使用深度神经网络的机器学习的子领域“。事实上，深度神经网络（DNNs）再视觉和语音上的成功导致了深度学习的兴起，并且当前所有的深度学习应用都基于神经网络进行构建，或者更专业的说，是通过反向传播训练的多层参数化可微非线性模型。

虽然深度神经网络很强大，但是它有许多的不足。首先，DNNs有许多超参，并且学习的性能依赖于详细的参数调整。事实上，已经有许多学者使用了卷积神经网络，但由于卷积层的结构，他们实际上使用了不同的学习模型来处理不同的操作。事实上，这个问题使得DNNs的训练非常困难，并使得DNNs更像一个艺术，而不是科学或工程，而且DNNs的理论分析也非常困难，因为有太多的干扰因素和几乎无限的参数组合。第二，众所周知，DNNs训练需要大量的训练数据，因此DNNs就很难应用于小规模训练数据量的任务，有时甚至中等规模的训练数据也会失败。需要注意的是，在大数据时代，由于数据标记的成本更好，许多实际的任务缺乏足够数量的数据标记，因此这也导致了DNNs在这些任务中性能较差。众所周知，神经网络是一个黑盒，即决策过程很难被理解，并且学习行为也很难进行理论分析。此外，在训练神经网络之前，并要把网络结构确认，并且预先确定模型的复杂度。作者猜测，深度模型的复杂度通常比深度模型要复杂的多，正如在最近的研究中表明许多DNNs性能的提高是通过快捷链接、剪枝、二值化等方法，因此这些操作都是原来的网络更加简单，并且降低了模型的复杂度。假如模型模型的复杂度能通过数据来动态决定，那就更好了。值得注意的是，尽管DNNs已经发展的很好，但在许多问题上DNNs的表现并不十分优秀，有时甚至不足，例如随机森林、XGBoost仍然在许多Kaggle比赛中获奖。

作者相信为了解决学习任务的复杂度问题，必须对学习模型进行深入研究。然而，目前的深度模型总是建立在神经网络的基础上。基于上面的原因，有充分的理由去探索非神经网络模式的深度模型，换句话说，考虑是否可以与其他模块一起实现深度学习，因为它们有自己的优势，如果能够深入，可能会显示出巨大的潜力。特别是，因为神经网络是多层参数化可微非线性模型，在现实世界中，不是所有的属性都是可微的或最好的模型都是可微的，在本文中，我们试图解决这个基本问题：

”深度学习是否可以使用不可微模型进行实现？“

本文的结果可以帮助理解更多重要的问题，如（1）深度模型是否就是DNNs（或者，深度模型只能够通过可微模型进行构建）；（2）训练深度模型时候可以不适用反向传播？（反向传播必须要可微）；（3）是否有可能有一个深度模型比现有的其他模型（如随机森林或XGBoost）在任务上的表现更好？事实上，机器学习社区已经开发了许多不可微的学习模块，并且理解这些基于不可微模型的深度模型结构将有利于解决这些模块是否可以在深层学习中使用的问题。

本文，作者扩展了他们的初步研究，并提出了使用gcForest（多粒度级联森林）方法来构建深度森林，该方法不同于使用神经网络的深度模型。该模型是一个新颖的具有级联结构的集成决策树，并通过森林进行表达学习。它的表达学习能力可以通过多粒度扫描进一步增强，这使gcForest具有上下文或结构感知能力。其中级联的等级可以被自动确定，使得模型的复杂度能够依赖于数据来进行确认，而不是在训练前手动确定，这使得gcForest也能够在小规模数据上也能表现的很好，并且使得用户能够根据可用的计算资源来控制训练成本。此外，gcForest相比DNNs具有更少的超参。更好的消息是，它的性能对超参数设置非常健壮；通过实验表明，在大多数情况下它能够在默认设置下就获得优异的性能，即使是不同域的不同数据。

# 灵感