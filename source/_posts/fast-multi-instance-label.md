---
title: 【2018】Fast Multi-Instance Multi-Label Learning
mathjax: true
date: 2020-02-09 09:53:06
updated: {{ date }}
tags: [多标签学习, 深度学习]
categories: [论文, 多标签学习]
---

# 摘要

在许多实际任务中，特别是那些涉及图像、文本等语义复杂的数据对象的任务中，一个对象可以用多个实例表示，同时与多个标签相关联。这些任务都可以表示为多实例多标签学习（MIML）问题，并且近几年得到了广泛的研究。现有的MIML方法已经在许多应用中得到证明是有用的；然而，这些方法中的大部分只能处理中等规模的数据。为了有效的处理大型数据集，本文作者提出了“MIML-Fast”方法，该方法首先构建了一个所有标签共享的低维子空间，然后通过随即梯度下降（SGD）来训练特定的标签线性模型来近似标签的排序损失。尽管MIML问题非常复杂，但是MIML-Fast能够利用共享空间中标签之间的关系发现复杂的标签子关系，从而获得优秀的性能。试验表明，MIML-Fast的性能与目前最先进的技术相当，但是却花费更少的时间。此外，MIML-Fast方法能够识别每个标签最具代表性的实例，从而提供了一个了解输入模式和输出标签语义之间关系的机会。

# 介绍

在传统的监督学习中，一个对象表示为一个实例，并且只与一个标签相关联。然而，在许多实际应用中，一个对象可以由多个实例来描述，并且关联多个标签分类。例如，在图像分类任务中，一个图像可以与多个语义标签相关联，并且可以分成多个段落，每个段落由一个实例来表示，如图1所示。在文本分类中，一篇文章可能属于多个分类，并且可以使用一个实例包来进行表示，每个段落代表一个实例；在基因功能预测人物中，一个基因通常与多个标签关联，因为这个基因与多个功能相关，并且可以通过一组不同视图的图像来表达。因此，MIML学习是近几年提出的面向复杂对象的学习框架。

{% asset_img miml-example.png MIML实例 %}

图1：MIML实例：图像由多个实例表示，并与多个标签关联

在过去几年，许多MIML算法被提出，并且其中的一些方法是把任务进行简化。例如，MIMLBoost把MIML任务简化为彝族多实例单标签的学习任务；MIMLSVM则通过把一个实例包转化为单实例来把任务简化为多标签学习任务。而DMIMLSVM方法则直接优化了MIML任务中的损失函数。还有其他相关的工作来试图使用现有的技术来解决MIML任务。

该方法获得了良好的性能，并且验证了MIML框架在各种应用中的优越性。然而，随着表达能力的增强，MIML的假设空间急剧膨胀，导致现有方法较高的复杂性和较低的效率。这些方法通常比较耗时，并且不能处理大规模数据，严重限制了应用MIML的学习。

为了克服这个问题，本文中，作者提出了一个全新的方法“MIML-Fast”来加速在MIML中数据的学习。尽管为提高效率采用了简单的线性模型，但MIML-Fast可以有效地近似原始的MIML问题。具体来说，为了有效利用多标签之间的关系，作者首先从所有标签的原始特征中得到一个共享空间，然后从共享空间中训练一个标签的线性模型。为了识别具有关键特征的标签实例包，作者训练了一个用于划分实例等级的分类模型，并且选择具有最大预测值的实例作为关键实例。为了使学习的高效性，作者使用了随机梯度下降（SGD）来优化排序损失。在SGD的每步中，MIML-Fast随即采样了一个元组，其中包含一个包，一个包相关的标签和一个不相关的标签，并且优化模型，使其排在不相关标签之前。与目前最先进的MIML方向相比，MIML-Fast具有很强的竞争性，并且在大数据集上的性能提高了100倍。

虽然大多数现有方法主要侧重于改进方法的泛化性，但MIML学习的另一个重要任务是理解输入模式和输出标签语义之间的相关性，即利用实例和标签之间的对应关系。作者的方法能够根据实例级别的预测来得到每个标签最典型的实例。

此外，我们的方法尝试利用标签的子关系来处理复杂的标签。通过对每个标签自适应多个模型，该方法可以区分复杂标签中嵌入的不同子概念。

# 相关工作

作者首先回顾了两个与MIML相关的学习框架，分别是多标签多系和多实例学习。

在多标签学习中，每个对象都可以表示为一个实例，并且这个实例有多个相关的标签。在过去许多年中多标签学习已经被很好的研究，并且提出了许多算法。目前的方法可以被分为两类，分别是问题转换法和算法适配法。在问题转换法中，最简单的方法就是把多标签任务分解为一串二分类问题，每一个问题对应一个标签，每个标签不具有相关性。而其他方法则试图把多标签任务转化为多分类问题，其中每个类都都可能是一个标签的子集。在算法适配法中尝试将流行的学习技术应用到多标签中。代表方法包括Boosting风格算法AdaBoost.MH，惰性学习算法ML-KNN，基于决策树的算法ML-DT，基于神经网络的算法。此外，弱监督的多标签学习已经在研究，无论是主动查询还是局部标注。近年来，如果利用标签之间的关系越来越引起研究者的兴趣。这方面的工作包括利用标签结构的先验知识和自动挖掘标签之间的相关性。

多实例学习最初是由Dietterich等人提出的，应用于药物活性预测。在该框架下，每个对象都可以用一组实例来进行描述，并与一个标签进行关联。学习任务在没有实例注释的情况下是弱监督的，因此具有相当大的挑战性。许多流程的机器学习方法已经用于多实例的表达。例如，决策树算法MITI，核方法Mi-Kernel和边缘化的Mi-Kernel，惰性学习算法Citation-knn和Bayesian-knn，以及集成方法HSMILE。

MIML学习是一个更为通用的框架。如文献中所述，多学习框架的存在是由于在表达现实世界的对象中存在歧义性。MIML同时考虑了输入和输出空间的歧义性，因此更自然和方便地处理涉及此类对象的任务。图2总结了四种学习框架之间的差异。

{% asset_img four-different-learning-frameworks.png 四种学习框架之间的差异 %}

图2：四种学习框架之间的差异

过去几年许多MIML方法被提出。例如，MIMLSVM会把MIML问题转化为单实例多标签任务来解决。MIMLBoost把MIML问题转化为多实例单标签任务来解决。一种MIML的生成模型被杨等人提出。文献中还提出了最近邻和神经网络方法。查等人提出了一种用于MIML图像标注的隐藏条件随机场模型。Briggs等人提出了在MIML标识时的优化排序损失。在文献中，作者试图通过为每个带聚类的标签构造一个原型来发现哪些模式触发了MIML学习中的哪些标签。最近，有一些研究试图将MIML框架扩展到新的环境中，包括类扩展、多视图学习和实例聚类。现有的MIML方法已经成功应用在许多场景中，由于计算量大，所以大多数应用于中等规模的数据。要处理大规模的数据，MIML就需要高效的方法。

在查阅前期的文章中，一些新的研究被提出。如文献31中提出了一种判别概率模型。该方法侧重于实例的标注，而不是在MIML环境下的包标注，并且提出了一种计算实例标签后验概率的动态规划方法。在文献12中，作者提出了一种深度MIML模型，该模型自动学习实例的描述，而不是手工设计实例的描述。在文献43中，另一个解决MIML学习的名叫MIML-FCN+的深度模型被提出。在文献1中，提出了一种具有多变量性能度量的可扩展优化方法。此外，激活标签查询和新类别发现也在MIML学习环境下被研究。

有一些研究试图通过排名来实现分类。在文献40中，一个类似的技术被用于优化图片标注中的WARP损失；然而，它处理的是单实例单标签问题，这与我们的MIML问题大不相同。在文献55中，提出了一种基于聚类的复杂概念中子概念发现的方法。然而，该方法关注的是但标签学习，也与我们的MIML任务不同。在MIML-Fast方法中利用标签信息，使用监督模型而不是启发式聚类来发现子概念。

# MIML-Fast方法

假设$\left\{\left(X_{1}, Y_{1}\right),\left(X_{2}, Y_{2}\right), \cdots,\left(X_{n}, Y_{n}\right)\right\}$是一个MIML的数据集，其中有$n$个实例集合包，每个包$X_{i}$都有$z_{i}$个实例$\left\{\mathbf{x}_{i, 1}, \mathbf{x}_{i, 2}, \cdots, \mathbf{x}_{i, z_{i}}\right\}$，而$Y_{i}$包含了$X_{i}$所有相关的标签，并且这些标签可能是所有可能标签$\left\{y_{1}, y_{2} \cdots y_{L}\right\}$的一个子集。在本节的其余部分中，作者将首先介绍所提出的MIML预测模型，然后介绍学习任务的目标。其次，介绍提出的快速算法来优化目标函数。最后，为了进一步利用实例的标签信息，提出了一种改进的算法。

## 预测模型

首先讨论如何在实例等级上建立分类模型，然后尝试从实例预测结果中得到实例包所对应的标签集。要处理多标签问题，最简单的方法就是为每个不相关的标签训练一个模型，并把多标签问题转化为一串单标签问题。然而，这种方法将会丢失标签之间相关性的信息，因为这种方法独立的对待标签，并且忽略标签之间的相关性信息。在作者的方法中，我们将模型表示为两个组件的组合。第一个组件是学习一个从原始特征空间到低维空间的线性映射，该空间与所有标签共享。第二个组件是基于这个共享空间学习一组特定的标签模型。这两个组件以交互方式优化，以适应所有标签中的训练示例。

从形式上来说，给定实例$\mathbf{x}$，定义一个在标签$l$上的分类模型

$$f_{l}(\mathbf{x})=\mathbf{w}_{l}^{\top} W_{0} \mathbf{x}$$

其中$W_{0}$是一个把原始特征向量映射到共享空间的$m \times d$维矩阵，$\mathbf{w}_{l}$是标签$l$的$m$维权重向量。$d$和$m$分别是特征空间和共享空间的维数。图3展示了两层模型。这种模型有两个显著的优势。首先，通常$m \ll d$，需要学习变量数从$d \times L$降到了$(d+L) \times m$。这将是的内存和计算消耗都显著降低。其次，标签之间的关系也可以被利用。每个标签的实例包都有助于共享空间的优化，并且具有强关系的标签可以相互帮助。例如，假设$l$是罕见的标签，由于只有很少的正例，所以该标签能难训练一个精准的模型，然而在我们的模型中，通过拟合其他标签的实例包，共享空间（$W_{0}$）可以得到足够的训练，因为通过具有较好训练结果的$W_{0}$来优化$\mathbf{w}_{l}$要容易的多。

{% asset_img two-level.png 两层模型 %}

图3：两层模型：$W_{0}$表示从原始特征向量映射到共享子空间，$\mathbf{w}_{l}$是标签$l$的权重向量

然后作者改进模型来处理复杂标签中子关系。在MIML学习任务中，对象通常具有复杂的语义，因此可以为内容多样的实例包分配相同的标签。图4展示了一个在图像分类任务中具有多个子概念的复杂标签的例子。在图中，不同形状（圆形、方形、三角形、星形）表示属于不同标签的实例集，不同颜色表示不同子概念。正如所看到的，同一标签山的图像内容可以是一座沙山、一座雪山或一座树木覆盖的山。很难训练单个模型（对应于虚线红线）将内容如此多样化的图像分类到同一类别中。相反，作者建议一个标签学习多个模型，为每个子概念学习一个模型，并自动决定一个示例属于哪个子概念。每个子概念的模型（对应于黑色实线）都非常简单，并且可以很容易的拟合数据。假设每个标签有$K$个子概念。对于具有标签$l$的实例，首先检查$K$个模型的预测值，然后选择具有最大预测值的子概念来自动确定所属的子概念。现在可以将具有标签$l$的实例$\mathbf{x}$预测重新定义为：

\begin{equation}
f_{l}(\mathbf{x})=\max _{k=1 \cdots K} f_{l, k}(\mathbf{x})=\max _{k=1 \cdots K} \mathbf{w}_{l, k}^{\top} W_{0} \mathbf{x}
\end{equation}

其中$\mathbf{w} \iota, k$表示标签$l$的第$k$的子概念。需要注意的是，虽然假设每个标签有$K$个子概念，但是允许空的子概念，即简单标签的示例可以仅分布在几个甚至一个子概念中。

{% asset_img sub-concepts.png 一个子概念的例子 %}

图4：一个子概念的例子：带有“山”标签的图像可以是沙山、雪山或树木覆盖的山。

然后，接下来是如何从实例等级模型中获得每个包的预测。通常假设当且仅当至少包含一个正实例时，那么这个包才是正例。在这个假设下，一个在标签$l$的包$X$的预测能够被定义为在该包中所有实例预测的最大值：

$$f_{l}(X)=\max _{\mathbf{x} \in X} f_{l}(\mathbf{x})$$

称最大预测的实例为具有标签$l$的包$X$的典型实例。

## 目标函数

作者把MIML分类问题转化为一个标签的排序问题，其目的是将所有实例的相关标签排在不相关标签之前。根据前面章节介绍的模型，对于实例$X$和一个相关标签集$l$，可以定义$R(X, l)$为：

\begin{equation}
R(X, l)=\sum_{j \in \bar{Y}} I\left[f_{j}(X)>f_{l}(X)\right]
\end{equation}

其中$\bar{Y}$表示$X$的不相关标签集，而$I[\cdot]$是一个指示函数，当参数为真时返回1，否则就返回0。本质上，$R(X, l)$计算的是在排在包$X$的相关标签$l$之前有多少个不相关的标签。

基于$R(X, l)$就可以定义实例$X$的相关标签$l$的排序误差损失函数：

\begin{equation}
\epsilon(X, l)=\sum_{i=1}^{R(X, l)} \frac{1}{i}
\end{equation}

很明显，如果标签$l$的排序很低，那么就说明排序误差$\epsilon$很大。最后，在所有数据集上就可以得到排序误差损失为：

$$\text { Rank Error }=\sum_{i=1}^{n} \sum_{l \in Y_{i}} \epsilon(X, l)$$

基于式子2，排序误差损失函数$\epsilon(X, l)$可以扩展到所有不相关标签$\bar{Y}$：

\begin{equation}
\epsilon(X, l)=\sum_{j \in \bar{Y}} \epsilon(X, l) \frac{I\left[f_{j}(X)>f_{l}(X)\right]}{R(X, l)}
\end{equation}

由于非凸性和不连续性，直接优化上述方程是相当困难的，因为这样的优化常常导致NP难问题。相反，我们研究了下面的Hinge损失，它被证明是所有凸代理损失中的最佳选择：

\begin{equation}
\Psi(X, l)=\sum_{j \in \bar{Y}} \epsilon(X, l) \frac{\left|1+f_{j}(X)-f_{l}(X)\right|_{+}}{R(X, l)}
\end{equation}

其中，当$q \geq 0$时，$|q|_{+}=q$，否则$|q|_{+}=0$。代理损失函数$\Psi(X, l)$可以视为$\epsilon(X, l)$损失函数的上界，因为$I[q] \leq|1+q|_{+}$。

为了将相关标签排在不相关标签之前，我们只需要最小化训练数据的排序损失，从而得到以下目标函数：

\begin{equation}
\min \sum_{i=1}^{n} \sum_{l \in Y_{i}} \Psi\left(X_{i}, l\right)
\end{equation}

## 算法

作者使用随即梯度下降（SGD）来最小化排序损失函数。在SGD每次迭代过程中，作者随即采样一个实例包$X$，一组相关标签$y$，以及一组不相关标签$\bar{y} \in \bar{Y}$，形成一个元组$(X, y, \bar{y})$，就可以得到损失函数：

\begin{equation}
\mathcal{L}(X, y, \bar{y})=\epsilon(X, y)\left|1+f_{\bar{y}}(X)-f_{y}(X)\right|_{+}
\end{equation}

结下来通过引理来揭示$\Psi(X, y)$和$\mathcal{L}(X, y, \bar{y})$之间的关系。

**引理1：**$\Psi(X, y)=E_{\bar{y}}[\mathcal{L}(X, y, \bar{y})]$，其中$E[\cdot]$表示$\bar{Y}$上均匀分布的期望值。

证明：这个引理来源于在$\bar{Y}$中随机选择$\bar{y}$的概率是$\frac{1}{R(X, y)}$。

要最小化$\mathcal{L}(X, y, \bar{y})$，就需要提前计算$R(X, y)$，即必须对每个$\bar{y} \in \bar{Y}$比较$f_{y}(X)$和$f_{\bar{y}}(X)$，而当可能的标签数量很大时，这可能会很耗时。因此，受到Weston等人的启发，作者使用近似来估计$R(X, y)$。特别是在SGD的每步迭代中中，作者随即从不相关的标签集$\bar{Y}$逐个随机采样标签集，直到$\bar{y}$标签出现。如果某个标签出现在$y$标签之前，那么就称$\bar{y}$为错误标签，例如$f_{\bar{y}}(X)>f_{y}(X)-1$。为了不是损失函数不失一般性，假设第一个错误标签被发现在第$v$步的采样过程，然后$R(X, y)$可以按照下面的引理通过$\lfloor|\bar{Y}| / v\rfloor$进行近似：

**引理2：**作者使用$\theta$表示一个随机事件，$\theta=i$表示第一个错误标签的事件在第i个采样步骤。就可以得到：

$$\frac{R(X, y)}{|\bar{Y}|} \approx E_{\theta}\left[\frac{1}{\theta}\right]$$

证明：为了方便，作者设置$p=\frac{R(X, y)}{|\bar{Y}|}$，并且假定$0 < p < 1$。可以很容易得出，当$i \geq 1$时的概率：

$$\operatorname{Pr}[\theta=i]=(1-p)^{i-1} p$$

并且可以得到

$$
\begin{aligned}
E_{\theta}\left[\frac{1}{\theta}\right] &=\sum_{i=1}^{\infty} \frac{1}{i} p(1-p)^{i-1}=\frac{p}{1-p} \sum_{i=1}^{\infty} \frac{1}{i}(1-p)^{i} \\
&=\frac{-p}{1-p} \ln (1-(1-p)) \approx p
\end{aligned}
$$

其中，$\sum_{i=1}^{\infty} \frac{1}{i}(1-p)^{i}=-\ln (p)$，并且$\ln (1+q) \approx q$。证明完成。

作者假设在第$t$步的SGD迭代中得到采样的元组为$(X, y, \bar{y})$，其中相关标签为$y$，关键实例为$x$，并在第$k$个子概念上获得了最大预测，而在标签$\bar{y}$上，实例$\bar{x}$在第$k$个子概念上获得了最大的预测。然后就可以得到了元组的近似排序损失：

$$
\mathcal{L}(X, y, \bar{y})=\epsilon(X, y)\left|1+f_{\bar{y}}(X)-f_{y}(X)\right|_{+} \approx \left\{\begin{matrix}
0 & \bar{y} \neq \text{violated}\\
S_{\bar{Y}, v}\left(1+\left[\mathbf{w}_{\bar{y}, \bar{k}}^{t}\right]^{\top} W_{0}^{t} \overline{\mathbf{x}}-\left[\mathbf{w}_{y, k}^{t}\right]^{\top} W_{0}^{t} \mathbf{x}\right) & \bar{y} = \text{violated}
\end{matrix}\right.
$$

这里为了方便介绍，作者引入了$S_{\bar{Y}, v}=\sum_{i=1}^{\left\lfloor\frac{|\bar{Y}|}{v}\right\rfloor} \frac{1}{i}$。因此，如果对错误标签$\bar{y}$进行采样，作者根据下面三个参数进行梯度下降：

\begin{equation}
W_{0}^{t+1}=W_{0}^{t}-\gamma_{t} S_{\bar{Y}, v}\left(\mathbf{w}_{\bar{y}, \bar{k}}^{t} \overline{\mathbf{x}}^{\top}-\mathbf{w}_{y, k}^{t} \mathbf{x}^{\top}\right)
\end{equation}

\begin{equation}
\mathbf{w}_{y, k}^{t+1}=\mathbf{w}_{y, k}^{t}+\gamma_{t} S_{\bar{Y}, v} W_{0}^{t} \mathbf{x}
\end{equation}

\begin{equation}
\mathbf{w}_{\bar{y}, \bar{k}}^{t+1}=\mathbf{w}_{\bar{y}, \bar{k}}^{t}-\gamma_{t} S_{\bar{Y}, v} W_{0}^{t} \overline{\mathbf{x}}
\end{equation}

其中$\gamma_{t}$是第$t$次迭代的SGD步长。在更新完参数后，$\mathbf{w}_{y, k}$和$\mathbf{w}_{\bar{y}, \bar{k}}$，以及$W_{0}$的每列进行归一化，使其L2范式小于常数$C$。

MIML-Fast伪代码如算法1所示。首先，$W_{0}$的每列和所有标签$y$的权重$\mathbf{w}_{y}^{k}$，以及所有子概念$k$将被随机初始化，并且平均值为0，标准差为$1 / \sqrt{d}$。然后在SGD的每步迭代中，元组$(X, y, \bar{y})$是随即采样的，并且他们相应的代表实例和子概念会被识别出来。之后，按照式8-10执行梯度下降来更新这三个参数$W_{0}$、$\mathbf{w}_{y, k}$和$\mathbf{w}_{\bar{y}, \bar{k}}$。最后，对更新后的参数进行归一化，使其范数小于常数$C$。重复此过程，直到达到某些停止标准。在作者的实验中，作者从训练数据中抽取一小部分样本组成一个验证集，如果验证集上的排名损失不再减少，则停止训练。

{% asset_img mimlfast-alg.png MIML-Fast伪代码 %}

在算法的测试阶段，对于测试包$X_{\text {test }}$，可以得到在每个标签上的预测，并且得到相应所有标签的排序。对于单标签分类问题，可以非常简单的通过选择预测值中最大的一个就可以得到$X_{\text {test }}$的标签。但是，在多标签学习中，包$X_{\text {test }}$可能至少一个以上的相关标签，并且不知道因该从标签排序列表中选择多少个标签作为相关标签。为了解决这个问题，作者为每个包赋予一个虚拟的标签（表示为$\hat{y}$），并且训练模型中，所有无关标签在虚拟标签之后，所有相关标签排在虚拟标签之前。为了实现这一想法，作者特别考虑了构造不相关标签集$\bar{Y}$。具体来说，当实例包$X$及其标签$y$被采样时（在算法1的第6行中），算法将首先检查$y$是否是虚拟标签。如果$y=\hat{y}$，那么$\bar{Y}$包含所有的不相关标签，否则$\bar{Y}$既包含虚拟标签，也包含不相关标签。使用这种方法，模型将被训练在相关标签和不相关标签之间排列虚拟标签。对于测试实例包，选择预测值大于假标签上预测值的标签作为相关标签。

最后，给出了算法收敛速度的一些理论保证。具体如下：

$$\mathcal{L}_{t}\left(W_{0}, \mathbf{w}_{y, k}, \mathbf{w}_{\bar{y}, \bar{k}}\right)=S_{\bar{Y}, v}\left(1+\mathbf{w}_{\bar{y}, \bar{k}}^{\top} W_{0} \overline{\mathbf{x}}_{t}-\mathbf{w}_{y, k}^{\top} W_{0} \mathbf{x}_{t}\right)_{+}$$

上式表示用模型参数$W_{0}$、$\mathbf{w}_{y, k}$和$\mathbf{w}_{\bar{y}, \bar{k}}$和

$$\left(W_{0}^{*}, \mathbf{w}_{l, k}^{*}\right) \in \arg \min \sum_{t} \mathcal{L}_{t}\left(W_{0}, \mathbf{w}_{y, k}, \mathbf{w}_{\bar{y}, \bar{k}}\right)$$

进行的第$t$次SGD迭代损失是最优解，我们有以下定理。

**定理1：**

证明：

通过选择合适的初始值和简单的计算。这个定理如所愿。

## 使用实例标签进行学习

上面介绍的算法可以为每个标签识别一个实例包的典型实例。在一些MIML任务中，除了实例包标签外，每个标签的关键实例是已知的。例如，图像的标记任务中，不仅要为图像标记，还要为每个标记标识相应的区域。在这种情况下，调整算法来进一步利用这些监督信息。基本思想是将每个相关标签的关键实例排在其他实例之前。形式上，给定一个包实例$X$及其相关标签中的一个$y$，其代表实例$\mathbf{x}^{*}$的排序损失可以定义为：

\begin{equation}
\xi\left(y, \mathbf{x}^{*}\right)=\sum_{i=1}^{R\left(y, \mathbf{x}^{*}\right)} \frac{1}{i}
\end{equation}

其中

\begin{equation}
R\left(y, \mathbf{x}^{*}\right)=\sum_{\mathbf{x} \in X} I\left[f_{y}(\mathbf{x})>f_{y}\left(\mathbf{x}^{*}\right)\right]
\end{equation}

统计在标签$y$上的代表实例之前排列的实例数。显然，当代表实例$\mathbf{x}^{*}$在标签$y$上最好的预测值时，则误差为0。与标签排序类似，如果实例$\tilde{\mathbf{x}}$排在典型实例$\mathbf{X}^{*}$之前，那么将会引入代理排序损失函数如下：

\begin{equation}
\mathcal{J}\left(y, \mathbf{x}^{*}, \tilde{\mathbf{x}}\right)=\xi\left(y, \mathbf{x}^{*}\right)\left|1+f_{y}(\tilde{\mathbf{x}})-f_{y}\left(\mathbf{x}^{*}\right)\right|_{+}
\end{equation}

根据第3.3节中类似的偏差，我们可以使用以下梯度下降规则来更新模型：

\begin{equation}
W_{0}^{t+1}=W_{0}^{t}-\gamma_{t} \xi\left(y, \mathbf{x}^{*}\right) \mathbf{w}_{y, k}^{t}\left(\tilde{\mathbf{x}}^{\top}-\mathbf{x}^{* \top}\right)
\end{equation}

\begin{equation}
\mathbf{w}_{y, k}^{t+1}=\mathbf{w}_{y, k}^{t}-\gamma_{t} \xi\left(y, \mathbf{x}^{*}\right) W_{0}^{t}\left(\tilde{\mathbf{x}}-\mathbf{x}^{*}\right)
\end{equation}

需要注意的是，由于一个实例包中的实例数量通常不多，因此可以根据公式11有效地计算$\xi\left(y, \mathbf{x}^{*}\right)$。在实例包较大情况下，也可以通过引理2的得到近似值。总之，当实例标签集可用时，可以组合式子14-15和式子8-10来共同优化标签和实例的排名。

# 实验

# 结论

MIML是一种面向复杂对象的学习框架，在许多应用中都被证明是有效的。然而，现有的MIML方法通常过于耗时，无法处理大规模的问题。本文提出了快速学习的MIML-Fast方法，并用MIML实例进行了快速学习，扩展了我们的初步研究。一方面，通过基于两级线性模型的SGD优化近似排序损失，有效地提高了效率；另一方面，通过利用共享空间中的标签关系和发现复杂标签的子概念来实现有效性。此外，作者的方法可以自然地检测每个标签的关键实例，从而提供了一个发现输入模式和输出标签语义之间关系的机会。在未来，作者将尝试优化其他损失函数，而不是排名损失。此外，还将研究更大规模的问题。