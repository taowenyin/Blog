---
title: AutoEncoder理解和推导
mathjax: true
date: 2020-07-01 20:24:37
updated: {{ date }}
tags: [机器学习]
categories: [机器学习]
---

自编码器是神经网络的一种，经过训练后能尝试将输入复制到输出。自编码器内部由一个隐藏层$h$，可以产生编码表示输入。该网络可以看作由两部分组成：一个由函数$h=f(x)$表示的编码器和一个生成重构的解码器$r=g(h)$，其目的是尽量使$g(f(x))=x$。此外，还可以扩展到随机性概率分布$p_{encoder}(h|x)=x \to h$和$p_{decoder}(x|h)=h \to x$。

{% asset_img autoencoder-structure.png 自编码器的一般结构 %}

# 欠完备自编吗器

从自编码器获得有用特征的一种方法是**限制$h$的维度比$x$小**，这种编码维度小于输入维度的自编码器称为欠完备自编码器。学习欠完备自编码器的表示将强制自编码器捕捉训练数据中最显著的特征。其最小化损失函数为

$$L(x,g(f(x)))=\frac{1}{N} \sum_{i=1}^{N}(x-g(f(x)))^{2}$$

即输入数据与经自编码器重构后的数据之间的误差。**损失函数一般分为经验风险损失函数与结构风险损失函数。** 经验风险损失函数指预测结果和实际结果之间的差别，如上式就是经验风险损失函数。结构风险损失函数是在经验风险损失函数上添加一个正则项，如L0、L1、L2和稀疏正则项等。常用的损失函数有六种，分别是0-1损失、绝对值损失函数函数、平方损失函数、对数损失函数、指数损失函数、铰链损失函数。

当解码器是线性的且$L$是均方误差（如上式），**欠完备的自编码器会学习出与PCA相同的生成子空间**，即训练数的主元子空间。不幸的是，**如果编码器和解码器被赋予过大的容量，自编码器会执行复制任务而捕捉不到任何有关数据分布的有用信息。**

# 正则自编码器

通过正则自编码器，即使模型容量大到足以学习一个无意义的恒等函数，非线性且过完备的正则自编码器仍然能够从数据中学到一些关于数据分布有用的信息。

## 稀疏自编码器

稀疏自编码器在训练时结合编码层的稀疏惩罚项$\Omega (h)$和重构误差

$$L(x, g(f(x)))+\Omega (h)$$

其中$g(h)$是解码器的输出$r$，通常$h$是编码器的输出特征，即$h=f(x)$，惩罚项为L1范数$\Omega(h)=\lambda \sum_{i}\left|h_{i}\right|$。

1、使用$a_{j}^{(2)}$表示第2层第$j$个神经元的激活度；

2、使用$a_{j}^{(2)}(x)$表示在给定输入值为$x$的情况下，第2层第$j$个神经元的激活度；

3、使用$\hat{\rho}_{j}=\frac{1}{m} \sum_{i=1}^{m}\left[a_{j}^{(2)}\left(x^{(i)}\right)\right]$表示第2层第$j$个神经元在训练集上的平均活跃度；

4、使用$\rho$表示稀疏性参数，通常是一个接近0的值（如$\rho=0.05$），可以令$\hat{\rho}_{j}=\rho$，来对神经元$a_{j}^{(2)}$的稀疏性进行限制，**即$\hat{\rho}_{j}$与$\rho$越接近越好，因此就需要使用KL散度来衡量他们之间的差异。**

KL散度又称相对熵，是对两个概率分布$P$和$Q$差异的非对称性度量，非对称性意味着$D(P \| Q) \neq D(Q \| P)$，$D(P \| Q)$表示用概率分布$Q$来拟合概率分布$P$时所产生的信息损耗。假设由随机变量$x$，那么

**1、离散性随机变量的散度**为$D(P \| Q)=\sum\left(p(x) \log \left(\frac{p(x)}{q(x)}\right)\right)$，其中$p$和$q$表示随机变量的分布，$p(x)$表示随机变量取$x$的概率。

**2、连续性随机变量的散度**为$D(P \| Q)=\int p(s) \log \left(\frac{p(x)}{q(x)}\right) d(s)$，其中$p$和$q$表示随机变量$x$的概率分布。

当$P=Q$时，$D=0$，并且当$P$和$Q$的差异增大时，$D$值递增。

因此，在稀疏子编码器中，使用$\hat{\rho}_{j}$来逼近$\rho$，即KL散度为

$$\sum_{j=1}^{N} K L\left(\rho \| \hat{\rho}_{j}\right)=\sum_{j=1}^{N} \rho \log \frac{\rho}{\hat{\rho}_{j}}+(1-\rho) \log \frac{1-\rho}{1-\hat{\rho}_{j}}$$

此时的惩罚函数就是$\Omega(h)=\sum_{j=1}^{N} K L\left(\rho \| \hat{\rho}_{j}\right)$。

## 去噪自编码器

去噪自编码器是一类接受损坏数据作为输入，并训练来预测原始未被损坏数据作为输出的自编码器，其本质是最小化

$$L(x, g(f(\tilde{x})))$$

其中$\tilde{x}$是被某种噪声破坏的$x$的副本，即可以加入高斯噪声等。去噪自编码器是强制$f$和$g$隐式的学习$p_{data}(x)$的结构，然后通过最小化重构误差来获得有用的特征，基本步骤分三步：

1、从训练数据中采一个训练样本$x$；

2、从损坏过程$C(\tilde{x}|x=x)$采一个损坏样本$\tilde{x}$；

3、将$(x,\tilde{x})$作为训练样本来估计自编码器的重构分布$p_{reconstruct}(x|\tilde{x})=p_{decoder}(x|h)$，其中$h$是编码器$f(\tilde{x})$的输出，$p_{decoder}$根据解码器函数$g(h)$定义。

通常采用梯度法（如小批量梯度下降）来近似最小化。

{% asset_img dae-structure.png 去噪自编码器的一般结构 %}


