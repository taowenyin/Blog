---
title: AutoEncoder理解和推导
mathjax: true
date: 2020-07-01 20:24:37
updated: {{ date }}
tags: [机器学习]
categories: [机器学习]
---

自编码器是神经网络的一种，经过训练后能尝试将输入复制到输出。自编码器内部由一个隐藏层$h$，可以产生编码表示输入。该网络可以看作由两部分组成：一个由函数$h=f(x)$表示的编码器和一个生成重构的解码器$r=g(h)$，其目的是尽量使$g(f(x))=x$。此外，还可以扩展到随机性概率分布$p_{encoder}(h|x)=x \to h$和$p_{decoder}(x|h)=h \to x$。

{% asset_img autoencoder-structure.png 自编码器的一般结构 %}

# 欠完备自编吗器

从自编码器获得有用特征的一种方法是**限制$h$的维度比$x$小**，这种编码维度小于输入维度的自编码器称为欠完备自编码器。学习欠完备自编码器的表示将强制自编码器捕捉训练数据中最显著的特征。其最小化损失函数为

$$L(x,g(f(x)))=\frac{1}{N} \sum_{i=1}^{N}(x-g(f(x)))^{2}$$

即输入数据与经自编码器重构后的数据之间的误差。**损失函数一般分为经验风险损失函数与结构风险损失函数。** 经验风险损失函数指预测结果和实际结果之间的差别，如上式就是经验风险损失函数。结构风险损失函数是在经验风险损失函数上添加一个正则项，如L0、L1、L2和稀疏正则项等。常用的损失函数有六种，分别是0-1损失、绝对值损失函数函数、平方损失函数、对数损失函数、指数损失函数、铰链损失函数。

当解码器是线性的且$L$是均方误差（如上式），**欠完备的自编码器会学习出与PCA相同的生成子空间**，即训练数的主元子空间。不幸的是，**如果编码器和解码器被赋予过大的容量，自编码器会执行复制任务而捕捉不到任何有关数据分布的有用信息。**

# 正则自编码器

通过正则自编码器，即使模型容量大到足以学习一个无意义的恒等函数，非线性且过完备的正则自编码器仍然能够从数据中学到一些关于数据分布有用的信息。

## 稀疏自编码器

稀疏自编码器在训练时结合编码层的稀疏惩罚项$\Omega (h)$和重构误差

$$L(x, g(f(x)))+\Omega (h)$$

其中$g(h)$是解码器的输出$r$，通常$h$是编码器的输出特征，即$h=f(x)$，惩罚项为L1范数$\Omega(h)=\lambda \sum_{i}\left|h_{i}\right|$。

## 去噪自编码器

去噪自编码器是一类接受损坏数据作为输入，并训练来预测原始未被损坏数据作为输出的自编码器，其本质是最小化

$$L(x, g(f(\tilde{x})))$$

其中$\tilde{x}$是被某种噪声破坏的$x$的副本，即可以加入高斯噪声等。去噪自编码器是强制$f$和$g$隐式的学习$p_{data}(x)$的结构，然后通过最小化重构误差来获得有用的特征，基本步骤分三步：

1、从训练数据中采一个训练样本$x$；

2、从损坏过程$C(\tilde{x}|x=x)$采一个损坏样本$\tilde{x}$；

3、将$(x,\tilde{x})$作为训练样本来估计自编码器的重构分布$p_{reconstruct}(x|\tilde{x})=p_{decoder}(x|h)$，其中$h$是编码器$f(\tilde{x})$的输出，$p_{decoder}$根据解码器函数$g(h)$定义。

通常采用梯度法（如小批量梯度下降）来近似最小化。

{% asset_img dae-structure.png 去噪自编码器的一般结构 %}


