---
title: 决策树
mathjax: true
date: 2019-11-27 23:46:23
updated: {{ date }}
tags: [机器学习, 决策树]
categories: [基础]
---

{% asset_img dt.png 决策树 %}

# 原理

决策树的构造通常分为两个阶段，分别是`构造和剪枝`。

1、构造：`生成一颗完整的树`，即在构造的过程总选择什么属性作为节点，树中的节点有三个，分别是：

（1）根节点：树的顶端，即最开始的节点，如上图中的“天气”。

（2）内部节点：树内部的分叉节点，如上图中的“温度”、“湿度”等

（3）叶节点：树最底部的节点，即决策树最终的结果，并且叶节点没有子节点。

在进行树构造时，需要解决三个主要问题：

（1）选择哪个属性作为根节点？

（2）选择哪个属性作为子节点？

（3）停止并获得目标状态的条件？即产生一个叶节点

2、剪枝：`为防止过拟合`，就需要给决策树瘦身（并不需要精确的判断所有属性）。

{% asset_img overfitting.png 过拟合 %}

上图中的1表示欠拟合，2表示具有较好泛化性，3表示过拟合。

**泛化能力：**指分类器是通过训练集获得的分类能力。如果对训练集分类的过于精准，即过度依赖于训练数据，那么得到的决策树其容错率就会比较低，泛化能力差。因为训练集只是全部数据的抽样，并不能体现全部数据的特点。

**预剪枝：**在决策树构造时就进行剪枝。即在构造的过程中对节点进行评估，如果对某个节点进行划分，在验证集中不能带来准确性的提升，那么对这个节点进行划分就没有意义，这时就会把当前节点作为`叶节点`，不对其进行划分。

**后剪枝：**在生成决策树之后再进行剪枝。通常会从决策树的叶节点开始，逐层向上对每个节点进行评估。如果剪掉这个节点子树，与保留该节点子树在分类准确性上差别不大，或者剪掉该节点子树，能在验证集中带来准确性的提升，那么就可以把该节点子树进行剪枝。`方法是：用该节点子树中使用最频繁的哪个节点来代替该节点。`

## 示例

{% asset_img example1.png 示例 %}

该示例是用于构造是否去打篮球的决策树，其中的属性有天气、温度、湿度、刮风。

# 纯度与信息熵

## 纯度

决策树的构造过程可以理解为寻找纯净划分的过程，纯度则是让目标变量的分歧最小。

例如下面的三个集合：

>* 集合 1：6 次都去打篮球；
>* 集合 2：4 次去打篮球，2 次不去打篮球；
>* 集合 3：3 次去打篮球，3 次不去打篮球。

按照纯度来分，集合 1 > 集合 2 > 集合 3。因为集合 1的分歧最小，集合 3的分歧最大。

## 信息熵

**信息熵：**表示信息的不确定度。由于随机离散事件的出现概率存在着不确定性，因此为了衡量这种信息的不确定性，就是用信息熵来表示。

\begin{equation}
    Entropy(t)=-\sum_{i=0}^{c-1}p(i|t)\log_{2}p(i | t)
\end{equation}

其中$p(i|t)$表示$t$为分类$i$的概率，$t$表示事件发生的预测次数，$i$表示事件发生的实际次数。**当不确定性越大时，它所包含的信息量也就越大，信息熵也就越高。**以示例中的集合为例：

>* 集合1：$Entropy(t)=-\frac{1}{6}\log_{2}(\frac{6}{6})=0$
>* 集合2：$Entropy(t)=-\frac{4}{6}\log_{2}(\frac{4}{6})-\frac{2}{6}\log_{2}(\frac{2}{6})=0.9$
>* 集合3：$Entropy(t)=-\frac{3}{6}\log_{2}(\frac{3}{6})-\frac{3}{6}\log_{2}(\frac{3}{6})=1$

从结果可以看出，**信息熵越大，纯度越低。当集合中的所有样本均匀混合时，信息熵最大，纯度最低。**

因此，在构建决策树时，会基于纯度来进行构建，典型的“不纯度”指标有三种，分别是**信息增益（ID3 算法）、信息增益率（C4.5 算法）以及基尼指数（Cart 算法）。**

# 信息增益（ID3 算法）

## 原理

父亲节点的信息熵减去所有子节点的信息熵。在计算的过程中会计算每个子节点的归一化信息熵，**即按照每个子节点在父节点中出现的概率**来计算这些子节点的信息熵。

\begin{equation}
    Gain(D, a)=Entropy(D)-\sum_{i=1}^{k} \frac{\left|D_{i}\right|}{|D|} Entropy\left(D_{i}\right)
\end{equation}

其中$D$表示父节点，$D_{i}$是子节点，$a$是在父节点$D$中选择的属性，$\frac{\left|D_{i}\right|}{|D|}$表示以父节点的分叉属性$a$为例，子节点在父节点出现的概率。

{% asset_img infogain.png 信息增益 %}

\begin{equation}
    Gain(D, a)=Entropy(D)-\left(\frac{3}{10} Entropy\left(D_{1}\right)+\frac{7}{10} Entropy\left(D_{2}\right)\right)
\end{equation}

## 缺陷

ID3的算法规则相对简单，可解释性强。同样也存在缺陷，比如在ID3算法中倾向于选择出现概率比较多的属性。因此，如果把“编号”作为一个属性的话，那么“编号”将会被选为最优属性 。但实际上“编号”是无关属性的，它对结果的分类并没有太大作用。

所以，ID3的缺陷为当有些属性可能对分类任务没有太大作用，但是他们仍然可能会被选为最优属性，即对噪声敏感，并且当训练数据如果有少量错误，可能会产生决策树分类错误。

# 信息增益率（C4.5算法）

## 原理

因为 ID3 在计算的时候，倾向于选择取值多的属性。为了避免这个问题，C4.5 采用信息增益率的方式来选择属性。

\begin{equation}
    \text { 信息增益率 }=\frac{\text { 信息增益 }}{\text { 属性熵 }}
\end{equation}

当属性有很多值的时候，相当于被划分成了许多份，**虽然信息增益变大了，但是对于C4.5来说，属性熵也会变大，**所以整体的信息增益率并不大。

## 剪枝

ID3构造决策树时，容易产生过拟合。在C4.5中，会在决策树构造之后采用悲观剪枝（PEP），这样可以提升决策树的泛化能力。

悲观剪枝是后剪枝技术中的一种，通过递归估算每个内部节点的分类错误率，比较剪枝前后这个节点的分类错误率来决定是否对其进行剪枝。**这种剪枝方法的优势是不再需要一个单独的测试数据集。**

## 离散化处理连续值

对于处理连续值时，C4.5选择具有最高信息增益节点的属性值作为阈值。

## 缺陷

C4.5在ID3的基础上，用信息增益率代替了信息增益，解决了噪声敏感的问题，并且可以对所构造树进行剪枝，同时还能处理连续数值以及数值缺失等情况。

**但由于C4.5需要对数据集进行多次扫描，算法效率相对较低。**

# 基尼指数（Cart 算法）

# 参考资料

[1] [决策树](https://www.cnblogs.com/molieren/articles/10664954.html)
