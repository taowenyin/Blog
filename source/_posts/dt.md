---
title: 决策树
mathjax: true
date: 2019-11-27 23:46:23
updated: {{ date }}
tags: [机器学习, 决策树]
categories: [基础]
---

{% asset_img dt.png 决策树 %}

# 原理

决策树是一种**非参数监督**学习方法，用于分类和回归任务。对于离散值的构建的树模型，一般为分类树，而用连续值构建的树模型，一般为回归树。

>* 决策树的本质：从训练集中归纳出一组分类规则，或者说是由训练数据集估计条件概率模型。
>* 决策树学习的损失函数：正则化的极大似然函数。
>* 决策树学习的测试：最小化损失函数。

决策树的构造通常分为两个阶段，分别是`构造和剪枝`。

1、构造：`生成一颗完整的树`，即在构造的过程总选择什么属性作为节点，树中的节点有三个，分别是：

（1）根节点：树的顶端，即最开始的节点，如上图中的“天气”。

（2）内部节点：树内部的分叉节点，如上图中的“温度”、“湿度”等

（3）叶节点：树最底部的节点，即决策树最终的结果，并且叶节点没有子节点。

在进行树构造时，需要解决三个主要问题：

（1）选择哪个属性作为根节点？

（2）选择哪个属性作为子节点？

（3）停止并获得目标状态的条件？即产生一个叶节点

2、剪枝：`为防止过拟合`，就需要给决策树瘦身（并不需要精确的判断所有属性）。

{% asset_img overfitting.png 过拟合 %}

上图中的1表示欠拟合，2表示具有较好泛化性，3表示过拟合。

**泛化能力：**指分类器是通过训练集获得的分类能力。如果对训练集分类的过于精准，即过度依赖于训练数据，那么得到的决策树其容错率就会比较低，泛化能力差。因为训练集只是全部数据的抽样，并不能体现全部数据的特点。

**预剪枝：**在决策树构造时就进行剪枝。即在构造的过程中对节点进行评估，如果对某个节点进行划分，在验证集中不能带来准确性的提升，那么对这个节点进行划分就没有意义，这时就会把当前节点作为`叶节点`，不对其进行划分。

**后剪枝：**在生成决策树之后再进行剪枝。通常会从决策树的叶节点开始，逐层向上对每个节点进行评估。如果剪掉这个节点子树，与保留该节点子树在分类准确性上差别不大，或者剪掉该节点子树，能在验证集中带来准确性的提升，那么就可以把该节点子树进行剪枝。`方法是：用该节点子树中使用最频繁的哪个节点来代替该节点。`

## 特点

>* 优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。
>* 缺点：可能会产生过度匹配的问题。
>* 适用数据类型：数值型和标称型（结果只在有限目标集中取值，如列表值）。

创建分支的伪代码createBranch()如下所示：

```python
If so return 类标签：
Else
    寻找划分数据集的最好特征
    划分数据集
    创建分支节点
        for 每个划分的子集
            调用函数createBranch()并增加返回结果到分支节点中
        return 分支节点

```

## 示例

{% asset_img example1.png 示例 %}

该示例是用于构造是否去打篮球的决策树，其中的属性有天气、温度、湿度、刮风。

# 纯度与信息熵

## 纯度

决策树的构造过程可以理解为寻找纯净划分的过程，纯度则是让目标变量的分歧最小。

例如下面的三个集合：

>* 集合 1：6 次都去打篮球；
>* 集合 2：4 次去打篮球，2 次不去打篮球；
>* 集合 3：3 次去打篮球，3 次不去打篮球。

按照纯度来分，集合 1 > 集合 2 > 集合 3。因为集合 1的分歧最小，集合 3的分歧最大。

## 信息熵

**信息熵：**表示信息的不确定度。由于随机离散事件的出现概率存在着不确定性，因此为了衡量这种信息的不确定性，就是用信息熵来表示。同时，**信息熵也用于在构建树的每个步骤决定要拆分的特征**。

1、经验熵：熵中的概率由数据估计(特别是最大似然估计)得到。

\begin{equation}
    Entropy(D)=-\sum_{i=1}^{n}\frac{\left | c_{k} \right |}{\left | D \right |}\log_{2}\frac{\left | c_{k} \right |}{\left | D \right |}
\end{equation}

其中$\frac{\left | c_{k} \right |}{\left | D \right |}$为$\left | c_{k} \right |$样本数量和总样本数量$\left | D \right |$的比。

2、条件熵：已知在随机事件$X$的条件下随机事件$Y$的不确定性。

\begin{equation}
    Entropy(Y | X)=-\sum_{i=1}^{n}p_{i}H\left ( Y | X = x_{i} \right )
\end{equation}

其中$p_{i} = P\left (X = x_{i} \right )$表示条件$x_{i}$出现的概率，$H\left ( Y | X = x_{i} \right )$表示事件$x_{i}$发生时，发生事件$Y$的概率。

**当不确定性越大时，它所包含的信息量也就越大，信息熵也就越高。**以示例中的集合为例：

>* 集合1：$Entropy(t)=-\frac{1}{6}\log_{2}(\frac{6}{6})=0$
>* 集合2：$Entropy(t)=-\frac{4}{6}\log_{2}(\frac{4}{6})-\frac{2}{6}\log_{2}(\frac{2}{6})=0.9$
>* 集合3：$Entropy(t)=-\frac{3}{6}\log_{2}(\frac{3}{6})-\frac{3}{6}\log_{2}(\frac{3}{6})=1$

从结果可以看出，**信息熵越大，纯度越低。当集合中的所有样本均匀混合时，信息熵最大，纯度最低。**

因此，在构建决策树时，会基于纯度来进行构建，典型的“不纯度”指标有三种，分别是**信息增益（ID3 算法）、信息增益率（C4.5 算法）以及基尼指数（Cart 算法）。**

# 信息增益（ID3 算法）

## 原理

父亲节点的信息熵减去所有子节点的信息熵。在计算的过程中会计算每个子节点的归一化信息熵，**即按照每个子节点在父节点中出现的概率**来计算这些子节点的信息熵。

\begin{equation}
    Gain(D, A)=H(D)-H(D|A)=Entropy(D)-\sum_{i=1}^{k} \frac{\left|D_{i}\right|}{|D|} Entropy\left(D_{i}\right)
\end{equation}

其中$D$表示父节点，$D_{i}$是子节点，$A$是在父节点$D$中选择的属性，$\frac{\left|D_{i}\right|}{|D|}$表示以父节点的分叉属性$A$为例，子节点在父节点出现的概率。

{% asset_img infogain.png 信息增益 %}

\begin{equation}
    Gain(D, A)=Entropy(D)-\left(\frac{3}{10} Entropy\left(D_{1}\right)+\frac{7}{10} Entropy\left(D_{2}\right)\right)
\end{equation}

## 实现步骤

>* Step1：根据要分类目标计算其信息熵。
>* Step2：根据分类目标，为每个属性计算信归一化信息熵。
>* Step3：根据分类目标和式子（2）计算每个属性的信息增益，并把**信息增益最大的作为根节点**。
>* Step4：根据根节点的属性值进行分类，计算每一类中信息增益最大的属性，并把该属性作为该节点的分割属性，如下图。
>* Step5：循环Step4从而得到整棵树。

{% asset_img dt-split.png 节点分裂 %}

## 缺陷

ID3的算法规则相对简单，可解释性强。同样也存在缺陷，比如在ID3算法中倾向于选择出现概率比较多的属性。因此，如果把“编号”作为一个属性的话，那么“编号”将会被选为最优属性 。但实际上“编号”是无关属性的，它对结果的分类并没有太大作用。

所以，ID3的缺陷为**当有些属性可能对分类任务没有太大作用，但是他们仍然可能会被选为最优属性**，即对噪声敏感，并且当训练数据如果有少量错误，可能会产生决策树分类错误。

# 信息增益率（C4.5算法）

## 原理

因为 ID3 在计算的时候，倾向于选择取值多的属性。为了避免这个问题，C4.5 采用信息增益率的方式来选择属性。

\begin{equation}
    \text { 信息增益率 }=\frac{\text { 信息增益 }}{\text { 属性熵 }}
\end{equation}

当属性有很多值的时候，相当于被划分成了许多份，**虽然信息增益变大了，但是对于C4.5来说，属性熵也会变大，**所以整体的信息增益率并不大。

## 剪枝

ID3构造决策树时，容易产生过拟合。而在C4.5中，会在决策树构造之后采用悲观剪枝（PEP），这样可以提升决策树的泛化能力。

悲观剪枝是后剪枝技术中的一种，通过递归估算每个内部节点的分类错误率，比较剪枝前后这个节点的分类错误率来决定是否对其进行剪枝。**这种剪枝方法的优势是不再需要一个单独的测试数据集。**

## 离散化处理连续值

对于处理连续值时，C4.5选择具有最高信息增益属性的属性值作为阈值。

## 处理缺省数据

当某个属性由于数据缺失，使得只有$B$条样本，而总样本数为$A$，那么最后算出的信息增益率为：

\begin{equation}
    \text { 某属性的信息增益率 }=\frac{B}{A}*\text { 某属性的实际信息增益率 }
\end{equation}

## 缺陷

C4.5在ID3的基础上，用信息增益率代替了信息增益，解决了噪声敏感的问题，并且可以对所构造树进行剪枝，同时还能处理连续数值以及数值缺失等情况。**但由于C4.5需要对数据集进行多次扫描，算法效率相对较低。**

# 基尼指数（CART算法）

CART的全称是分类与回归树，即该算法既可以用于分类问题，也可以用于回归问题，并且使用CART生成的树只能是二叉树。

1、回归树：使用平方误差最小化准则来选择特征并进行划分。每一个叶节点给出的预测值是划分到该叶子节点的所有样本目标值的均值，这样只是在给定划分的情况下最小化了平方误差。

2、分类树：使用基尼指数（GINI）最小化准则来选择特征并进行划分。基尼指数表示集合的不确定性，或者是不纯度。基尼指数越大，集合不确定性越高，不纯度也越大。

## 优点

>* 基尼指数的计算不需要对数运算，更加高效。
>* 基尼指数更偏向于连续属性，熵更偏向于离散属性。

# 参考资料

[1] [决策树](https://www.cnblogs.com/molieren/articles/10664954.html)

[2] [机器学习实战（三）——决策树](https://blog.csdn.net/jiaoyangwm/article/details/79525237#3__680)