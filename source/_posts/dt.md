---
title: 决策树
mathjax: true
date: 2019-11-27 23:46:23
updated: {{ date }}
tags: [机器学习, 决策树]
categories: [基础]
---

{% asset_img dt.png 决策树 %}

# 原理

决策树是一种**非参数监督**学习方法，用于分类和回归任务。对于离散值的构建的树模型，一般为分类树，而用连续值构建的树模型，一般为回归树。

>* 决策树的本质：从训练集中归纳出一组分类规则，或者说是由训练数据集估计条件概率模型。
>* 决策树学习的损失函数：正则化的极大似然函数。
>* 决策树学习的测试：最小化损失函数。

决策树的构造通常分为两个阶段，分别是`构造和剪枝`。

1、构造：`生成一颗完整的树`，即在构造的过程总选择什么属性作为节点，树中的节点有三个，分别是：

（1）根节点：树的顶端，即最开始的节点，如上图中的“天气”。

（2）内部节点：树内部的分叉节点，如上图中的“温度”、“湿度”等

（3）叶节点：树最底部的节点，即决策树最终的结果，并且叶节点没有子节点。

在进行树构造时，需要解决三个主要问题：

（1）选择哪个属性作为根节点？

（2）选择哪个属性作为子节点？

（3）停止并获得目标状态的条件？即产生一个叶节点

2、剪枝：`为防止过拟合`，就需要给决策树瘦身（并不需要精确的判断所有属性）。

{% asset_img overfitting.png 过拟合 %}

上图中的1表示欠拟合，2表示具有较好泛化性，3表示过拟合。

**泛化能力：**指分类器是通过训练集获得的分类能力。如果对训练集分类的过于精准，即过度依赖于训练数据，那么得到的决策树其容错率就会比较低，泛化能力差。因为训练集只是全部数据的抽样，并不能体现全部数据的特点。

**预剪枝：**在决策树构造时就进行剪枝。即在构造的过程中对节点进行评估，如果对某个节点进行划分，在验证集中不能带来准确性的提升，那么对这个节点进行划分就没有意义，这时就会把当前节点作为`叶节点`，不对其进行划分。预剪枝降低了过拟合的风险，显著减少了决策树的训练时间开销和测试时间开销，但可能带来欠拟合的风险 。

**后剪枝：**在生成决策树之后再进行剪枝。通常会从决策树的叶节点开始，逐层向上对每个节点进行评估。如果剪掉这个节点子树，与保留该节点子树在分类准确性上差别不大，或者剪掉该节点子树，能在验证集中带来准确性的提升，那么就可以把该节点子树进行剪枝。`方法是：用该节点子树中使用最频繁的哪个节点来代替该节点。`后剪枝的欠拟合风险很小，泛化性能往往优于预剪枝决策树，但其训练时间开销比未剪枝和预剪枝都要大得多。

## 特点

>* 优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。
>* 缺点：可能会产生过度匹配的问题。
>* 适用数据类型：数值型和标称型（结果只在有限目标集中取值，如列表值）。

创建分支的伪代码createBranch()如下所示：

```python
If so return 类标签：
Else
    寻找划分数据集的最好特征
    划分数据集
    创建分支节点
        for 每个划分的子集
            调用函数createBranch()并增加返回结果到分支节点中
        return 分支节点

```

## 示例

{% asset_img example1.png 示例 %}

该示例是用于构造是否去打篮球的决策树，其中的属性有天气、温度、湿度、刮风。

# 纯度与信息熵

## 纯度

决策树的构造过程可以理解为寻找纯净划分的过程，纯度则是让目标变量的分歧最小。

例如下面的三个集合：

>* 集合 1：6 次都去打篮球；
>* 集合 2：4 次去打篮球，2 次不去打篮球；
>* 集合 3：3 次去打篮球，3 次不去打篮球。

按照纯度来分，集合 1 > 集合 2 > 集合 3。因为集合 1的分歧最小，集合 3的分歧最大。

## 信息熵

**信息熵：**表示信息的不确定度。由于随机离散事件的出现概率存在着不确定性，因此为了衡量这种信息的不确定性，就是用信息熵来表示。同时，**信息熵也用于在构建树的每个步骤决定要拆分的特征**。

1、经验熵：熵中的概率由数据估计(特别是最大似然估计)得到。

\begin{equation}
    Entropy(D)=-\sum_{i=1}^{n}\frac{\left | c_{k} \right |}{\left | D \right |}\log_{2}\frac{\left | c_{k} \right |}{\left | D \right |}
\end{equation}

其中$\frac{\left | c_{k} \right |}{\left | D \right |}$为$\left | c_{k} \right |$样本数量和总样本数量$\left | D \right |$的比，当$\log$底为2时，成为比特熵，以$e$为底时称为纳特熵。

2、条件熵：已知在随机事件$X$的条件下随机事件$Y$的不确定性。

\begin{equation}
    Entropy(Y | X)=-\sum_{i=1}^{n}p_{i}H\left ( Y | X = x_{i} \right )
\end{equation}

其中$p_{i} = P\left (X = x_{i} \right )$表示条件$x_{i}$出现的概率，$H\left ( Y | X = x_{i} \right )$表示事件$x_{i}$发生时，发生事件$Y$的概率。

**当不确定性越大时，它所包含的信息量也就越大，信息熵也就越高。**以示例中的集合为例：

>* 集合1：$Entropy(t)=-\frac{1}{6}\log_{2}(\frac{6}{6})=0$
>* 集合2：$Entropy(t)=-\frac{4}{6}\log_{2}(\frac{4}{6})-\frac{2}{6}\log_{2}(\frac{2}{6})=0.9$
>* 集合3：$Entropy(t)=-\frac{3}{6}\log_{2}(\frac{3}{6})-\frac{3}{6}\log_{2}(\frac{3}{6})=1$

从结果可以看出，**信息熵越大，纯度越低。当集合中的所有样本均匀混合时，信息熵最大，纯度最低。**

因此，在构建决策树时，会基于纯度来进行构建，典型的“不纯度”指标有三种，分别是**信息增益（ID3 算法）、信息增益率（C4.5 算法）以及基尼指数（Cart 算法）。**

# 信息增益（ID3 算法）

## 原理

父亲节点的信息熵减去所有子节点的信息熵。在计算的过程中会计算每个子节点的归一化信息熵，**即按照每个子节点在父节点中出现的概率**来计算这些子节点的信息熵。

\begin{equation}
    Gain(D, A)=H(D)-H(D|A)=Entropy(D)-\sum_{i=1}^{k} \frac{\left|D_{i}\right|}{|D|} Entropy\left(D_{i}\right)
\end{equation}

其中$D$表示父节点，$D_{i}$是子节点，$A$是在父节点$D$中选择的属性，$\frac{\left|D_{i}\right|}{|D|}$表示以父节点的分叉属性$A$为例，子节点在父节点出现的概率。

{% asset_img infogain.png 信息增益 %}

\begin{equation}
    Gain(D, A)=Entropy(D)-\left(\frac{3}{10} Entropy\left(D_{1}\right)+\frac{7}{10} Entropy\left(D_{2}\right)\right)
\end{equation}

## 实现步骤

>* Step1：根据要分类目标计算其信息熵。
>* Step2：根据分类目标，为每个属性计算信归一化信息熵。
>* Step3：根据分类目标和式子（2）计算每个属性的信息增益，并把**信息增益最大的作为根节点**。
>* Step4：根据根节点的属性值进行分类，计算每一类中信息增益最大的属性，并把该属性作为该节点的分割属性，如下图。
>* Step5：循环Step4从而得到整棵树。

{% asset_img dt-split.png 节点分裂 %}

## 缺陷

1、ID3没有剪枝策略，容易过拟合。

2、ID3算法中倾向于选择出现概率比较多的属性。例如，“编号”属性容易将会被选为最优属性。

3、只能用于处理离散分布的特征。

4、没有考虑缺失值。

所以，ID3的缺陷为**当有些属性可能对分类任务没有太大作用，但是他们仍然可能会被选为最优属性**，即对噪声敏感，并且当训练数据如果有少量错误，可能会产生决策树分类错误。

# 信息增益率（C4.5算法）

## 改进

1、引入悲观剪枝策略进行后剪枝。

2、引入信息增益率作为划分标准。

3、离散化处理连续值。

4、可以处理缺省数据。

## 原理

因为 ID3 在计算的时候，倾向于选择取值多的属性。为了避免这个问题，C4.5 采用信息增益率的方式来选择属性。

\begin{equation}
    \text { 信息增益率 }=\frac{\text { 信息增益 }}{\text { 父节点熵 }}=\frac{H(D)-H(D|A)}{\text { H(D) }}
\end{equation}

当属性有很多值的时候，相当于被划分成了许多份，**虽然信息增益变大了，但是对于C4.5来说，属性熵也会变大，**所以整体的信息增益率并不大。

## 剪枝

ID3构造决策树时，容易产生过拟合。而在C4.5中，会在决策树构造之后采用悲观剪枝（PEP），这样可以提升决策树的泛化能力。

悲观剪枝是后剪枝技术中的一种，通过递归估算每个内部节点的损失函数，比较剪枝前后这个节点的损失函数来决定是否对其进行剪枝。**这种剪枝方法的优势是不再需要一个单独的测试数据集。**

决策树的损失函数为：

\begin{equation}
    C_{\alpha}(T)=\sum_{t=1}^{|T|} N_{t} H_{t}(T)+\alpha|T|=C(T)+\alpha|T|
\end{equation}

其中$T$表示子树的叶节点；$H_{t}(T)$表示第$t$个叶子的熵；$N_{t}$表示该叶子所含的训练样例的个数；$\alpha$表示惩罚系数，并控制模型和训练数据之间拟合程度，当$\alpha$较大时，树模型较为简单，反之则树模型较为复杂，为0时只考虑模型与训练数据的拟合程度，不考虑模型的复杂度；C(T)表示模型对训练数据的预测误差，即模型与训练数据的拟合程度。

### 实现步骤

>* Step1：计算每个结点的经验熵。
>* Step2：递归的从树的叶结点向上回缩。

设一组叶结点回缩到其父结点之前与之后的整体树分别为$T_{B}$和$T_{A}$，其对应的损失函数值分别是$C_{\alpha}(T_{B})$和$C_{\alpha}(T_{A})$，如果$C_{\alpha}(T_{A}) \leq C_{\alpha}(T_{B})$，则进行剪枝，即将父结点变为新的叶结点。

>* Step3：返回Step3，直到不能继续为止，从而得到损失函数最小的子树$T$。

## 离散化处理连续值

对于处理连续值时，C4.5选择具有最高信息增益属性的属性值作为阈值。

## 处理缺省数据

当某个属性由于数据缺失，使得只有$B$条样本，而总样本数为$A$，那么最后算出的信息增益率为：

\begin{equation}
    \text { 某属性的信息增益率 }=\frac{B}{A}*\text { 某属性的实际信息增益率 }
\end{equation}

## 缺陷

1、C4.5只能用于分类。

2、熵模型中的对数运算、连续值、排序运算都消耗大量运算。

C4.5在ID3的基础上，用信息增益率代替了信息增益，解决了噪声敏感的问题，并且可以对所构造树进行剪枝，同时还能处理连续数值以及数值缺失等情况。**但由于C4.5需要对数据集进行多次扫描，算法效率相对较低。**

# 基尼指数（CART算法）

## 原理

CART的全称是分类与回归树，即该算法既可以用于分类问题，也可以用于回归问题，并且使用CART生成的树只能是二叉树。

1、回归树：使用平方误差最小化准则来选择特征并进行划分。每一个叶节点给出的预测值是划分到该叶子节点的所有样本目标值的均值，这样只是在给定划分的情况下最小化了平方误差。

2、分类树：使用基尼指数（GINI）最小化准则来选择特征并进行划分。基尼指数表示集合的不确定性，或者是不纯度。基尼指数越大，集合不确定性越高，不纯度也越大。

## 算法组成

**1、决策树生成：**基于训练数据集生成决策树，生成的决策树要尽量大。

**2、决策树剪枝：**用于验证数据集对已生成的数进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准。

\begin{equation}
    Gini(D)=1-\sum_{k=1}^{K}\left ( \frac{\left | C_{k} \right |}{\left | D \right |} \right )^2
\end{equation}

其中$\left | C_{k} \right |$表示$D$中属于第$k$类样本的个数，$\left | D \right |$数据集的个数。

如果样本集合$D$中根据特征$A$的某个取值$a$来把$D$分割为$D_{1}$和$D_{2}$，那么在特征A的条件下，集合D的基尼指数为：

\begin{equation}
    Gini(D, A)=\frac{\left|D_{1}\right|}{D} Gini\left(D_{1}\right)+\frac{\left|D_{2}\right|}{D} Gini\left(D_{2}\right)
\end{equation}

其中$Gini(D, A)$表示经$A=a$分割后集合$D$的不确定性。基尼指数值越大，样本集合的不确定性就越大。**在树生成时选择基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点。**

## 剪枝

决策树的损失函数为：

\begin{equation}
    C_{\alpha}(T)=C(T)+\alpha|T|
\end{equation}

其中$T$为任意子树；$C(T)$为预测误差，用来衡量模型与训练数据的拟合程度；$|T|$为子树$T$的叶子节点个数，即树的复杂度；$\alpha$为惩罚系数，用来控制模型和训练数据之间拟合程度，当$\alpha$较大时，树模型较为简单，反之则树模型较为复杂。并且当$\alpha$从0开始不断增加到$+\infty$时，会产生一系列的子树序列：$T_{0},T_{1},\cdots,T_{n}$，并且每一个$T_{i+1}$子树都是由前一个子树$T_{i}$剪掉某一个内部节点来生成的。

对于任意一个内部节点$t$，剪枝前有$|T_{t}|$个叶子节点，并且预测误差为$C(T_{t})$，剪枝后因为只有一个叶节点$t$，因此预测误差为$C(t)$，，因此损失函数的变化为：

剪枝前：

\begin{equation}
    C_{\alpha}(T)=C(T)+\alpha|T|
\end{equation}

剪枝后：

\begin{equation}
    C_{\alpha}(t)=C(t)+\alpha \times 1=C(t)+\alpha
\end{equation}

要是剪枝后的损失函数与剪枝前的损失函数相同$C(T)+\alpha|T|=C(t)+\alpha$，就可以得到：

\begin{equation}
    \alpha=\frac{C(t)-C\left(T_{t}\right)}{\left|T_{t}\right|-1}
\end{equation}

$\alpha$的意义在于在$\left [ \alpha_{i}, \alpha_{i+1} \right )$中惩罚系数的临界值，如果比该$\alpha$大，那么一定有$C_{\alpha}(T) > C_{\alpha}(t)$，即剪掉这个节点后都比不剪掉要更优。

## 回归树

对于连续值的处理，CART分类树采用基尼系数的大小来度量特征的划分点。在回归模型中，我们使用常见的和方差来进行度量，对于任意特征A进行划分，可以按照任意划分点$s$把数据分为两部分，分别是数据集$D_{1}$和数据集$D_{2}$，只要使$D_{1}$和$D_{2}$的均方差最小，并且$D_{1}$和$D_{2}$的均方差之和最小，那么$s$就是所对的特征划分点。表达式为：

\begin{equation}
    \min _{a, s}\left[\min _{c_{1}} \sum_{x_{i} \in D_{1}}\left(y_{i}-c_{1}\right)^{2}+\min _{c_{2}} \sum_{x_{i} \in D_{2}}\left(y_{i}-c_{2}\right)^{2}\right]
\end{equation}

其中，$c_{1}$为数据集$D_{1}$的样本输出均值，$c_{2}$为数据集$D_{2}$的样本输出均值。

对于分类树来说采用叶子节点里概率最大的类别作为当前对象的预测类别。而在回归树中，则是采用最终叶子的均值或者中位数作为预测结果输出。

## 优点

>* 基尼指数的计算不需要对数运算，更加高效。
>* 基尼指数更偏向于连续属性，熵更偏向于离散属性。

# 参考资料

[1] [决策树](https://www.cnblogs.com/molieren/articles/10664954.html)

[2] [机器学习实战（三）——决策树](https://blog.csdn.net/jiaoyangwm/article/details/79525237#3__680)

[3] [【机器学习】决策树](https://zhuanlan.zhihu.com/p/85731206)