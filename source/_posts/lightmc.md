---
title: 【2019】LightMC：A Dynamic And Efficient Multiclass Decomposition Algorithm
date: 2019-11-16 20:49:01
mathjax: true
updated: {{ date }}
tags:
- 集成学习
- 机器学习
categories: 
- 论文
---

# 0.摘要

解决多分类问题目前的做法有三种，分别是OVA（一对多）、OVO（一对一）和ECOC（纠错输出码），其中OVA和OVO非常简单，但是忽略了类与类之间的关系，而ECOC虽然考虑了类与类之间的关系，但是其性能取决与编码矩阵和解码策略，而要通过找到一个合适的解码策略来发现一个有效的编码矩阵是非常耗时和不确定的。因此，作者提出了一个高效的动态多类分解算法LightMC。该算法不同于以往的固定编码矩阵和解码策略，LightMC采用可微的解码策略，在每次迭代中通过反向传播来训练基础学习器，使其能够动态地优化编码矩阵和解码策略，从而提高多分类的总体精度。

# 1.介绍

多分类问题是把一个数据集分为三个或者更多的分类。在一个标准的多分类学习过程中，一共有$K > 2$个数据类（例如：$Y=\left \{ C_{1}, C_{2}, \cdots , C_{K} \right \}$），有$n$个训练集（例如：$S=\left \{ \left \{ x_{1}, y_{1} \right \}, \left \{ x_{2}, y_{2} \right \}, \cdots , \left \{ x_{n}, y_{n} \right \} \right \}$），每个训练实例都除以$K$个分类中的一个，并且还有分类器函数$f\left ( x \right )$，当有一个新实例$x$时，可以预测该实例属于哪个分类。

目前多分类最热的方法就是多类分解方法，即是将多分类问题变为一系列互不相关的二分类问题，然后把这些二分类问题进行重新组合，从而解决原始的多分类问题。

**本篇论文的主要贡献：**

1、作者提出的动态分解算法LightMC在输出精度和效率上都优于传统的ECOC。

2、作者定义了一个可微的解码策略，并通过已知的反向传播算法，得到了一种动态优化编码矩阵的有效算法。

3、通过对多个公共大数据集的大量实验分析，证明了新分解算法的有效性和高效性。

## 1.1OVA和OVO

1、OVA：训练K个不同的基学习器，当第$i$个学习在所有实例中学得的正例就属于$C_{i}$，否则就不属于$C_{i}$。

2、OVO：训练$\frac{K \times \left ( K - 1 \right )}{2}$个不同的基学习器，每一个学习器用于区分两个分类。

OVA和OVO虽然简单，并且也被广泛使用，但是缺陷也很明显，即忽略了类和类之间的关联，例如“Kitty”分类与“猫”的类分的关联性就远大于与“狗”分类的关联性。因此采用OVA和OVA的训练时就不能利用分类之间的关联性降低分类的成本，并提高分类的精度，因此存在低效，且计算复杂度高的问题。同时，当K很大时，且处理大型分类数据时，会导致极高的训练成本。

## 1.2ECOC

ECOC优于OVA和OVO，一定程度上解决了OVA和OVO中类之间完全独立的问题。ECOC依赖于一个编码矩阵，该编码矩阵为实例定义了一个转换标签，从而把多分类问题变为二分类问题，然后通过去相关和纠错的方式进行重新组合。在ECOC中，通过为不同的分类对生成不同的距离，使得分类之间的相关性能够应用到整个学习过程中。假如“猫”、“Kitty”、“狗”对应的编码矩阵为（1,1,1）、（1,1,-1）和（-1,-1,-1），那么学习模型就可以保证”猫“和”Kitty“之间的距离比”猫“和”Kitty“之间的距离更近。由于编码矩阵的长度和基学习器的数量可以比$K$小得多，因此基于ECOC的方法可以显著降低OVA和OVO的计算复杂度，特别是类别数量K非常大时。

基于ECOC方法的模型性能高度依赖编码矩阵和解码策略的设计，而随机生成的编码矩阵对结果具有高度的不确定性。为了解决这一问题，在优化编码矩阵方面做了很多工作。由于其多分类问题的复杂性，因此要找到一个优化矩阵是不可能的，甚至次优化矩阵也很难找到，因此无法得到优化矩阵阻碍了ECOC的应用。

## 1.3LightMC

不同于以往ECOC使用固定的编码矩阵和解码策略，LightMC能够动态的优化编码矩阵和解码策略，并且在每次迭代时对基学习器进行训练，从而达到更为精确的多类分类。为了达到这个目的，LightMC利用了一种可微的解码策略，使得可以通过梯度下降来进行优化，从而保证进一步降低训练损失。LightMC除了提高最终分类精度和获得更利于分类性能的编码矩阵和解码策略外，LightMC还可以显著提高效率，因为它节省了搜索次优编码矩阵的时间。LightMC在模型的训练过程中优化了编码矩阵，不需要再花费太多的时间来初始化编码矩阵，即使是随机编码矩阵也可以得到令人满意的结果。

# 2.相关工作

## 2.1ECOC

在ECOC中，每一个分类$k$都有一个编码字$M_{k}$，该编码字$M_{k}$表示在第$j$个基分类器中类$k$中数据的标签。所有编码字组合一个编码矩阵$M \in \left \{ 1,-1 \right \}^{K \times L}$，其中L是一个编码字的长度和所有基学习器的数量。所有基学习器的输出为$o=\left \{ o_{1}, o_{2}, \cdots , o_{L} \right \}$，最后通过解码策略就可以得到多分类的结果。

\begin{align}
    \hat{y} = argmin_{k}\left ( t \right ),where\ t_{k}=\frac{1}{2}\sum_{j=1}^{L}\left | M_{kj} - sgn\left ( o_{j} \right ) \right |
\end{align}

其中$\hat{y}$表示预测的分类，$sgn$是一个符号函数，当$o \geq 0$时$sgn\left ( 0 \right ) = 1$，否则就为$-1$。解码策略采用Hamming解码，通过选择Hamming距离最小的类来进行预测，此外，通过Hamming解码还能够纠正基学习期中一定数量的错误。

与传统的分解方法相比，ECOC方法具有许多优点：

1、通过编码矩阵来表示不同类对之间距离，使得能够将类之间的相关性集成到分类模型中，从而进一步提高分类精度。

2、由于代码长度$L$（即基学习器的数量）可能比类$K$的数量小得多，因此ECOC方法比OVA和OVO更有效，特别是当$K$非常大时。

但是ECOC方法的性能主要取决于编码矩阵的设计，并且寻找一个最优的编码矩阵的复杂度是NP-难问题。因此，要找到一个最优的编码矩阵是不可能的，甚至找到一个次优的编码矩阵也是很难，这个问题也制约了ECOC的使用。

## 2.2其他工作

近年来，许多人试图改进找到合适的编码举证来改进ECOC分解方法。例如，对分类空间进行分层划分，从而以生成相应的编码；通过遗传算法来产生具有较好性能的编码矩阵；通过使用频谱分解找到良好的编码矩阵或通过采用连续值编码矩阵，以及在松弛编码矩阵上的整数约束，可以对ECOC方法有显著改善。虽然这些研究对ECOC的改进具有一定的帮助，但是还是有两个主要的挑战：

1、效率：为了提高多分类的精度，前面的许多工作是设计一个具有$L$长度的编码矩阵（长度从$K-1$到$K^{2}$），这使得基学习器几乎与OVA和OVO所需的模型一样多。这使得ECOC方法在大规模分类问题中非常低效。

2、可扩展性：事实上，以前的ECOC方法研究主要在小规模分类数据下进行，通常只有几十个类和数千个样本组成。

# 3.LightMC

为了解决ECOC方法的两个主要问题，LightMC没有在训练前确定编码矩阵和解码策略，而是试图通过直接优化目标函数，并结合基学习器的训练，动态改善ECOC分解。更具体地说，LightMC引入了一种新的可微解码策略，使得LightMC能够在基学习器训练过程中直接通过梯度下降来优化编码矩阵和解码策略。LightMC有两方面的优势：

1、有效性：LightMC没有将编码矩阵和解码策略的设计与基学习器的训练分离，而是通过联合优化编码矩阵、解码策略和基学习器，从而进一步提高了ECOC分类的精度。

2、高效性：由于在后续的训练中会自动优化编码矩阵，因此LightMC可以显著降低在训练前找到一个好的编码矩阵所需的时间成本。

{% asset_img lightmc.png LightMC的算法 %}

图1是LightMC算法的流程：

1、首先通过现有的ECOC方法初始化编码矩阵。

2、为了充分利用基学习器的训练信息，LightMC采用交替优化算法，将基学习器的学习与编解码优化相结合，即训练基学习器时，编解码策略是固定的，反之则是动态的。

3、这样的联合学习将反复运行，直到整个训练结果收敛。

需要注意的是，不同于在训练前确定编码矩阵，LightMC开发了一个端到端的解决方案，以迭代的方式联合训练基学习器和分解模型。算法1就是LightMC算法的描述，在该算法中有两个步骤：

1、解码训练：用于优化解码策略。

2、编码矩阵训练：用于优化编码举证。

## 3.1新的可微解码策略：Softmax解码

为了寻找最优的编码与解码策略，就需要对全局目标函数进行直接优化。由于大多数现有的解码策略是不可微的，因此广泛使用的反向传播方法无法用来直接优化全局目标函数。为了解决这个问题，设计一种在保持纠错特性的同时具有可微性的解码策略是至关重要的。

通过研究式1中的解码策略，可以知道有两个不可微的函数，分别是：$sgn$和$argmin$。根据文献可以知道，当生成的距离结果为`Manhattan（L1）距离`时，可以直接删除$sgn$函数，并且仍然保持纠错特性。此外，$argmin$函数可以由被广泛使用的$softmax$函数代替，该函数可以近似于$argmin$函数，并且具有连续的输出概率和可微的特性，步骤如下：

1、把$argmin$替换成$argmax$，同时把$M_{kj}$的符号颠倒。通过这个方法，当第$j$个分类器$o_{j}$输出为$M_{kj}$时，距离将是最大值而不是最小值。

2、把$argmax$替换为$softmax$，则整个解码策略变为：

\begin{align}
    \hat{y} = softmax\left ( t \right ),where\ t_{k}=\frac{1}{2}\sum_{j=1}^{L}\left | -M_{kj} - o_{j} \right |
\end{align}

其中$t_{k}$表示分类器输出和分类$k$的编码之间的相似性。尽管在本算法中使用了L1损失，但在文献中提到的L2损失或其他距离函数也同样适用，并且会产生类似的结果。通过上述转换，解码策略将把最高得分分配给最接近输出向量的类，即输出向量具有纠错属性。基于这个原因就可以采用被广泛使用的梯度下降算法来直接优化解码策略。此外，新的解码函数可以重写为单层softmax回归的形式，因为等式2中的距离函数满足：

\begin{align}
    \left | -M_{kj} - o_{j} \right | = \left\{
    \begin{matrix}
        1-o_{j}, & M_{kj}=-1 \\ 
        1+o_{j}, & M_{kj}=1
    \end{matrix}\right.
    =1+M_{kj}o_{j}
\end{align}

因此允许将解码策略重写为：

\begin{align}
    \begin{matrix}
        t_{k}=\frac{1}{2}\sum_{j=1}^{L}\left ( 1+M_{kj}o_{j} \right )=\frac{1}{2}\left ( M_{k}^{T}o+L \right )\\
        let\ \theta_{k}=M_{k},b_{k}=L \\ 
        \hat{y}=softmax(t),\ t_{k} = \frac{1}{2}\left ( \theta_{k}^{T}o + b_{k} \right )
    \end{matrix}
\end{align}

其形式与具有softmax激活的单层线性模型完全相同。因此，就可以使用梯度下降来训练由$M$初始化的softmax参数$\Theta$，以减低整体误差。考虑到导数计算的方便性，我们选择了与softmax函数一起使用的多类交叉熵作为损失函数。单个数据点上的总损失可以表示为：

\begin{align}
    \begin{matrix}
        J = -\sum_{k=1}^{K}\left ( 1-y_{k} \right )\log\left ( 1-\hat{y_{k}} \right )+y_{k}\log\left (\hat{y_{k}} \right )\\ 
        where\ \Theta\ is\ updated\ by\ \theta_{k}^{t}=\theta_{k}^{t-1}-\gamma_{1}\frac{\partial J}{\partial\theta_{k}^{t-1}}
    \end{matrix}
\end{align}

其中$\gamma_{1}$为学习率，$y$是从原始标签转化而来的one-hot向量。这个优化的过程被称为解码训练（TrainDecoding），如算法2。像普通的梯度下降一样，数据被划分成小批量，用于计算当前梯度，以便进行一次梯度更新。这里也可以使用L1/L2正则化来提高泛化能力。由于梯度下降能够有效性保证了迭代过程中整体损失的减少，从而保证了解码训练（TrainDecoding）是一种有效的改进译码策略的方法。

## 3.2编码矩阵优化

如果softmax解码的输入$o$可以通过反向传播来更新，那么就能够进一步降低总体训练损失。那么对应的更新过程就可以定义为$o^{t}=o^{t-1}-\gamma_{2}\frac{\partial J}{\partial o^{t-1}}$，其中$\gamma_{2}$表示学习率。但是，$o$不能够直接被更新，因为$o$是基学习器的输出。幸运的是，优化编码矩阵$M$可以间接地更新$o$，从而进一步降低整体训练损失。

当$M_{kj}$被用于训练基学习器$j$时，它已经决定了数据属于哪一个分类$k$。假设基学习器能够完美地匹配给定的学习目标，那么对于分类器$j$来说，任何属于$k$类的数据该分类类器的输出总是满足$o_{j}^{i}=M_{kj}$。因此，$M$的变化会影响基学习器的学习目标，从而进一步影响基学习器的输出$o$。由于在当基学习器能够完美地匹配给定的学习目标时，梯度$\frac{\partial J}{\partial M_{kj}}$与$G_{ij} = \frac{\partial J}{\partial o_{j}^{i}}$相同，此时就可以使用梯度下降的方法来优化$M$：$M_{kj}^{t}=M_{kj}^{t-1}-\gamma_{2}\frac{\partial J}{\partial M_{kj}^{t-1}}=M_{kj}^{t-1}-\gamma_{2}\frac{\partial J}{\partial G_{kj}^{t-1}}$。

但是，实践中往往没有完美的基学习器，所以也就不能用上面的方法来优化直接$M$，因为$\frac{\partial J}{\partial M_{kj}} \neq G_{ij}$。然而，有许多数据样本可以用于单个类$k$，即一个$M_{kj}$对应多个$G_{ij}$，其中$y_{i}=k$。因此，可以不必使用不稳定梯度点$G_{ij}$，而是使用每个类的平均梯度来对$\frac{\partial J}{\partial o_{j}^{i}}$进行更稳定的估计：

\begin{align}
    \frac{\partial J}{\partial M_{kj}} :=\frac{1}{\left | \Omega_{k} \right |}\underset{i \in \Omega_{k}}{\sum G_{ij}},\ where\ \Omega_{k}=\left \{ i \mid y_{i} = k \right \}
\end{align}

然后利用该估计更新编码矩阵。该优化算法在算法3中已经描述，这一算法与一般的反向传播算法几乎相同，不同的是在执行更新之前使用整个批处理数据来计算平均梯度。实验结果也证明了该方法的有效性，即通过优化全局目标函数，可以对编码矩阵进行一定的细化，以减少损失，提高泛化能力。

## 3.3讨论

1、效率：与现有的ECOC方法相比，LightMC更高效，因为它可以在训练之前使用更少的时间来找到编码矩阵。同时，由于编码矩阵在后续的训练中会被动态地细化，因此它甚至可以产生可比较的性能。此外，LightMC只需要很少的额外优化计算成本，这与单层线性模型的成本相同，并且比神经网络和GBDT等强大的基学习器相比成本要小得多。

2、小批量编码优化方法：由于使用整个批处理进行更新，因此算法3的在内存的使用效率上较低。实际上，切换到较小的批处理（mini-batch）进行更新是很自然的，因为平均梯度也可以在mini-batch中进行计算。

3、分布式编码：在现有的ECOC方法中使用二进制编码。另一方面，LightMC采用分布式编码进行连续优化。显然，分布式编码（也称为`嵌入`）比二进制编码包含了更多的信息，这使得LightMC能够利用更多的信息来处理类之间的相关性。

4、基学习器的交替训练：在算法1中，如果基学习器不是Boosting学习器，而是如神经网络，那么每次迭代时都可以调用LightMC。而对于Boosting学习器，LightMC从第$i_{s}$轮开始，每$\frac{1}{\alpha}$轮调用一次。这是因为在Boosting学习器中有一个学习率$\alpha$，它会在每次迭代时减少模型的输出。因此，Boosting学习器需要更多的迭代来适应新的训练目标。因此，使用循环次数$i_{s}$和每$\frac{1}{\alpha}$次调用一次LightMC可以提高效率，而不需要在每个迭代中调用LightMC。

5、与神经网络中的Softmax层比较：softmax解码的形式确实与神经网络中的softmax层相类似，但是它们确实不同的。

（1）神经网络中的softmax层实际上与OVA分解相同，它不使用编码矩阵来编码类之间的相关性。

（2）它们使用不同的优化方案，优化神经网络中的softmax层主要是为了降低了每个样本的损失，而softmax解码则是优化了每个类的损失（公式6）。softmax在神经网络的实际应用中，很难说哪一种方法更好，甚至最近的一些研究发现，在使用固定的softmax层时，精度几乎相同。