---
title: S5-决策树
mathjax: true
date: 2020-03-18 11:30:23
updated: {{ date }}
tags:
categories: [统计学习方法]
---

{% asset_img dt.png 决策树 %}

# 原理

决策树是一种**非参数监督**学习方法，用于分类和回归任务。对于离散值的构建的树模型，一般为分类树，而用连续值构建的树模型，一般为回归树。

>* 决策树的本质：从训练集中归纳出一组分类规则，或者说是由训练数据集估计条件概率模型。
>* 决策树学习的损失函数：正则化的极大似然函数。
>* 决策树学习的测试：最小化损失函数。

决策树的构造通常分为两个阶段，分别是`构造和剪枝`。

1、构造：`生成一颗完整的树`，即在构造的过程总选择什么属性作为节点，树中的节点有三个，分别是：

（1）根节点：树的顶端，即最开始的节点，如上图中的“天气”。

（2）内部节点：树内部的分叉节点，如上图中的“温度”、“湿度”等

（3）叶节点：树最底部的节点，即决策树最终的结果，并且叶节点没有子节点。

在进行树构造时，需要解决三个主要问题：

（1）选择哪个属性作为根节点？

（2）选择哪个属性作为子节点？

（3）停止并获得目标状态的条件？即产生一个叶节点

2、剪枝：`为防止过拟合`，就需要给决策树瘦身（并不需要精确的判断所有属性）。

{% asset_img overfitting.png 过拟合 %}

上图中的1表示欠拟合，2表示具有较好泛化性，3表示过拟合。

**泛化能力：** 指分类器是通过训练集获得的分类能力。如果对训练集分类的过于精准，即过度依赖于训练数据，那么得到的决策树其容错率就会比较低，泛化能力差。因为训练集只是全部数据的抽样，并不能体现全部数据的特点。

**预剪枝：** 在决策树构造时就进行剪枝。即在构造的过程中对节点进行评估，如果对某个节点进行划分，在验证集中不能带来准确性的提升，那么对这个节点进行划分就没有意义，这时就会把当前节点作为`叶节点`，不对其进行划分。预剪枝降低了过拟合的风险，显著减少了决策树的训练时间开销和测试时间开销，但可能带来欠拟合的风险 。

**后剪枝：** 在生成决策树之后再进行剪枝。通常会从决策树的叶节点开始，逐层向上对每个节点进行评估。如果剪掉这个节点子树，与保留该节点子树在分类准确性上差别不大，或者剪掉该节点子树，能在验证集中带来准确性的提升，那么就可以把该节点子树进行剪枝。`方法是：用该节点子树中使用最频繁的哪个节点来代替该节点。`后剪枝的欠拟合风险很小，泛化性能往往优于预剪枝决策树，但其训练时间开销比未剪枝和预剪枝都要大得多。

## 特点

>* 优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。
>* 缺点：可能会产生过度匹配的问题。
>* 适用数据类型：数值型和标称型（结果只在有限目标集中取值，如列表值）。

创建分支的伪代码createBranch()如下所示：

```python
If so return 类标签：
Else
    寻找划分数据集的最好特征
    划分数据集
    创建分支节点
        for 每个划分的子集
            调用函数createBranch()并增加返回结果到分支节点中
        return 分支节点

```

## 示例

{% asset_img example1.png 示例 %}

该示例是用于构造是否去打篮球的决策树，其中的属性有天气、温度、湿度、刮风。

# 决策树学习

假设给定训练数据集

$$D=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right\}$$

其中$x_{i}=\left(x_{i}^{(1)}, x_{i}^{(2)}, \cdots, x_{i}^{(n)}\right)^{\mathrm{T}}$为树如实例（特征向量），$n$为特征个树，$y_{i} \in \{1,2, \cdots, K\}$为类标记，$i=1,2, \cdots, N$，$N$为样本容量。**决策树学习的目标是更具给定的训练数据集构建一个决策树模型，使它能够对实例进行正确的分类。**

决策树学习算法包括特征选择、决策树的生成与决策树的剪枝过程。

# 特征选择

## 纯度与信息熵

### 纯度

决策树的构造过程可以理解为寻找纯净划分的过程，纯度则是让目标变量的分歧最小。

例如下面的三个集合：

>* 集合 1：6 次都去打篮球；
>* 集合 2：4 次去打篮球，2 次不去打篮球；
>* 集合 3：3 次去打篮球，3 次不去打篮球。

按照纯度来分，集合 1 > 集合 2 > 集合 3。因为集合 1的分歧最小，集合 3的分歧最大。

### 信息熵

**信息熵表示随机变量不确定性的度量，也是在构建树的每个步骤决定要拆分特征的依据。** 设$X$是一个取有限个值的离散随机变量，其概率分布为：

$$\begin{matrix}
    P(X=x_{i})=p_{i} & i=1,2,\cdots ,n
\end{matrix}$$

则随机变量$X$的熵（经验熵）定义为

$$H(X)=H(p)=-\sum_{i=1}^{n} p_{i} \log p_{i}$$

其中，当$\log$底为2时，该信息上为比特（bit）熵，当以以$e$为底时称为纳特（nat）熵。由于熵值依赖于$X$的分布，而与$X$的取值无关，所以可以将$X$的熵记作$H(p)$。因此，熵越大，随机变量的不确定性就越大

设由随机变量$(X, Y)$，其联合概率分布为：

$$P\left(X=x_{i}, Y=y_{j}\right)=p_{i j}, i=1,2, \cdots, n ; j=1,2, \cdots, m$$

条件熵（经验条件熵）$H(Y | X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性。

$$\begin{matrix}
    H(Y | X)=-\sum_{i=1}^{n}p_{i}H\left ( Y | X = x_{i} \right )
\end{matrix}$$

其中，$p_{i} = P\left (X = x_{i} \right )$表示条件$x_{i}$出现的概率，$H\left ( Y | X = x_{i} \right )$表示事件$x_{i}$发生时，发生事件$Y$的概率。

**当不确定性越大时，它所包含的信息量也就越大，信息熵也就越高。信息熵越大，纯度越低。当集合中的所有样本均匀混合时，信息熵最大，纯度最低。**

因此，在构建决策树时，会基于纯度来进行构建，典型的“不纯度”指标有三种，分别是**信息增益（ID3 算法）、信息增益率（C4.5 算法）以及基尼指数（Cart 算法）。**

## 信息增益（ID3 算法）

特征$A$对训练数据集$D$的信息增益$g(D, A)$，定义为集合$D$的经验熵$H(D)$与特征$A$给定条件下$D$的经验条件熵$H(D | A)$之差，即

$$g(D, A)=H(D)-H(D | A)$$

父亲节点的信息熵减去所有子节点的信息熵。在计算的过程中会计算每个子节点的归一化信息熵，**即按照每个子节点在父节点中出现的概率**来计算这些子节点的信息熵。**该式可以理解为由于特征$A$而使得对数据集$D$的分类的不确定减少的程度。**

{% asset_img infogain.png 信息增益 %}

### 实现步骤

设训练数据集$D$，$|D|$表示其样本容量，即样本个树。设有$K$个类$C_{k}$，$k=1,2, \cdots, K$，$|C_{k}|$为属于类$C_{k}$的样本个树，$\sum_{k=1}^{K}\left|C_{k}\right|=|D|$。设特征$A$有$n$个不同的取值$\left\{a_{1}, a_{2}, \cdots, a_{n}\right\}$，根据特征$A$的取值将$D$划分为$n$个子集$D_{1}, D_{2}, \cdots, D_{n}$，$\left|D_{i}\right|$为$D_{i}$个样本个数，$\sum_{i=1}^{n}\left|D_{i}\right|=|D|$。记子集$D_{i}$中属于类$C_{k}$的样本的集合为$D_{i k}$，即$D_{i k}=D_{i} \cap C_{k}$，$\left|D_{i k}\right|$为$D_{i k}$的样本个树。

输入：训练数据集$D$和特征$A$；

输出：特征$A$对训练数据集$D$的信息增益$g(D, A)$。

（1）计算数据集$D$的经验熵$H(D)$

$$H(D)=-\sum_{k=1}^{K} \frac{\left|C_{k}\right|}{|D|} \log _{2} \frac{\left|C_{k}\right|}{|D|}$$

（2）计算特征$A$对数据集$D$的经验条件熵$H(D | A)$

$$H(D | A)=\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} H\left(D_{i}\right)=-\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{D |} \sum_{k=1}^{K} \frac{\left|D_{i k}\right|}{\left|D_{i}\right|} \log _{2} \frac{\left|D_{i k}\right|}{\left|D_{i}\right|}$$

（3）计算信息增益

$$g(D, A)=H(D)-H(D | A)$$

{% asset_img dt-split.png 节点分裂 %}

### 缺陷

1、ID3没有剪枝策略，容易过拟合。

2、ID3算法中倾向于选择出现概率比较多的属性。例如，“编号”属性容易将会被选为最优属性。

3、只能用于处理离散分布的特征。

4、没有考虑缺失值。

所以，ID3的缺陷为**当有些属性可能对分类任务没有太大作用，但是他们仍然可能会被选为最优属性**，即对噪声敏感，并且当训练数据如果有少量错误，可能会产生决策树分类错误。

## 信息增益比（C4.5算法）

### 改进

1、引入悲观剪枝策略进行后剪枝。

2、引入信息增益率作为划分标准。

3、离散化处理连续值。

4、可以处理缺省数据。

### 原理

特征$A$对训练数据集$D$的信息增益比$g_{R}(D, A)$为信息增益$g(D, A)$与训练数据集$D$关于特征$A$的值的熵$H_{A}(D)$之比。

$$g_{R}(D, A)=\frac{g(D, A)}{H_{A}(D)}$$

其中，$H_{A}(D)=-\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} \log _{2} \frac{\left|D_{i}\right|}{|D|}$，$n$是特征$A$取值的个数。

因为ID3在计算的时候，倾向于选择取值多的属性。为了避免这个问题，C4.5 采用信息增益比的方式来选择属性。

当属性有很多值的时候，相当于被划分成了许多份，**虽然信息增益变大了，但是对于C4.5来说，属性熵也会变大，** 所以整体的信息增益率并不大。

### 离散化处理连续值

对于处理连续值时，C4.5选择具有最高信息增益属性的属性值作为阈值。

### 处理缺省数据

当某个属性由于数据缺失，使得只有$B$条样本，而总样本数为$A$，那么最后算出的信息增益率为：

$$\text { 某属性的信息增益率 }=\frac{B}{A}*\text { 某属性的实际信息增益率 }$$

### 缺陷

1、C4.5只能用于分类。

2、熵模型中的对数运算、连续值、排序运算都消耗大量运算。

C4.5在ID3的基础上，用信息增益率代替了信息增益，解决了噪声敏感的问题，并且可以对所构造树进行剪枝，同时还能处理连续数值以及数值缺失等情况。**但由于C4.5需要对数据集进行多次扫描，算法效率相对较低。**

# 决策树的生成

## ID3的生成算法

输入：训练数据集$D$，特征集$A$阈值$\varepsilon$；

输出：决策树$T$。

（1）若$D$中所有实例属于同一类$C_{k}$，则$T$为单节点树，并将类$C_{k}$作为该结点的类标记，返回$T$；

（2）若$A=\varnothing$，则$T$为单节点树，并将$D$中实例数最大的类$C_{k}$作为该结点的类标记，返回$T$；

（3）否则，按照ID3算法计算$A$中各特征对$D$的信息增益，选择信息增益最大的特征$A_{g}$；

（4）如果$A_{g}$的信息增益小于阈值$\varepsilon$，则置$T$为单结点树，并将$D$中实例数最大的类$C_{k}$作为该结点的类标记，返回$T$；

（5）否则，对$A_{g}$的每一可能$a_{i}$，依$A_{g}=a_{i}$将$D$分割为若干非空子集$D_{i}$，将$D_{i}$中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树$T$，返回$T$；

（6）对第$i$个子结点，以$D_{i}$为训练集，以$A-\left\{A_{g}\right\}$为特征集，递归地调用步骤（1）～（5），得到子树$T_{i}$，返回$T_{i}$。

## C4.5的生成算法

输入：训练数据集$D$，特征集$A$阈值$\varepsilon$；

输出：决策树$T$。

（1）若$D$中所有实例属于同一类$C_{k}$，则$T$为单节点树，并将类$C_{k}$作为该结点的类标记，返回$T$；

（2）若$A=\varnothing$，则$T$为单节点树，并将$D$中实例数最大的类$C_{k}$作为该结点的类标记，返回$T$；

（3）否则，按照C4.5算法计算$A$中各特征对$D$的**信息增益比，** 选择信息增益比最大的特征$A_{g}$；

（4）如果$A_{g}$的信息增益比小于阈值$\varepsilon$，则置$T$为单结点树，并将$D$中实例数最大的类$C_{k}$作为该结点的类标记，返回$T$；

（5）否则，对$A_{g}$的每一可能$a_{i}$，依$A_{g}=a_{i}$将$D$分割为若干非空子集$D_{i}$，将$D_{i}$中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树$T$，返回$T$；

（6）对第$i$个子结点，以$D_{i}$为训练集，以$A-\left\{A_{g}\right\}$为特征集，递归地调用步骤（1）～（5），得到子树$T_{i}$，返回$T_{i}$。

# 决策树的剪枝

**决策树的剪枝往往通过极小化决策树整体的损失函数来实现。** 设树$T$的叶结点个数为$|T|$，$t$是树$|T|$的叶结点，该叶结点有$N_{t}$个样本点，其中$k$类的样本点有$N_{t k}$个，$k=1,2, \cdots, K$，$H_{t}(T)$为叶结点$t$上的经验熵，$\alpha \geqslant 0$为参数，则决策树学习的损失函数可以定义为

$$C_{\alpha}(T)=\sum_{t=1}^{|T|} N_{t} H_{t}(T)+\alpha|T|$$

其中经验熵为

$$H_{t}(T)=-\sum_{k} \frac{N_{t k}}{N_{t}} \log \frac{N_{t k}}{N_{t}}$$

同时，损失函数的第一项可以记作

$$C(T)=\sum_{t=1}^{|T|} N_{t} H_{t}(T)=-\sum_{t=1}^{|T|} \sum_{k=1}^{K} N_{t k} \log \frac{N_{t k}}{N_{t}}$$

因此，损失函数就变为

$$C_{\alpha}(T)=C(T)+\alpha|T|$$

$C(T)$表示模型对训练数据的预测误差，即模型与训练树的拟合程度，$|T|$表示模型复杂度，参数$\alpha \geqslant 0$控制两者之间的影响。较大的$\alpha$促使选择较简单的模型，较小的$\alpha$促使选择较复杂的模型。$\alpha=0$意味着只考虑模型与训练数据的拟合程度，不考虑模型的复杂度。

## 树的剪枝算法

输入：生成算法产生的整个树$T$，参数$\alpha$；

输出：修剪后的子树$T_{\alpha}$。

（1）计算每个结点的经验熵。

（2）递归的从树的叶结点向上回缩。

设一组叶结点回缩到其父结点之前与之后的整体树分别为$T_{B}$和$T_{A}$，其对应的损失函数值分别是$C_{\alpha}(T_{B})$和$C_{\alpha}(T_{A})$，如果$C_{\alpha}(T_{A}) \leq C_{\alpha}(T_{B})$，则进行剪枝，即将父结点变为新的叶结点。

（3）返回（2），直到不能继续为止，从而得到损失函数最小的子树$T$。

# 基尼指数（CART算法）

## 原理

CART的全称是分类与回归树，即该算法既可以用于分类问题，也可以用于回归问题，并且使用CART生成的树只能是二叉树，并且左分支为“是”，右分支为“否”。

## 算法步骤

（1）决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大。

（2）决策树剪枝：用于验证数据集对已生成的数进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准。

## 回归树

使用**平方误差最小化准则**来选择特征并进行划分。每一个叶节点给出的预测值是划分到该叶子节点的所有样本目标值的均值，这样只是在给定划分的情况下最小化了平方误差。

假设$X$和$Y$分别是输入和输出变量，并且$Y$是连续变量，给定一组训练数据为

$$D=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right\}$$

所谓回归树就把输入空间（特征空间）进行划分，并且在每个划分的单元上进行输出。如果把空间划分为$M$个单元$R_{1},R_{2},\cdots,R_{M}$，并且在每个单元$R_{m}$上有一个固定的输出$c_{m}$，那么回归树就可以表示为：

$$f(x)=\sum_{m=1}^{M} c_{m} I\left(x \in R_{m}\right)$$

并且回归树使用平方误差作为损失函数$\underset{x_{i} \in R_{m}}{\sum} \left(y_{i}-f\left(x_{i}\right)\right)^{2}$，因此使用最小平方误差来求每个单元$R_{m}$上的最优输出值。由平方误差最小准则就可以知道单元$R_{m}$上的$c_{m}$最优值$\hat{c_{m}}$为所有输入实例输出的$y_{i}$的均值，即：

$$\hat{c}_{m}=\operatorname{ave}\left(y_{i} | x_{i} \in R_{m}\right)$$

在CART回归树中，输入空间的划分采用启发式方法，即选择第$j$个变量$x^{(j)}$和空间取值$s$作为分割变量和分割点，并通过分割点定义两个区域：

$$R_{1}(j, s)=\left\{x | x^{(j)} \leqslant s\right\} \text{和}R_{2}(j, s)=\left\{x | x^{(j)}>s\right\}$$

因此，最优的分割变量$j$和分割点$s$选择就是求解下面的式子：

$$\min _{j, s}\left[\min _{c_{1}} \sum_{x_{i} \in R_{1}(j, s)}\left(y_{i}-c_{1}\right)^{2}+\min _{c_{2}} \sum_{x_{i} \in R_{2}(j, s)}\left(y_{i}-c_{2}\right)^{2}\right]$$

并且对于固定输入$j$，可以求的最优切点$s$，使得$\hat{c}_{1}$和$\hat{c}_{2}$最优。

$$\hat{c}_{1}=\operatorname{ave}\left(y_{i} | x_{i} \in R_{1}(j, s)\right)\text{和}\hat{c}_{2}=\operatorname{ave}\left(y_{i} | x_{i} \in R_{2}(j, s)\right)$$

### 算法流程

输入：训练数据集$D$；

输出：回归树$f(x)$。

（1）选择最优切分变量$j$与切分点$s$，求解

$$\min _{j, s}\left[\min _{c_{1}} \sum_{x_{i} \in R_{1}(j, s)}\left(y_{i}-c_{1}\right)^{2}+\min _{c_{2}} \sum_{x_{i} \in R_{2}(j, s)}\left(y_{i}-c_{2}\right)^{2}\right]$$

遍历变量$j$，对固定的切分变量$j$扫描切分点$s$，选择使上式达到最小值的$(j, s)$。

（2）用选定的$(j, s)$划分区域并决定相应的输出值

$$R_{1}(j, s)=\left\{x | x^{(j)} \leqslant s\right\} \text{和}R_{2}(j, s)=\left\{x | x^{(j)}>s\right\}$$

$$\hat{c}_{m}=\frac{1}{N_{m}} \sum_{x_{i} \in R_{m}(j, s)} y_{i}, x \in R_{m}, m=1,2$$

（3）继续对两个子区域调用步骤（1）和（2），直至满足停止条件。

（4）将输入空间划分为$M$个区域$R_{1}, R_{2}, \cdots, R_{M}$，生成决策树

$$f(x)=\sum_{m=1}^{M} \hat{c}_{m} I\left(x \in R_{m}\right)$$

## 分类树

使用**基尼指数（GINI）最小化准则**来选择特征并进行划分。基尼指数表示集合的不确定性，或者是不纯度。基尼指数越大，集合不确定性越高，不纯度也越大。假设有$K$个类，样本点属于$k$类的概率为$p_{k}$，则概率分布的基尼值数定义为：

$$\operatorname{Gini}(p)=\sum_{k=1}^{K} p_{k}\left(1-p_{k}\right)=1-\sum_{k=1}^{K} p_{k}^{2}$$

对于二分类问题，若样本点属于第一个类的概率为$p$，则概率分布的基尼值数为

$$\operatorname{Gini}(p)=2 p(1-p)$$

对于给定的样本集合$D$，其基尼值数为

$$\operatorname{Gini}(D)=1-\sum_{k=1}^{K}\left(\frac{\left|C_{k}\right|}{|D|}\right)^{2}$$

其中$\left | C_{k} \right |$表示$D$中属于第$k$类样本的个数，$\left | D \right |$数据集的个数，$K$是类的个数。

如果样本集合$D$中根据特征$A$的某个取值$a$来把$D$分割为$D_{1}$和$D_{2}$，即

$$D_{1}=\{(x, y) \in D | A(x)=a\}, D_{2}=D-D_{1}$$

那么在特征A的条件下，集合D的基尼指数为：

$$\operatorname{Gini}(D, A)=\frac{\left|D_{1}\right|}{|D|} \operatorname{Gini}\left(D_{1}\right)+\frac{\left|D_{2}\right|}{|D|} \operatorname{Gini}\left(D_{2}\right)$$

其中$\operatorname{Gini}\left(D\right)$表示集合$D$的不确定性，$\operatorname{Gini}(D, A)$表示经$A=a$分割后集合$D$的不确定性。基尼指数值越大，样本集合的不确定性就越大。**在树生成时选择基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点。**

### 算法流程

输入：训练数据集$D$，停止计算的条件；

输出：CART决策树。

（1）设结点的训练数据集$D$，计算现有特征对数据集的基尼值数。此时，对每个特征$A$，对其可能取的每个值$a$，根据样本点对$A=a$的测试为“是”或“否”将$D$分割为$D_{1}$和$D_{2}$两部分，然后使用公式$\operatorname{Gini}(D, A)$计算$A=a$的基尼值数。

（2）在所有可能的特征$A$以及它所有可能的切分点$a$中，选择基尼值数最小的特征及其对应的切分点作为最优特征与最优切分点。依最优特征与最优切分点，从现结点生成两个子结点，将训练数据集依特征分配到两个子结点中去。

（3）对两个子结点递归调用（1）和（2），直至满足停止条件。

（4）生成CART决策树

停止条件是结点中的样本个数小于阈值，或样本集的基尼值数小于预定阈值（样本基本属于同一类），或没有更多特征。

## CART剪枝

决策树的损失函数为：

$$C_{\alpha}(T)=C(T)+\alpha|T|$$

其中$T$为任意子树；$C(T)$为预测误差，用来衡量模型与训练数据的拟合程度；$|T|$为子树$T$的叶子节点个数，即树的复杂度；$\alpha$为惩罚系数，用来控制模型和训练数据之间拟合程度，当$\alpha$较大时，树模型较为简单，最优子树$T_{\alpha}$偏小，反之则树模型较为复杂，即最优子树$T_{\alpha}$偏大。当$\alpha=0$时，整体树最优。当$\alpha \rightarrow \infty$时，根结点组成的单结点树最优。

### 剪枝算法

输入：CART算法生成的决策树$T_{0}$

输出：最优决策树$T_{\alpha}$。

（1）设$k=0$，$T=T_{0}$

（2）设$\alpha=+\infty$

（3）自下而上地对个内部结点$t$计算$C\left(T_{t}\right)$，$\left|T_{t}\right|$以及

$$g(t)=\frac{C(t)-C\left(T_{t}\right)}{\left|T_{t}\right|-1}$$

$$\alpha=\min (\alpha, g(t))$$

其中，$T_{t}$表示以$t$为根结点的子树，$C\left(T_{t}\right)$是对训练树的预测误差，$\left|T_{t}\right|$是$T_{t}$的叶结点个数。

（4）对$g(t)=\alpha$的内部结点$t$进行剪枝，并对叶结点$t$以多数表决决定其类，得到树$T$。

（5）设$k=k+1$，$\alpha_{k}=\alpha$，$T_{k}=T$

（6）如果$T_{k}$不是由根结点及两个也结点构成的树，则回到步骤（2）；否则令$T_{k}=T_{n}$

（7）采用交叉验证法在子树序列$T_{0}, T_{1}, \cdots, T_{n}$中选取最优子树$T_{\alpha}$

## 优点

>* 基尼指数的计算不需要对数运算，更加高效。
>* 基尼指数更偏向于连续属性，熵更偏向于离散属性。

# 工程实践

```python
sklearn.tree.DecisionTreeClassifier(
    criterion='gini', splitter='best', max_depth='None', 
    min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0,
    max_features=None, random_state=None, max_leaf_nodes=None,
    min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.0
)
```

1. criterion（类型：str，默认值：gini）：分类结点的度量指标。scikit-learn中支持2种度量指标，分别是：
   - gini：基尼值数
   - entropy：信息增益
2. splitter（类型：str，默认值：best）：分类点的选择方式。scikit-learn中支持2种度量指标，分别是：
   - best：选择最好的结点
   - random：随机选择最好的结点
3. max_depth（类型：int，默认值：None）：树的最大深度。
4. min_samples_split（类型：int或float，默认值：2）：最小样本树的阈值。
   - 参数为int：min_samples_split作为最小数量。
   - 参数为float：min_samples_split * n_samples作为每个分割点的样本最小数量。
5. min_samples_leaf（类型：int或float，默认值：1）：叶结点的最小样本数。
   - 参数为int：min_samples_leaf作为最小数量。
   - 参数为float：min_samples_leaf * n_samples作为每个叶结点的最小样本数。
6. min_weight_fraction_leaf（类型：float，默认值：0.0）：叶节点的权重总和的最小加权分数。
7. max_features（类型：int、float或str，默认值：None）：寻找最佳分割点时考虑的特征数。
   - 参数为int：每个分割点考虑max_features个特征。
   - 参数为float：每个分割点考虑int(max_features * n_features)个特征。
   - 参数为auto：每个分割点考虑max_features=sqrt(n_features)个特征。
   - 参数为sqrt：每个分割点考虑max_features=sqrt(n_features)个特征。
   - 参数为log2：每个分割点考虑max_features=log2(n_features)个特征。
   - 参数为None：每个分割点考虑max_features=n_features个特征。
8. random_state（类型：int，默认值：None）：随机数种子。
9. max_leaf_nodes（类型：int，默认值：None）：最大叶结点数量。
10. min_impurity_decrease（类型：float，默认值：0.0）：分割点损失值的阈值。
11. class_weight（类型：dict、 列表dict或"balanced"，默认值：None）：类别权重。
12. ccp_alpha（类型：非负float，默认值：0.0）：惩罚项系数。

# 作业

1、证明CART剪枝算法中，当$\alpha$确定时，存在唯一的最小子树$T_{\alpha}$使损失函数$C_{\alpha}(T)$最小。

反证法：假设最小子树不唯一，有2个最优子树。

情况1：假设有两个最优子树$C_{\alpha}(T_{0})$和$C_{\alpha}(T_{1})$，并且$C_{\alpha}(T_{1})$由$C_{\alpha}(T_{0})$剪枝得到，所以就可以得到$C_{\alpha}(T_{1}) \leq C_{\alpha}(T_{0})$，所以就与有两个最优子树相矛盾。

情况2：假设有两个最优子树$C_{\alpha}(T_{1})$和$C_{\alpha}(T_{2})$，并且由$C_{\alpha}(T_{0})$剪枝得到，那么也就是说同时把$C_{\alpha}(T_{1})$和$C_{\alpha}(T_{2})$剪掉可以得到比$C_{\alpha}(T_{1})$和$C_{\alpha}(T_{2})$更优子树，因此这与假设中的有两个最优子树相矛盾。

所以与假设相矛盾，因此最优子树是唯一的。

2、尝试调用sklearn.tree.DecisionTreeClassifier模块，训练数据集采用课本例题5.1的数据，判断是否应该批准下列人员的贷款申请。

| ID | 年龄 | 有工作 | 有自己的房子 | 信贷情况 |
| :---: | :---: | :---: | :---: | :---: |
| 1 | 青年 | 否 | 是 | 一般 |
| 2 | 中年 | 是 | 否 | 好 |
| 3 | 老年 | 否 | 是 | 一般 |

```python
import numpy as np
import pandas as pd

from sklearn.tree import DecisionTreeClassifier, export_graphviz
from sklearn.preprocessing import LabelEncoder
import pydotplus

if __name__ == '__main__':
    train_data = pd.DataFrame([
        [1, '青年', '否', '否', '一般', '否'],
        [2, '青年', '否', '否', '好', '否'],
        [3, '青年', '是', '否', '好', '是'],
        [4, '青年', '是', '是', '一般', '是'],
        [5, '青年', '否', '否', '一般', '否'],
        [6, '中年', '否', '否', '一般', '否'],
        [7, '中年', '否', '否', '好', '否'],
        [8, '中年', '是', '是', '好', '是'],
        [9, '中年', '否', '是', '非常好', '是'],
        [10, '中年', '否', '是', '非常好', '是'],
        [11, '老年', '否', '是', '非常好', '是'],
        [12, '老年', '否', '是', '好', '是'],
        [13, '老年', '是', '否', '好', '是'],
        [14, '老年', '是', '否', '非常好', '是'],
        [15, '老年', '否', '否', '一般', '否'],
    ])

    test_data = pd.DataFrame([
        [1, '青年', '否', '是', '一般'],
        [2, '中年', '是', '否', '好'],
        [3, '老年', '否', '是', '一般'],
    ])

    # 获取训练数据的X和Y
    x_train = train_data.iloc[:, 1:-1]
    y_train = train_data.iloc[:, -1]
    x_test = test_data.iloc[:, 1:test_data.shape[1]]

    # 获取数据中的数据类别
    x_unique = np.unique(x_train)
    y_unique = np.unique(y_train)

    # 标签编码预处理
    x_encoder = LabelEncoder()
    y_encoder = LabelEncoder()

    # 拟合X和Y的标签
    x_encoder.fit(x_unique)
    y_encoder.fit(y_unique)

    # 把Y转化为标签编码
    y_train = y_encoder.transform(y_train)
    # 把X转化为标签编码
    for i in range(x_train.shape[1]):
        x = x_train.iloc[:, i].values
        value = x_encoder.transform(x_train.iloc[:, i])
        data = dict(map(lambda x, y: [x, y], x, value))
        x_train.iloc[:, i].replace(data, inplace=True)
    x_train = x_train.values

    # 把X转化为标签编码
    for i in range(x_test.shape[1]):
        x = x_test.iloc[:, i].values
        value = x_encoder.transform(x_test.iloc[:, i])
        data = dict(map(lambda x, y: [x, y], x, value))
        x_test.iloc[:, i].replace(data, inplace=True)
    x_test = x_test.values

    # 创建分类器
    clf = DecisionTreeClassifier()
    # 训练数据拟合
    clf.fit(x_train, y_train)
    # 对测试数据进行预测
    y_test = clf.predict(x_test)
    # 反向获取描述
    y_test = y_encoder.inverse_transform(y_test)

    print(y_test)
```